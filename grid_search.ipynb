{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "TRAINING_DIR = os.getcwd()\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(TRAINING_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(course_vecs, course_descipts, trained_weights, trained_biases, num_words_per_course):\n",
    "    \"\"\"\n",
    "    lalalal\n",
    "    \n",
    "    \"\"\"\n",
    "    df_with_keywords = course_descipts.copy()\n",
    "    softmax_frame = course_vecs.iloc[:,1:].dot(trained_weights.values) + trained_biases # make predictions\n",
    "\n",
    "    # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "    print('[INFO] Sorting classification results...')\n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_words_per_course:]\n",
    "\n",
    "    print('[INFO] Predicting top k inferred keywords for each course...')\n",
    "    for i in range(num_words_per_course):\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        df_with_keywords['predicted_word_' + str(num_words_per_course-i)] = new_col.values\n",
    "        \n",
    "    return df_with_keywords\n",
    "\n",
    "def calculate_metric(df_with_keywords, metric):\n",
    "    \"\"\"\n",
    "    metrics: {r: recall, p: precision}\n",
    "    \"\"\"\n",
    "    def clean_descrip_title(row):\n",
    "        punc_remover = str.maketrans('', '', string.punctuation)\n",
    "        lowered = row['descrip_title'].lower()\n",
    "        lowered_removed_punc = lowered.translate(punc_remover)\n",
    "        cleaned_set = set(lowered_removed_punc.split())\n",
    "        return cleaned_set\n",
    "\n",
    "    def recall_keywords(row):\n",
    "        return row['description_title_set'].intersection(row['course_keywords_set'])\n",
    "    \n",
    "    prediction_df = df_with_keywords.copy()\n",
    "    only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'course_subject', 'course_alternative_names'])]\n",
    "    num_keywords_predicted_per_course = only_predicted_keywords_df.shape[1]\n",
    "    prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "    prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "    prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "    \n",
    "    if metric == 'r':\n",
    "        print('[INFO] Calculating Recall...')\n",
    "        assert num_keywords_predicted_per_course == max_descript_len, 'Number of keywords predicted should equal longest description length'\n",
    "        prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "        average_recall = np.mean(prediction_df['recall'])\n",
    "        return average_recall\n",
    "    if metric == 'p':\n",
    "        print('[INFO] Calculating Precision...')\n",
    "        assert num_keywords_predicted_per_course == num_top_words, 'Number of keywords predicted should equal number of predicted words per course'\n",
    "        prediction_df['precision'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / num_top_words)\n",
    "        average_precision = np.mean(prediction_df['precision'])\n",
    "        return average_precision\n",
    "    if metric == 'c':\n",
    "        print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "        predicted_keyword_list = only_predicted_keywords_df.values.tolist()\n",
    "        predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "        keyword_counter = Counter(predicted_keyword_list)\n",
    "        print('[INFO] Most common keywords by count: ', keyword_counter.most_common(10))\n",
    "        \n",
    "        num_possible_keywords = df_with_keywords.shape[0] * num_top_words\n",
    "        num_predicted_keywords = len(keyword_counter.keys())\n",
    "        assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "        'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "        unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "        predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "        assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "        'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "    \n",
    "        cos_sim = cosine_similarity(predicted_keyword_vector, unif_keyword_vector)\n",
    "        return cos_sim\n",
    "    if metric == 'df':\n",
    "        print('[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...')\n",
    "        document_df_cols = df_with_keywords.columns.difference(['course_title', 'course_description', 'course_name', 'course_alternative_names'])\n",
    "        document_df = df_with_keywords.loc[:,document_df_cols]\n",
    "        document_df.set_index('course_subject', inplace=True)\n",
    "        \n",
    "        document_dict = defaultdict(list)\n",
    "        terms = set()\n",
    "        for index, row in document_df.iterrows():\n",
    "            document_dict[index].extend(row.values)\n",
    "            terms.update(row.values)\n",
    "\n",
    "        doc_freq_dict = defaultdict()\n",
    "        num_docs = len(document_dict.keys())\n",
    "        for term in terms:\n",
    "            doc_freq_i = 0\n",
    "            for key in document_dict.keys():\n",
    "                if term in document_dict.get(key):\n",
    "                    doc_freq_i += 1\n",
    "            doc_freq_dict[term] = doc_freq_i / (num_docs)\n",
    "            \n",
    "        print('[INFO] Most common keywords by document frequencies: ', Counter(doc_freq_dict).most_common(10)) \n",
    "        average_document_frequency_score = np.mean(list(doc_freq_dict.values()))\n",
    "        return average_document_frequency_score\n",
    "        \n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column, max_df=0.0028, use_idf=True):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(bigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab, tf_bias=.5, use_idf=True):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, num_epochs=1):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "info_frame = pd.read_csv(infofile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(info_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = info_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())\n",
    "num_top_words = 10\n",
    "\n",
    "grid_search_df = pd.DataFrame(columns=hyperparams_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.004000, tf_bias: 0.500000, num_epochs: 5\n"
     ]
    }
   ],
   "source": [
    "np.arange(0.002, .005, .001)\n",
    "\n",
    "param_grid = {'use_idf': [True],\n",
    "              'max_df': np.arange(0.002, .005, .001), # np.arange(0, .0055, .0005)\n",
    "              'tf_bias': np.arange(.5, 1, .5), # np.arange(.5, 2, .5)\n",
    "              'num_epochs': [5]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in grid:\n",
    "    print(\"[HYPERPARAMS] use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.002000, tf_bias: 0.500000, num_epochs: 5***\n",
      "======== [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11382\n",
      "[INFO] Number of bigrams: 1138\n",
      "[INFO] Number of trigrams: 113\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 448us/step - loss: 14054.1881 - acc: 0.0076\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 272us/step - loss: 12914.8634 - acc: 0.0393\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 265us/step - loss: 12221.4571 - acc: 0.0713\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 272us/step - loss: 11661.7963 - acc: 0.0915\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 281us/step - loss: 11137.4561 - acc: 0.1181\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.005667.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.035637.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('residents', 222), ('imperialism', 157), ('activism', 113), ('sovereignty', 101), ('germany', 97), ('alliances', 95), ('fisheries', 92), ('artificial', 91), ('capitalist', 89), ('scene', 89)]\n",
      "[INFO] Fold 1 cosine similarity: 0.570956.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('residents', 0.45695364238410596), ('imperialism', 0.2847682119205298), ('activism', 0.23178807947019867), ('want', 0.2052980132450331), ('letters', 0.1986754966887417), ('capitalist', 0.1986754966887417), ('sovereignty', 0.18543046357615894), ('professor', 0.17880794701986755), ('scene', 0.17880794701986755), ('germany', 0.16556291390728478)]\n",
      "[INFO] Fold 1 cosine similarity: 0.022685.\n",
      "======== [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11218\n",
      "[INFO] Number of bigrams: 1121\n",
      "[INFO] Number of trigrams: 112\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 416us/step - loss: 13463.6449 - acc: 0.0064\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 309us/step - loss: 12394.8957 - acc: 0.0452\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 304us/step - loss: 11713.2725 - acc: 0.0791\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 302us/step - loss: 11176.6966 - acc: 0.1008\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 323us/step - loss: 10654.5933 - acc: 0.1244\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.005985.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.031233.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('brought', 205), ('racism', 126), ('opinion', 110), ('industrialization', 107), ('residents', 99), ('historiography', 97), ('wars', 96), ('derivative', 93), ('expanded', 80), ('theatre', 79)]\n",
      "[INFO] Fold 2 cosine similarity: 0.589433.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('brought', 0.34306569343065696), ('expanded', 0.24087591240875914), ('residents', 0.22627737226277372), ('perceptions', 0.21897810218978103), ('racism', 0.21897810218978103), ('derivatives', 0.20437956204379562), ('inter', 0.1897810218978102), ('existence', 0.1897810218978102), ('theatre', 0.18248175182481752), ('derivative', 0.18248175182481752)]\n",
      "[INFO] Fold 2 cosine similarity: 0.025753.\n",
      "======== [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11108\n",
      "[INFO] Number of bigrams: 1110\n",
      "[INFO] Number of trigrams: 111\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 2s 409us/step - loss: 13194.2508 - acc: 0.0068\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 327us/step - loss: 12138.3216 - acc: 0.0435\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 321us/step - loss: 11503.7425 - acc: 0.0735\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 323us/step - loss: 10926.1572 - acc: 0.0996\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 323us/step - loss: 10470.4358 - acc: 0.1149\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.005776.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.033153.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('bce', 118), ('indians', 115), ('multicultural', 112), ('gives', 108), ('reveal', 96), ('beams', 94), ('pacific', 91), ('mexican', 80), ('ensure', 76), ('matters', 74)]\n",
      "[INFO] Fold 3 cosine similarity: 0.614685.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('gives', 0.2847682119205298), ('pacific', 0.26490066225165565), ('reveal', 0.2582781456953642), ('bce', 0.24503311258278146), ('indians', 0.23178807947019867), ('matters', 0.2185430463576159), ('differently', 0.2119205298013245), ('characters', 0.2052980132450331), ('hone', 0.2052980132450331), ('interpretations', 0.1986754966887417)]\n",
      "[INFO] Fold 3 cosine similarity: 0.025350.\n",
      "======== [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 10995\n",
      "[INFO] Number of bigrams: 1099\n",
      "[INFO] Number of trigrams: 109\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 2s 399us/step - loss: 13111.0456 - acc: 0.0061\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 308us/step - loss: 12103.7076 - acc: 0.0441\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 292us/step - loss: 11484.9316 - acc: 0.0700\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 295us/step - loss: 10944.4357 - acc: 0.0922\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 309us/step - loss: 10481.2097 - acc: 0.1106\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.005547.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.031254.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('republic', 167), ('expressed', 156), ('powers', 122), ('mexican', 98), ('wars', 93), ('storytelling', 86), ('constitute', 83), ('vernacular', 73), ('iran', 72), ('publication', 72)]\n",
      "[INFO] Fold 4 cosine similarity: 0.587210.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('expressed', 0.36054421768707484), ('republic', 0.3197278911564626), ('powers', 0.24489795918367346), ('iran', 0.22448979591836735), ('situation', 0.22448979591836735), ('investigates', 0.22448979591836735), ('religions', 0.22448979591836735), ('constitute', 0.21768707482993196), ('historic', 0.21768707482993196), ('pharmacology', 0.20408163265306123)]\n",
      "[INFO] Fold 4 cosine similarity: 0.028500.\n",
      "======== [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11094\n",
      "[INFO] Number of bigrams: 1109\n",
      "[INFO] Number of trigrams: 110\n",
      "[INFO] Performing logistic regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 2s 397us/step - loss: 13313.4464 - acc: 0.0063\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 293us/step - loss: 12283.9329 - acc: 0.0474\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 288us/step - loss: 11660.7441 - acc: 0.0720\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 308us/step - loss: 11138.3509 - acc: 0.0940\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 288us/step - loss: 10683.2892 - acc: 0.1098\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.004851.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.027186.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('submission', 111), ('sovereignty', 96), ('contextual', 90), ('monuments', 80), ('semantics', 76), ('iran', 65), ('postcolonial', 64), ('residents', 64), ('presume', 60), ('beginners', 60)]\n",
      "[INFO] Fold 5 cosine similarity: 0.593525.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('sovereignty', 0.2732919254658385), ('contextual', 0.2360248447204969), ('san', 0.22981366459627328), ('clips', 0.2236024844720497), ('leader', 0.21739130434782608), ('postcolonial', 0.2111801242236025), ('francisco', 0.2111801242236025), ('residents', 0.20496894409937888), ('submission', 0.19875776397515527), ('semantics', 0.19254658385093168)]\n",
      "[INFO] Fold 5 cosine similarity: 0.026532.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.005565   0.031693           0.591162\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.003000, tf_bias: 0.500000, num_epochs: 5***\n",
      "======== [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11934\n",
      "[INFO] Number of bigrams: 1193\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 407us/step - loss: 20781.5851 - acc: 0.0139\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 327us/step - loss: 19147.6463 - acc: 0.0571\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 310us/step - loss: 18208.8098 - acc: 0.0856\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 318us/step - loss: 17432.9778 - acc: 0.1018\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 333us/step - loss: 16693.4827 - acc: 0.1201\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.008227.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.047154.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('violence', 156), ('extent', 124), ('trees', 117), ('molecules', 97), ('price', 90), ('formations', 89), ('norms', 87), ('statement', 84), ('achievement', 81), ('analyzes', 79)]\n",
      "[INFO] Fold 1 cosine similarity: 0.599798.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('violence', 0.3443708609271523), ('recordings', 0.2251655629139073), ('extent', 0.2185430463576159), ('path', 0.2119205298013245), ('graded', 0.2052980132450331), ('authority', 0.19205298013245034), ('combines', 0.18543046357615894), ('norms', 0.18543046357615894), ('formations', 0.18543046357615894), ('distinctive', 0.16556291390728478)]\n",
      "[INFO] Fold 1 cosine similarity: 0.022586.\n",
      "======== [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11785\n",
      "[INFO] Number of bigrams: 1178\n",
      "[INFO] Number of trigrams: 117\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 444us/step - loss: 20141.3857 - acc: 0.0134\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 317us/step - loss: 18579.6074 - acc: 0.0542\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 319us/step - loss: 17676.3003 - acc: 0.0769\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 318us/step - loss: 16893.0953 - acc: 0.0966\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 321us/step - loss: 16213.0497 - acc: 0.1107\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.008078.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.041125.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('islam', 169), ('imperial', 141), ('boundaries', 136), ('struggles', 134), ('twentieth', 126), ('cold', 121), ('craft', 96), ('company', 90), ('centers', 89), ('currents', 86)]\n",
      "[INFO] Fold 2 cosine similarity: 0.590604.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('islam', 0.38686131386861317), ('imperial', 0.25547445255474455), ('cold', 0.24817518248175183), ('liberal', 0.22627737226277372), ('currents', 0.21897810218978103), ('centers', 0.21897810218978103), ('folklore', 0.2116788321167883), ('simultaneously', 0.2116788321167883), ('struggles', 0.2116788321167883), ('emerged', 0.20437956204379562)]\n",
      "[INFO] Fold 2 cosine similarity: 0.025866.\n",
      "======== [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11669\n",
      "[INFO] Number of bigrams: 1166\n",
      "[INFO] Number of trigrams: 116\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 3s 453us/step - loss: 19828.0026 - acc: 0.0129\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 337us/step - loss: 18327.5052 - acc: 0.0515\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 341us/step - loss: 17439.4730 - acc: 0.0751\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 334us/step - loss: 16682.9980 - acc: 0.0927\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 346us/step - loss: 16051.2135 - acc: 0.1127\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.007828.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.045492.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('diaspora', 140), ('ages', 131), ('twentieth', 109), ('did', 108), ('cold', 107), ('created', 103), ('foster', 94), ('administrative', 89), ('sentence', 88), ('powers', 79)]\n",
      "[INFO] Fold 3 cosine similarity: 0.604259.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('diaspora', 0.33112582781456956), ('islam', 0.23178807947019867), ('sentence', 0.2251655629139073), ('ages', 0.2251655629139073), ('pedagogical', 0.2185430463576159), ('grade', 0.2052980132450331), ('created', 0.2052980132450331), ('foster', 0.2052980132450331), ('twentieth', 0.1986754966887417), ('did', 0.1986754966887417)]\n",
      "[INFO] Fold 3 cosine similarity: 0.024755.\n",
      "======== [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11567\n",
      "[INFO] Number of bigrams: 1156\n",
      "[INFO] Number of trigrams: 115\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5902/5902 [==============================] - 3s 457us/step - loss: 19773.1313 - acc: 0.0129\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 324us/step - loss: 18278.7151 - acc: 0.0476\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 308us/step - loss: 17398.7573 - acc: 0.0768\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 291us/step - loss: 16689.2318 - acc: 0.0927\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 319us/step - loss: 16018.3161 - acc: 0.1067\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.007843.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.040678.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('islam', 179), ('ideological', 151), ('discourses', 132), ('poetic', 121), ('chronological', 115), ('democratic', 113), ('authority', 110), ('norms', 102), ('constructed', 99), ('cold', 93)]\n",
      "[INFO] Fold 4 cosine similarity: 0.570148.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('islam', 0.4217687074829932), ('ideological', 0.3333333333333333), ('liberal', 0.272108843537415), ('chronological', 0.2585034013605442), ('spread', 0.25170068027210885), ('norms', 0.25170068027210885), ('poetic', 0.24489795918367346), ('constructed', 0.23809523809523808), ('discourses', 0.23809523809523808), ('violence', 0.23129251700680273)]\n",
      "[INFO] Fold 4 cosine similarity: 0.027805.\n",
      "======== [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11666\n",
      "[INFO] Number of bigrams: 1166\n",
      "[INFO] Number of trigrams: 116\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 3s 455us/step - loss: 20070.0294 - acc: 0.0086\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 325us/step - loss: 18539.2881 - acc: 0.0464\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 293us/step - loss: 17682.1022 - acc: 0.0678\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 319us/step - loss: 16940.9246 - acc: 0.0842\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 317us/step - loss: 16315.1724 - acc: 0.1001\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.006933.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.038712.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('sentence', 155), ('conquest', 122), ('liberal', 120), ('understood', 110), ('modernity', 107), ('peace', 79), ('prospective', 78), ('welcome', 76), ('islam', 72), ('literatures', 71)]\n",
      "[INFO] Fold 5 cosine similarity: 0.561791.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('sentence', 0.35403726708074534), ('understood', 0.2981366459627329), ('liberal', 0.2732919254658385), ('conquest', 0.2670807453416149), ('modernity', 0.21739130434782608), ('peace', 0.20496894409937888), ('islam', 0.19875776397515527), ('increasingly', 0.19254658385093168), ('resistance', 0.19254658385093168), ('serves', 0.18633540372670807)]\n",
      "[INFO] Fold 5 cosine similarity: 0.026257.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.005565   0.031693           0.591162\n",
      "0    True   0.003      0.5          5  0.006674   0.037162           0.588241\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.004000, tf_bias: 0.500000, num_epochs: 5***\n",
      "======== [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12254\n",
      "[INFO] Number of bigrams: 1225\n",
      "[INFO] Number of trigrams: 122\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 492us/step - loss: 27059.1315 - acc: 0.0144\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 342us/step - loss: 25009.7474 - acc: 0.0490\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 330us/step - loss: 23853.6681 - acc: 0.0773\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 346us/step - loss: 22893.3350 - acc: 0.0988\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 353us/step - loss: 22047.5313 - acc: 0.1085\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.010174.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.053455.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('mind', 167), ('democracy', 145), ('created', 138), ('liberal', 132), ('valuation', 118), ('ecosystem', 111), ('illustrated', 111), ('moral', 109), ('looking', 100), ('competition', 96)]\n",
      "[INFO] Fold 1 cosine similarity: 0.556544.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('liberal', 0.2980132450331126), ('mind', 0.26490066225165565), ('created', 0.24503311258278146), ('humanities', 0.23178807947019867), ('democracy', 0.2251655629139073), ('islam', 0.2119205298013245), ('expository', 0.19205298013245034), ('ages', 0.19205298013245034), ('federal', 0.17218543046357615), ('arguments', 0.17218543046357615)]\n",
      "[INFO] Fold 1 cosine similarity: 0.022507.\n",
      "======== [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12140\n",
      "[INFO] Number of bigrams: 1214\n",
      "[INFO] Number of trigrams: 121\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 482us/step - loss: 27081.8512 - acc: 0.0142\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 332us/step - loss: 25082.8631 - acc: 0.0537\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 316us/step - loss: 23886.7463 - acc: 0.0703\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 311us/step - loss: 22955.6223 - acc: 0.0886\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 305us/step - loss: 22060.5268 - acc: 0.1027\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.010713.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.054336.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('colonialism', 139), ('novels', 126), ('violence', 123), ('sexual', 101), ('seen', 93), ('freedom', 92), ('assumptions', 86), ('literatures', 85), ('contributions', 81), ('dealing', 81)]\n",
      "[INFO] Fold 2 cosine similarity: 0.605929.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('violence', 0.2846715328467153), ('colonialism', 0.2846715328467153), ('sexual', 0.25547445255474455), ('novels', 0.24817518248175183), ('play', 0.24817518248175183), ('literatures', 0.24817518248175183), ('contributions', 0.21897810218978103), ('humanities', 0.21897810218978103), ('offering', 0.19708029197080293), ('freedom', 0.1897810218978102)]\n",
      "[INFO] Fold 2 cosine similarity: 0.026349.\n",
      "======== [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12018\n",
      "[INFO] Number of bigrams: 1201\n",
      "[INFO] Number of trigrams: 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 3s 474us/step - loss: 26640.3961 - acc: 0.0151\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 356us/step - loss: 24681.5302 - acc: 0.0561\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 328us/step - loss: 23566.5439 - acc: 0.0768\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 342us/step - loss: 22568.7619 - acc: 0.0910\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 348us/step - loss: 21766.1704 - acc: 0.1069\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.010582.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.056678.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('earliest', 287), ('modernity', 112), ('discourses', 105), ('seeks', 101), ('ideology', 97), ('notions', 89), ('continues', 87), ('nations', 84), ('roman', 78), ('poetic', 78)]\n",
      "[INFO] Fold 3 cosine similarity: 0.593286.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('earliest', 0.37748344370860926), ('discourses', 0.25165562913907286), ('continues', 0.24503311258278146), ('modernity', 0.24503311258278146), ('argumentative', 0.2251655629139073), ('views', 0.2119205298013245), ('ideology', 0.2052980132450331), ('seeks', 0.2052980132450331), ('nations', 0.2052980132450331), ('expose', 0.18543046357615894)]\n",
      "[INFO] Fold 3 cosine similarity: 0.026147.\n",
      "======== [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11904\n",
      "[INFO] Number of bigrams: 1190\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 3s 440us/step - loss: 26289.4304 - acc: 0.0152\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 313us/step - loss: 24360.3698 - acc: 0.0527\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 301us/step - loss: 23281.0361 - acc: 0.0752\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 288us/step - loss: 22379.7757 - acc: 0.0917\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 298us/step - loss: 21570.7964 - acc: 0.1067\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.010473.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.054644.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('renaissance', 135), ('turn', 122), ('identities', 119), ('genre', 116), ('roman', 106), ('animals', 103), ('peoples', 102), ('novels', 98), ('histories', 90), ('beliefs', 79)]\n",
      "[INFO] Fold 4 cosine similarity: 0.598703.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('uc', 0.30612244897959184), ('turn', 0.2857142857142857), ('identities', 0.2789115646258503), ('peoples', 0.2653061224489796), ('relevance', 0.2585034013605442), ('histories', 0.24489795918367346), ('animals', 0.21768707482993196), ('beliefs', 0.21768707482993196), ('grading', 0.2108843537414966), ('genre', 0.2108843537414966)]\n",
      "[INFO] Fold 4 cosine similarity: 0.028392.\n",
      "======== [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11989\n",
      "[INFO] Number of bigrams: 1198\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5902/5902 [==============================] - 3s 442us/step - loss: 26299.2010 - acc: 0.0119\n",
      "Epoch 2/5\n",
      "5902/5902 [==============================] - 2s 336us/step - loss: 24322.7704 - acc: 0.0524\n",
      "Epoch 3/5\n",
      "5902/5902 [==============================] - 2s 327us/step - loss: 23293.7148 - acc: 0.0722\n",
      "Epoch 4/5\n",
      "5902/5902 [==============================] - 2s 343us/step - loss: 22413.4068 - acc: 0.0805\n",
      "Epoch 5/5\n",
      "5902/5902 [==============================] - 2s 302us/step - loss: 21637.1466 - acc: 0.0896\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.008574.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.044339.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] most common keywords:  [('indigenous', 133), ('notions', 110), ('genre', 103), ('grading', 81), ('museum', 81), ('literatures', 80), ('genomics', 79), ('forum', 78), ('roman', 77), ('near', 75)]\n",
      "[INFO] Fold 5 cosine similarity: 0.577531.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] most greatest document freqs:  [('indigenous', 0.32919254658385094), ('notions', 0.2732919254658385), ('arguments', 0.2608695652173913), ('genre', 0.2360248447204969), ('speak', 0.22981366459627328), ('socially', 0.2111801242236025), ('authentic', 0.20496894409937888), ('colonialism', 0.18633540372670807), ('democracy', 0.18633540372670807), ('near', 0.18633540372670807)]\n",
      "[INFO] Fold 5 cosine similarity: 0.026465.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.005565   0.031693           0.591162\n",
      "0    True   0.003      0.5          5  0.006674   0.037162           0.588241\n",
      "0    True   0.004      0.5          5  0.007817   0.042338           0.587627\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {'use_idf': [True],\n",
    "#               'max_df': np.arange(0.002, .005, .001), # np.arange(0, .0055, .0005)\n",
    "#               'tf_bias': np.arange(.5, 1.5, .5), \n",
    "#               'num_epochs': [5]} \n",
    "\n",
    "# grid = ParameterGrid(param_grid)\n",
    "\n",
    "recall_validation_scores = []\n",
    "precision_validation_scores = []\n",
    "distribution_validation_scores = []\n",
    "document_frequency_validation_scores = []\n",
    "\n",
    "for params in grid:\n",
    "    print(\"***[INFO] Evaluating cross-validated model with hyperparams use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d***\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))\n",
    "\n",
    "    fold_num = 1\n",
    "    kf = KFold(n_splits=5, random_state=42) # DO NOT FIX RANDOM STATE WHEN RUNNING THE ACTUAL EXPERIMENT - NVM, should be fixed for reproducibility\n",
    "    for train_idx, valid_idx in kf.split(filtered_vec_df):\n",
    "        print('======== [INFO] Fold %d' % (fold_num))\n",
    "        # X = vectors, Y = descriptions\n",
    "        split_X_train, split_X_valid = filtered_vec_df.iloc[train_idx], filtered_vec_df.iloc[valid_idx]\n",
    "        split_Y_train, split_Y_valid = filtered_descript_df.iloc[train_idx], filtered_descript_df.iloc[valid_idx]\n",
    "\n",
    "        vocab = get_vocab(split_Y_train, textcolumn, max_df=params['max_df'], use_idf=params['use_idf']) \n",
    "        vocab_frame = pd.DataFrame(vocab)\n",
    "        vocabsize = len(vocab)\n",
    "\n",
    "        # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "        split_Y_train_BOW = to_bag_of_words(split_Y_train, textcolumn, vocab, tf_bias=params['tf_bias'], use_idf=params['use_idf'])\n",
    "        split_Y_train_BOW = split_Y_train_BOW.toarray()\n",
    "\n",
    "        (weights_frame, biases) = logistic_regression(split_X_train.iloc[:,1:], split_Y_train_BOW, num_epochs=params['num_epochs'])\n",
    "\n",
    "        print('[INFO] Predicting on validation set for recall...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, max_descript_len)\n",
    "        fold_i_average_recall = calculate_metric(df_with_keywords, 'r')\n",
    "        recall_validation_scores.append(fold_i_average_recall)\n",
    "        print('[INFO] Fold %d recall: %f.' % (fold_num, fold_i_average_recall))\n",
    "\n",
    "        print('[INFO] Predicting on validation set for precision...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, num_top_words)\n",
    "        fold_i_average_precision = calculate_metric(df_with_keywords, 'p')\n",
    "        precision_validation_scores.append(fold_i_average_precision)\n",
    "        print('[INFO] Fold %d precision: %f.' % (fold_num, fold_i_average_precision))\n",
    "\n",
    "        fold_i_distribution_diff = calculate_metric(df_with_keywords, 'c')\n",
    "        distribution_validation_scores.append(fold_i_distribution_diff)\n",
    "        print('[INFO] Fold %d cosine similarity: %f.' % (fold_num, fold_i_distribution_diff))\n",
    "        \n",
    "        fold_i_document_frequency = calculate_metric(df_with_keywords, 'df')\n",
    "        document_frequency_validation_scores.append(fold_i_document_frequency)\n",
    "        print('[INFO] Fold %d document frequency: %f.' % (fold_num, fold_i_document_frequency))\n",
    "\n",
    "        fold_num += 1\n",
    "\n",
    "    recall_i = np.mean(recall_validation_scores)\n",
    "    precision_i = np.mean(precision_validation_scores)\n",
    "    distribution_diff_i = np.mean(distribution_validation_scores)\n",
    "    document_frequency_i = np.mean(document_frequency_validation_scores)\n",
    "\n",
    "    hyperparams_cols = ['use_idf', 'max_df','tf-bias', 'num_epochs', 'recall', 'precision', 'distribution_diff', \n",
    "                       'document_frequency']\n",
    "    model_i_params = [params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs'], \n",
    "                      recall_i, precision_i, distribution_diff_i, document_frequency_i]\n",
    "    model_i_params = pd.DataFrame([model_i_params], columns=hyperparams_cols)\n",
    "    grid_search_df = grid_search_df.append(model_i_params, sort = False)\n",
    "    print(grid_search_df)\n",
    "    # print('recall scores:', recall_validation_scores)\n",
    "    # print('precision scores:', precision_validation_scores)\n",
    "    # print('distribution scores:', distribution_validation_scores)\n",
    "    \n",
    "grid_search_df.to_csv('score_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use_idf</th>\n",
       "      <th>max_df</th>\n",
       "      <th>tf-bias</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>distribution_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.591162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>0.588241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.042338</td>\n",
       "      <td>0.587627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
       "0    True   0.002      0.5          5  0.005565   0.031693           0.591162\n",
       "0    True   0.003      0.5          5  0.006674   0.037162           0.588241\n",
       "0    True   0.004      0.5          5  0.007817   0.042338           0.587627"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
