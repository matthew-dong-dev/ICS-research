{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search \n",
    "\n",
    "1. [ ] add hidden layer for the regression\n",
    "1. instead of using regular max_df as cutoff use max_df across subjects as filter? \n",
    "    - We want to continue to filter out generic words using max df, how we calculate the max_df threshold isn't as important\n",
    "    - you can get which words were filtered out using stop_words_ attribute\n",
    "    - make it generalizable to different school datasets\n",
    "    - max_df = 1.0 (**NEEDS TO BE 1.0 OTHERWISE IT'S NOT TREATED AS A PERCENTAGE and you \"ignore words that only appear in more than 1 document\" so you get all the esoteric words that only appear in one course description**)\n",
    "    - what does this error mean `ValueError: max_df corresponds to < documents than min_df` and why does it occur was max_df = 0?\n",
    "1. does getting bigrams and trigrams separately make a difference? - try getting the vocab using ngram_range = 1,3 and no limit on max_features\n",
    "    - Yes it does make a difference and might be an option to explore for feature engineering\n",
    "    - remove trigrams / no limit on bigram featurse\n",
    "1. *optimization:* do not append to dataframes, start w/ lists and convert to dataframe OR initialize a numpy matrix for the hyperparameters using np.empty first and then [populate instead of appending](https://stackoverflow.com/questions/13370570/elegant-grid-search-in-python-numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "TRAINING_DIR = os.getcwd()\n",
    "DATA_DIR = './data'\n",
    "vectorfile = os.path.join(DATA_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(DATA_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(df_with_keywords, metric):\n",
    "    \"\"\"\n",
    "    metrics: {r: recall, p: precision}\n",
    "    \"\"\"\n",
    "    def clean_descrip_title(row):\n",
    "        punc_remover = str.maketrans('', '', string.punctuation)\n",
    "        lowered = row['descrip_title'].lower()\n",
    "        lowered_removed_punc = lowered.translate(punc_remover)\n",
    "        cleaned_set = set(lowered_removed_punc.split())\n",
    "        return cleaned_set\n",
    "\n",
    "    def recall_keywords(row):\n",
    "        return row['description_title_set'].intersection(row['course_keywords_set'])\n",
    "    \n",
    "    prediction_df = df_with_keywords.copy()\n",
    "    only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'course_subject', 'course_alternative_names'])]\n",
    "    num_keywords_predicted_per_course = only_predicted_keywords_df.shape[1]\n",
    "    prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "    prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "    prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "    \n",
    "    if metric == 'r':\n",
    "        print('[INFO] Calculating Recall...')\n",
    "        assert num_keywords_predicted_per_course == max_descript_len, 'Number of keywords predicted should equal longest description length'\n",
    "        prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "        average_recall = np.mean(prediction_df['recall'])\n",
    "        return average_recall\n",
    "    if metric == 'p':\n",
    "        print('[INFO] Calculating Precision...')\n",
    "        assert num_keywords_predicted_per_course == num_top_words, 'Number of keywords predicted should equal number of predicted words per course'\n",
    "        prediction_df['precision'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / num_top_words)\n",
    "        average_precision = np.mean(prediction_df['precision'])\n",
    "        return average_precision\n",
    "    if metric == 'c':\n",
    "        print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "        predicted_keyword_list = only_predicted_keywords_df.values.tolist()\n",
    "        predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "        keyword_counter = Counter(predicted_keyword_list)\n",
    "        print('[INFO] Most common keywords by count: ', keyword_counter.most_common(10))\n",
    "        \n",
    "        num_possible_keywords = df_with_keywords.shape[0] * num_top_words\n",
    "        num_predicted_keywords = len(keyword_counter.keys())\n",
    "        assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "        'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "        unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "        predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "        assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "        'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "    \n",
    "        cos_sim = cosine(predicted_keyword_vector, unif_keyword_vector)\n",
    "        return cos_sim\n",
    "    if metric == 'df':\n",
    "        print('[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...')\n",
    "        document_df_cols = df_with_keywords.columns.difference(['course_title', 'course_description', 'course_name', 'course_alternative_names'])\n",
    "        document_df = df_with_keywords.loc[:,document_df_cols]\n",
    "        document_df.set_index('course_subject', inplace=True)\n",
    "        \n",
    "        document_dict = defaultdict(list)\n",
    "        terms = set()\n",
    "        for index, row in document_df.iterrows():\n",
    "            document_dict[index].extend(row.values)\n",
    "            terms.update(row.values)\n",
    "\n",
    "        doc_freq_dict = defaultdict()\n",
    "        num_docs = len(document_dict.keys())\n",
    "        for term in terms:\n",
    "            doc_freq_i = 0\n",
    "            for key in document_dict.keys():\n",
    "                if term in document_dict.get(key):\n",
    "                    doc_freq_i += 1\n",
    "            doc_freq_dict[term] = doc_freq_i / (num_docs)\n",
    "            \n",
    "        print('[INFO] Most common keywords by document frequencies: ', Counter(doc_freq_dict).most_common(10)) \n",
    "        average_document_frequency_score = np.mean(list(doc_freq_dict.values()))\n",
    "        return average_document_frequency_score\n",
    "        \n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column, max_df=0.057611, use_idf=True):\n",
    "    \"\"\"Gets the vocab labels to be used as inferred keywords. \n",
    "    Args:\n",
    "        Dataframe with column name (string) to parse vocab from.\n",
    "        Max_df (float): max document frequency for sklearn's vectorizer\n",
    "        Use_idf (boolean): Use tf-idf to get top feature labels vs just using tf\n",
    "    Returns:\n",
    "        Array of vocab labels.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "    \n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    test_corpus = dataframe.course_title.fillna('') + ' ' + dataframe.course_title.fillna('') + ' ' + dataframe.course_description.fillna('')\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf) \n",
    "    X = vectorizer.fit_transform(test_corpus)   # vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] number unigrams: %d' % (len(unigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, stop_words='english', ngram_range=(2,2), use_idf=use_idf, max_features=max(1, int(len(unigrams)/2)))\n",
    "    X = vectorizer.fit_transform(test_corpus)  #  vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams)) # , trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab, use_idf=True, tf_bias=.5):\n",
    "    \"\"\"Converts text corpus into its BOW representation using predefined vocab.\n",
    "    Args:\n",
    "        raw dataframe, text column, and vocabulary.\n",
    "    Returns:\n",
    "        A sparse matrix of the bag of words representation of the column.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        print('[INFO] Not using tf-bias')\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, use_hidden_layer=False, hidden_layer_size=200, num_epochs=5):\n",
    "    \"\"\"Perform multinomial logistic regression from BOW vector space (Y) onto course vector space (X). \n",
    "    Args: \n",
    "        Matrix of course vectors and corresponding BOW description encodings and number of epochs. \n",
    "        Hidden_layer_size must be greater than the max_description_len trying to predict (181)\n",
    "    Returns:\n",
    "        Tuple of weights and bias dataframes to use in prediction.\n",
    "    \"\"\"\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],)) # course vec\n",
    "    if use_hidden_layer:\n",
    "        hidden_layer = Dense(hidden_layer_size, activation='sigmoid')(inputs)\n",
    "        predictions = Dense(vocabsize, activation='softmax')(hidden_layer)\n",
    "    else:\n",
    "        predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(course_vecs, course_descripts, trained_weights, trained_biases, vocab_frame, num_words_per_course=10):\n",
    "    \"\"\"Predict inferred keywords for each course using train the vectorspace coeffs to predict the BOW of a point.\n",
    "    Args:\n",
    "        Course vectors, course description, weights and biases\n",
    "        num_words_per_course (int): Number of words to predict per course\n",
    "    Returns:\n",
    "        Course description dataframe with a new column for every predicted word \n",
    "    \"\"\"\n",
    "    df_with_keywords = course_descripts.copy()\n",
    "    # Obtain the softmax predictions for all instances\n",
    "    softmax_frame = course_vecs.iloc[:,1:].dot(trained_weights.values) + trained_biases \n",
    "\n",
    "    # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "    print('[INFO] Sorting classification results...')\n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_words_per_course:]\n",
    "\n",
    "    print('[INFO] Predicting top k inferred keywords for each course...')\n",
    "    for i in range(num_words_per_course):\n",
    "        print(i)\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        df_with_keywords['predicted_word_' + str(num_words_per_course-i)] = new_col.values\n",
    "        \n",
    "    return df_with_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "info_frame = pd.read_csv(infofile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(info_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = info_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())\n",
    "num_top_words = 10\n",
    "\n",
    "hyperparams_cols = ['use_idf', 'max_df','tf-bias', 'use_hidden_layer', 'num_epochs', 'recall@max_len', 'precision@10', 'distribution_diff', \n",
    "                    'document_frequency']\n",
    "\n",
    "\n",
    "param_grid = {'use_idf': [True, False],\n",
    "              'max_df': np.arange(.02, .06, .01),\n",
    "              'tf_bias': np.append(np.arange(0, 2.5, .5), -999),\n",
    "              'num_epochs': [5, 10], \n",
    "              'use_hidden_layer': [True, False]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "print(len(grid))\n",
    "# for params in grid:\n",
    "#     print(\"[HYPERPARAMS] use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d\" % \n",
    "#           (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 1.000000, tf_bias: 0.500000, use_hidden_layer: True, num_epochs: 5***\n",
      "======== [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 5333\n",
      "[INFO] Number of bigrams: 2666\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "4918/4918 [==============================] - 2s 326us/step - loss: 894.0033 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4918/4918 [==============================] - 1s 196us/step - loss: 853.1479 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4918/4918 [==============================] - 1s 192us/step - loss: 823.7145 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "4918/4918 [==============================] - 1s 193us/step - loss: 785.5295 - acc: 4.0667e-04\n",
      "Epoch 5/5\n",
      "4918/4918 [==============================] - 1s 203us/step - loss: 730.7886 - acc: 4.0667e-04\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.000144.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.000244.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Most common keywords by count:  [('aggression', 1363), ('achieves', 1267), ('amnesia', 822), ('amer', 797), ('aces', 764), ('anatomic', 695), ('abandoning', 618), ('ads', 529), ('anaylsis', 526), ('ac', 513)]\n",
      "[INFO] Fold 1 cosine similarity: 0.440980.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] Most common keywords by document frequencies:  [('aggression', 0.7289156626506024), ('achieves', 0.7168674698795181), ('amer', 0.6746987951807228), ('amnesia', 0.6024096385542169), ('aces', 0.5602409638554217), ('affirmation', 0.5240963855421686), ('anatomic', 0.5180722891566265), ('amulets', 0.4759036144578313), ('abandoning', 0.45180722891566266), ('anaylsis', 0.43373493975903615)]\n",
      "[INFO] Fold 1 document frequency: 0.113524.\n",
      "======== [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 5245\n",
      "[INFO] Number of bigrams: 2622\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "4918/4918 [==============================] - 2s 336us/step - loss: 887.8963 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4918/4918 [==============================] - 1s 193us/step - loss: 847.6669 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4918/4918 [==============================] - 1s 188us/step - loss: 822.5673 - acc: 2.0333e-04\n",
      "Epoch 4/5\n",
      "4918/4918 [==============================] - 1s 189us/step - loss: 789.2288 - acc: 2.0333e-04\n",
      "Epoch 5/5\n",
      "4918/4918 [==============================] - 1s 190us/step - loss: 741.7259 - acc: 4.0667e-04\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.000130.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.000285.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Most common keywords by count:  [('affiliate', 1064), ('aide', 1014), ('aerobics', 1010), ('aliens', 859), ('aeolian', 841), ('absorber', 758), ('admixtures', 737), ('ameliorating', 548), ('ahead', 397), ('aloof', 391)]\n",
      "[INFO] Fold 2 cosine similarity: 0.415978.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] Most common keywords by document frequencies:  [('affiliate', 0.7407407407407407), ('absorber', 0.6851851851851852), ('aide', 0.6728395061728395), ('aliens', 0.6604938271604939), ('admixtures', 0.6481481481481481), ('aeolian', 0.6049382716049383), ('aerobics', 0.6049382716049383), ('ameliorating', 0.6049382716049383), ('aloof', 0.5), ('alleviation', 0.4567901234567901)]\n",
      "[INFO] Fold 2 document frequency: 0.136432.\n",
      "======== [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 5111\n",
      "[INFO] Number of bigrams: 2555\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "4918/4918 [==============================] - 1s 300us/step - loss: 847.2336 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4918/4918 [==============================] - 1s 191us/step - loss: 810.7833 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4918/4918 [==============================] - 1s 193us/step - loss: 785.8034 - acc: 2.0333e-04\n",
      "Epoch 4/5\n",
      "4918/4918 [==============================] - 1s 195us/step - loss: 753.3700 - acc: 2.0333e-04\n",
      "Epoch 5/5\n",
      "4918/4918 [==============================] - 1s 192us/step - loss: 709.3134 - acc: 6.1000e-04\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.000128.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.000041.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Most common keywords by count:  [('amarna', 947), ('airports', 913), ('abstractly', 893), ('aniconism', 840), ('agroecological', 706), ('adversely', 694), ('agroecosystem', 542), ('ahead', 494), ('abs', 461), ('additives', 432)]\n",
      "[INFO] Fold 3 cosine similarity: 0.396185.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] Most common keywords by document frequencies:  [('agroecological', 0.6946107784431138), ('amarna', 0.6586826347305389), ('abstractly', 0.6227544910179641), ('airports', 0.6107784431137725), ('aniconism', 0.6047904191616766), ('ahead', 0.562874251497006), ('abs', 0.5508982035928144), ('agroecosystem', 0.5508982035928144), ('anonymity', 0.5149700598802395), ('analects', 0.5149700598802395)]\n",
      "[INFO] Fold 3 document frequency: 0.167755.\n",
      "   use_idf  max_df  tf-bias  use_hidden_layer  num_epochs  recall@max_len  \\\n",
      "0     True       1      0.5              True           5        0.000134   \n",
      "\n",
      "   precision@10  distribution_diff  document_frequency  \n",
      "0       0.00019           0.417714            0.139237  \n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 1.000000, tf_bias: 1.000000, use_hidden_layer: True, num_epochs: 5***\n",
      "======== [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] number unigrams: 5333\n",
      "[INFO] Number of bigrams: 2666\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "4918/4918 [==============================] - 2s 367us/step - loss: 90641.1931 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4918/4918 [==============================] - 1s 195us/step - loss: 86396.7066 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4918/4918 [==============================] - 1s 190us/step - loss: 83424.4609 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "4918/4918 [==============================] - 1s 194us/step - loss: 79602.7112 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "4918/4918 [==============================] - 1s 193us/step - loss: 74310.4138 - acc: 0.0018\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.000144.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.000122.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Most common keywords by count:  [('amazonia', 1079), ('alliance', 931), ('accelerate', 899), ('annually', 883), ('aggression', 857), ('activate', 852), ('alliteration', 746), ('aligned', 675), ('achieves', 652), ('angle', 522)]\n",
      "[INFO] Fold 1 cosine similarity: 0.434267.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] Most common keywords by document frequencies:  [('alliance', 0.6686746987951807), ('achieves', 0.5963855421686747), ('amazonia', 0.5903614457831325), ('annually', 0.5843373493975904), ('accelerate', 0.5783132530120482), ('abounds', 0.5421686746987951), ('aggression', 0.5301204819277109), ('activate', 0.5), ('angle', 0.4939759036144578), ('alliteration', 0.4759036144578313)]\n",
      "[INFO] Fold 1 document frequency: 0.120207.\n",
      "======== [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 5245\n",
      "[INFO] Number of bigrams: 2622\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "4918/4918 [==============================] - 2s 309us/step - loss: 90690.2937 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4918/4918 [==============================] - 1s 183us/step - loss: 86321.7315 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4918/4918 [==============================] - 1s 190us/step - loss: 83653.9994 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "4918/4918 [==============================] - 1s 193us/step - loss: 80369.3713 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "4918/4918 [==============================] - 1s 195us/step - loss: 76049.9186 - acc: 2.0333e-04\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.000148.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.000163.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Most common keywords by count:  [('airfields', 1156), ('anatomic', 1107), ('absolutely', 956), ('amorphous', 868), ('aiding', 803), ('alloys', 771), ('advisers', 699), ('afro', 561), ('additives', 475), ('ambitions', 435)]\n",
      "[INFO] Fold 2 cosine similarity: 0.431135.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] Most common keywords by document frequencies:  [('airfields', 0.7345679012345679), ('amorphous', 0.7222222222222222), ('anatomic', 0.7222222222222222), ('aiding', 0.6666666666666666), ('alloys', 0.5987654320987654), ('absolutely', 0.5987654320987654), ('afro', 0.5432098765432098), ('accumulation', 0.5370370370370371), ('additives', 0.5185185185185185), ('alberti', 0.47530864197530864)]\n",
      "[INFO] Fold 2 document frequency: 0.138620.\n",
      "======== [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 5111\n",
      "[INFO] Number of bigrams: 2555\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "4918/4918 [==============================] - 2s 319us/step - loss: 84410.1741 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4918/4918 [==============================] - 1s 195us/step - loss: 80713.3991 - acc: 2.0333e-04\n",
      "Epoch 3/5\n",
      "4918/4918 [==============================] - 1s 191us/step - loss: 78142.8648 - acc: 2.0333e-04\n",
      "Epoch 4/5\n",
      "4918/4918 [==============================] - 1s 194us/step - loss: 74880.4064 - acc: 4.0667e-04\n",
      "Epoch 5/5\n",
      "4918/4918 [==============================] - 1s 193us/step - loss: 70637.1645 - acc: 2.0333e-04\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.000133.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.000041.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Most common keywords by count:  [('agit', 957), ('accidents', 952), ('amps', 910), ('anisotropic', 776), ('agreement', 754), ('abut', 689), ('acquistion', 477), ('analizes', 455), ('accomplishments', 442), ('albeit', 399)]\n",
      "[INFO] Fold 3 cosine similarity: 0.382763.\n",
      "[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...\n",
      "[INFO] Most common keywords by document frequencies:  [('anisotropic', 0.6526946107784432), ('agit', 0.6526946107784432), ('accidents', 0.6407185628742516), ('amps', 0.6347305389221557), ('acquistion', 0.592814371257485), ('accomplishments', 0.5568862275449101), ('analizes', 0.5508982035928144), ('abut', 0.5449101796407185), ('agreement', 0.5329341317365269), ('allowable', 0.49700598802395207)]\n",
      "[INFO] Fold 3 document frequency: 0.178888.\n",
      "   use_idf  max_df  tf-bias  use_hidden_layer  num_epochs  recall@max_len  \\\n",
      "0     True       1      0.5              True           5        0.000134   \n",
      "1     True       1      1.0              True           5        0.000138   \n",
      "\n",
      "   precision@10  distribution_diff  document_frequency  \n",
      "0      0.000190           0.417714            0.139237  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      0.000149           0.416885            0.142571  \n"
     ]
    }
   ],
   "source": [
    "# simple parameter grid search\n",
    "param_grid = {'use_idf': [True],\n",
    "              'max_df': [1], # np.arange(0, .0055, .0005),\n",
    "              'use_hidden_layer': [True],\n",
    "              'tf_bias': np.arange(.5, 1.5, .5), \n",
    "              'num_epochs': [5]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "recall_validation_scores = []\n",
    "precision_validation_scores = []\n",
    "distribution_validation_scores = []\n",
    "document_frequency_validation_scores = []\n",
    "grid_search_data = []\n",
    "\n",
    "for params in grid:\n",
    "    print(\"***[INFO] Evaluating cross-validated model with hyperparams use_idf: %r, max_df: %f, tf_bias: %f, use_hidden_layer: %r, num_epochs: %d***\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['use_hidden_layer'], params['num_epochs']))\n",
    "\n",
    "    fold_num = 1\n",
    "    kf = KFold(n_splits=3, random_state=42) # DO NOT FIX RANDOM STATE WHEN RUNNING THE ACTUAL EXPERIMENT - NVM, should be fixed for reproducibility\n",
    "    for train_idx, valid_idx in kf.split(filtered_vec_df):\n",
    "        print('======== [INFO] Fold %d' % (fold_num))\n",
    "        # X = vectors, Y = descriptions\n",
    "        split_X_train, split_X_valid = filtered_vec_df.iloc[train_idx], filtered_vec_df.iloc[valid_idx]\n",
    "        split_Y_train, split_Y_valid = filtered_descript_df.iloc[train_idx], filtered_descript_df.iloc[valid_idx]\n",
    "\n",
    "        vocab = get_vocab(split_Y_train, textcolumn, max_df=params['max_df'], use_idf=params['use_idf']) \n",
    "        vocab_frame = pd.DataFrame(vocab)\n",
    "        vocabsize = len(vocab)\n",
    "\n",
    "        # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "        split_Y_train_BOW = to_bag_of_words(split_Y_train, textcolumn, vocab, tf_bias=params['tf_bias'], use_idf=params['use_idf'])\n",
    "        split_Y_train_BOW = split_Y_train_BOW.toarray()\n",
    "\n",
    "        (weights_frame, biases) = logistic_regression(split_X_train.iloc[:,1:], split_Y_train_BOW, \n",
    "                                                      use_hidden_layer=params['use_hidden_layer'], num_epochs=params['num_epochs'])\n",
    "\n",
    "        print('[INFO] Predicting on validation set for recall...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, vocab_frame, max_descript_len)\n",
    "        fold_i_average_recall = calculate_metric(df_with_keywords, 'r')\n",
    "        recall_validation_scores.append(fold_i_average_recall)\n",
    "        print('[INFO] Fold %d recall: %f.' % (fold_num, fold_i_average_recall))\n",
    "        \n",
    "        print('[INFO] Predicting on validation set for precision...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, vocab_frame, num_top_words)\n",
    "        fold_i_average_precision = calculate_metric(df_with_keywords, 'p')\n",
    "        precision_validation_scores.append(fold_i_average_precision)\n",
    "        print('[INFO] Fold %d precision: %f.' % (fold_num, fold_i_average_precision))\n",
    "        \n",
    "\n",
    "        fold_i_distribution_diff = calculate_metric(df_with_keywords, 'c')\n",
    "        distribution_validation_scores.append(fold_i_distribution_diff)\n",
    "        print('[INFO] Fold %d cosine similarity: %f.' % (fold_num, fold_i_distribution_diff))\n",
    "        \n",
    "        fold_i_document_frequency = calculate_metric(df_with_keywords, 'df')\n",
    "        document_frequency_validation_scores.append(fold_i_document_frequency)\n",
    "        print('[INFO] Fold %d document frequency: %f.' % (fold_num, fold_i_document_frequency))\n",
    "\n",
    "        fold_num += 1\n",
    "\n",
    "    recall_i = np.mean(recall_validation_scores)\n",
    "    precision_i = np.mean(precision_validation_scores)\n",
    "    distribution_diff_i = np.mean(distribution_validation_scores)\n",
    "    document_frequency_i = np.mean(document_frequency_validation_scores)\n",
    "\n",
    "    model_i_params = [params['use_idf'], params['max_df'], params['tf_bias'], params['use_hidden_layer'],\n",
    "                      params['num_epochs'], recall_i, precision_i, distribution_diff_i, document_frequency_i]\n",
    "\n",
    "#     model_i_params = pd.DataFrame([model_i_params], columns=hyperparams_cols)\n",
    "#     grid_search_df.append(model_i_params, sort = False)\n",
    "    grid_search_data.append(dict(zip(hyperparams_cols, model_i_params)))\n",
    "    grid_search_df = pd.DataFrame(grid_search_data, columns=hyperparams_cols) \n",
    "    print(grid_search_df)\n",
    "    # print('recall scores:', recall_validation_scores)\n",
    "    # print('precision scores:', precision_validation_scores)\n",
    "    # print('distribution scores:', distribution_validation_scores)\n",
    "    \n",
    "grid_search_df.to_csv('./scores/score_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use_idf</th>\n",
       "      <th>max_df</th>\n",
       "      <th>tf-bias</th>\n",
       "      <th>use_hidden_layer</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>recall@max_len</th>\n",
       "      <th>precision@10</th>\n",
       "      <th>distribution_diff</th>\n",
       "      <th>document_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.417714</td>\n",
       "      <td>0.139237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.416885</td>\n",
       "      <td>0.142571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   use_idf  max_df  tf-bias  use_hidden_layer  num_epochs  recall@max_len  \\\n",
       "0     True       1      0.5              True           5        0.000134   \n",
       "1     True       1      1.0              True           5        0.000138   \n",
       "\n",
       "   precision@10  distribution_diff  document_frequency  \n",
       "0      0.000190           0.417714            0.139237  \n",
       "1      0.000149           0.416885            0.142571  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
