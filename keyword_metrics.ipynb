{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "TRAINING_DIR = os.getcwd()\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(TRAINING_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = os.getcwd()\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(TRAINING_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'\n",
    "num_top_words = 10\n",
    "use_idf = True\n",
    "tf_bias = .5\n",
    "num_epochs = 5\n",
    "max_df = 0.0028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[UNIGRAMS] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[BIGRAMS] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(bigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[TRIGRAMS] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descrip_title(row):\n",
    "    punc_remover = str.maketrans('', '', string.punctuation)\n",
    "    lowered = row['descrip_title'].lower()\n",
    "    lowered_removed_punc = lowered.translate(punc_remover)\n",
    "    cleaned_set = set(lowered_removed_punc.split())\n",
    "    return cleaned_set\n",
    "\n",
    "def recall_keywords(row):\n",
    "    return row['description_title_set'].intersection(row['keywords_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def test(x=1):\n",
    "    print(x)\n",
    "test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Experiment with Only 1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_subject</th>\n",
       "      <th>course_alternative_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xart 98</td>\n",
       "      <td>Directed Group Study</td>\n",
       "      <td>This is a student-initiated course to be offer...</td>\n",
       "      <td>FPF-Art Practice</td>\n",
       "      <td>FPF-Art Practice 98 XART98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xanthro 2ac</td>\n",
       "      <td>Introduction to Archaeology</td>\n",
       "      <td>Prehistory and cultural growth. Introduction t...</td>\n",
       "      <td>FPF-Anthropology</td>\n",
       "      <td>FPF-Anthropology 2AC XANTHRO2AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xstat 2</td>\n",
       "      <td>Introduction to Statistics</td>\n",
       "      <td>Population and variables. Standard measures of...</td>\n",
       "      <td>FPF-Statistics</td>\n",
       "      <td>FPF-Statistics 2 XSTAT2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xmath 1b</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>Continuation of 1A. Techniques of integration;...</td>\n",
       "      <td>FPF-Mathematics</td>\n",
       "      <td>FPF-Mathematics 1B XMATH1B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xphilos 3</td>\n",
       "      <td>The Nature of Mind</td>\n",
       "      <td>Introduction to the philosophy of mind. Topics...</td>\n",
       "      <td>FPF-Philosophy</td>\n",
       "      <td>FPF-Philosophy 3 XPHILOS3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course_name                 course_title  \\\n",
       "0      Xart 98         Directed Group Study   \n",
       "1  Xanthro 2ac  Introduction to Archaeology   \n",
       "2      Xstat 2   Introduction to Statistics   \n",
       "3     Xmath 1b                     Calculus   \n",
       "4    Xphilos 3           The Nature of Mind   \n",
       "\n",
       "                                  course_description    course_subject  \\\n",
       "0  This is a student-initiated course to be offer...  FPF-Art Practice   \n",
       "1  Prehistory and cultural growth. Introduction t...  FPF-Anthropology   \n",
       "2  Population and variables. Standard measures of...    FPF-Statistics   \n",
       "3  Continuation of 1A. Techniques of integration;...   FPF-Mathematics   \n",
       "4  Introduction to the philosophy of mind. Topics...    FPF-Philosophy   \n",
       "\n",
       "          course_alternative_names  \n",
       "0       FPF-Art Practice 98 XART98  \n",
       "1  FPF-Anthropology 2AC XANTHRO2AC  \n",
       "2          FPF-Statistics 2 XSTAT2  \n",
       "3       FPF-Mathematics 1B XMATH1B  \n",
       "4        FPF-Philosophy 3 XPHILOS3  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "info_frame = pd.read_csv(infofile, sep = '\\t') # Course information\n",
    "info_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_frame['abbr_cid'] = info_frame.course_name.str.replace(' ', '_').str.upper()\n",
    "# api_df = pd.read_csv('/home/matthew/Models-AskOski/shared/course_info.tsv', sep = '\\t', ).drop(['Unnamed: 0', 'idx', 'updated_date'],axis=1)\n",
    "# api_df.head(5) \n",
    "# any(api_df.course_subject.isnull())\n",
    "# temp = pd.merge(info_frame, api_df, on='abbr_cid')[['course_name', 'course_title', 'course_description', 'course_alternative_names', 'course_subject']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonempty_indices = np.where(info_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = info_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5901 1476\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(filtered_vec_df, filtered_descript_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Getting vocab...\n",
      "[UNIGRAMS] Number of unigrams: 11580\n",
      "[BIGRAMS] Number of bigrams: 1158\n",
      "[TRIGRAMS] Number of trigrams: 115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(Y_train, textcolumn) # get_vocab(raw_frame, textcolumn) \n",
    "vocab_frame = pd.DataFrame(vocab)\n",
    "    \n",
    "vocabsize = len(vocab)\n",
    "\n",
    "# Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "Y_train_BOW = to_bag_of_words(Y_train, textcolumn, vocab)\n",
    "Y_train_BOW = Y_train_BOW.toarray()\n",
    "Y_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 428us/step - loss: 18854.7786 - acc: 0.0105\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 312us/step - loss: 17371.5825 - acc: 0.0525\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 313us/step - loss: 16545.1459 - acc: 0.0754\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 316us/step - loss: 15840.3174 - acc: 0.0947\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 320us/step - loss: 15214.7901 - acc: 0.1132\n"
     ]
    }
   ],
   "source": [
    "(weights_frame, biases) = logistic_regression(X_train.iloc[:,1:], Y_train_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n"
     ]
    }
   ],
   "source": [
    "softmax_frame = X_test.iloc[:,1:].dot(weights_frame.values) + biases\n",
    "print('[INFO] Sorting classification results...')\n",
    "sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "predicted_keyword_list = []\n",
    "for i in range(num_top_words):\n",
    "    new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "    predicted_keyword_list.extend(new_col.values)\n",
    "    Y_test['predicted_word_' + str(num_top_words-i)] = new_col.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_subject</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_9</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>Theater 52ac</td>\n",
       "      <td>Dance in American Cultures</td>\n",
       "      <td>Dance as a meaning-making expressive form.  De...</td>\n",
       "      <td>Theater Dance &amp; Perf Stds</td>\n",
       "      <td>Theater Dance &amp; Perf Stds 52AC THEATER52AC</td>\n",
       "      <td>explorations</td>\n",
       "      <td>theatre</td>\n",
       "      <td>recording</td>\n",
       "      <td>formations</td>\n",
       "      <td>allowing</td>\n",
       "      <td>performing</td>\n",
       "      <td>performances</td>\n",
       "      <td>supplies</td>\n",
       "      <td>vocal</td>\n",
       "      <td>productions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>Slavic 138</td>\n",
       "      <td>Topics in Russian and Soviet Film</td>\n",
       "      <td>This course will examine the Russian contribut...</td>\n",
       "      <td>Slavic Languages &amp; Lit</td>\n",
       "      <td>Slavic Languages &amp; Lit 138 SLAVIC138</td>\n",
       "      <td>folklore</td>\n",
       "      <td>looking</td>\n",
       "      <td>aspect</td>\n",
       "      <td>soviet</td>\n",
       "      <td>beginner</td>\n",
       "      <td>avant</td>\n",
       "      <td>viewing</td>\n",
       "      <td>garde</td>\n",
       "      <td>russia</td>\n",
       "      <td>slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>Art 301</td>\n",
       "      <td>The Teaching of Art: Practice</td>\n",
       "      <td>Utilizing aspects of pedagogical and andragogi...</td>\n",
       "      <td>Art Practice</td>\n",
       "      <td>Art Practice 301 ART301</td>\n",
       "      <td>notion</td>\n",
       "      <td>communicating</td>\n",
       "      <td>centered</td>\n",
       "      <td>print</td>\n",
       "      <td>audience</td>\n",
       "      <td>avant</td>\n",
       "      <td>garde</td>\n",
       "      <td>performances</td>\n",
       "      <td>recording</td>\n",
       "      <td>talks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>Env sci 100</td>\n",
       "      <td>Introduction to the Methods of Environmental S...</td>\n",
       "      <td>Introduction to basic methods used in environm...</td>\n",
       "      <td>Environmental Sciences</td>\n",
       "      <td>Environmental Sciences 100 ENV SCI100</td>\n",
       "      <td>remote</td>\n",
       "      <td>fate</td>\n",
       "      <td>greatest</td>\n",
       "      <td>habitat</td>\n",
       "      <td>trip</td>\n",
       "      <td>insects</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>geologic</td>\n",
       "      <td>espm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>Nuc eng 221</td>\n",
       "      <td>Corrosion in Nuclear Power Systems</td>\n",
       "      <td>Structural metals in nuclear power plants; pro...</td>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Nuclear Engineering 221 NUC ENG221</td>\n",
       "      <td>calculations</td>\n",
       "      <td>radioactive</td>\n",
       "      <td>fast</td>\n",
       "      <td>neutron</td>\n",
       "      <td>beam</td>\n",
       "      <td>fission</td>\n",
       "      <td>reactors</td>\n",
       "      <td>fuel</td>\n",
       "      <td>fusion</td>\n",
       "      <td>reactor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       course_name                                       course_title  \\\n",
       "2288  Theater 52ac                         Dance in American Cultures   \n",
       "6602    Slavic 138                  Topics in Russian and Soviet Film   \n",
       "4483       Art 301                      The Teaching of Art: Practice   \n",
       "5321   Env sci 100  Introduction to the Methods of Environmental S...   \n",
       "5447   Nuc eng 221                 Corrosion in Nuclear Power Systems   \n",
       "\n",
       "                                     course_description  \\\n",
       "2288  Dance as a meaning-making expressive form.  De...   \n",
       "6602  This course will examine the Russian contribut...   \n",
       "4483  Utilizing aspects of pedagogical and andragogi...   \n",
       "5321  Introduction to basic methods used in environm...   \n",
       "5447  Structural metals in nuclear power plants; pro...   \n",
       "\n",
       "                 course_subject                    course_alternative_names  \\\n",
       "2288  Theater Dance & Perf Stds  Theater Dance & Perf Stds 52AC THEATER52AC   \n",
       "6602     Slavic Languages & Lit        Slavic Languages & Lit 138 SLAVIC138   \n",
       "4483               Art Practice                     Art Practice 301 ART301   \n",
       "5321     Environmental Sciences       Environmental Sciences 100 ENV SCI100   \n",
       "5447        Nuclear Engineering          Nuclear Engineering 221 NUC ENG221   \n",
       "\n",
       "     predicted_word_10 predicted_word_9 predicted_word_8 predicted_word_7  \\\n",
       "2288      explorations          theatre        recording       formations   \n",
       "6602          folklore          looking           aspect           soviet   \n",
       "4483            notion    communicating         centered            print   \n",
       "5321            remote             fate         greatest          habitat   \n",
       "5447      calculations      radioactive             fast          neutron   \n",
       "\n",
       "     predicted_word_6 predicted_word_5 predicted_word_4 predicted_word_3  \\\n",
       "2288         allowing       performing     performances         supplies   \n",
       "6602         beginner            avant          viewing            garde   \n",
       "4483         audience            avant            garde     performances   \n",
       "5321             trip          insects     agricultural         wildlife   \n",
       "5447             beam          fission         reactors             fuel   \n",
       "\n",
       "     predicted_word_2 predicted_word_1  \n",
       "2288            vocal      productions  \n",
       "6602           russia           slavic  \n",
       "4483        recording            talks  \n",
       "5321         geologic             espm  \n",
       "5447           fusion          reactor  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_word_1</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course_subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Engineering</th>\n",
       "      <td>sponsoring</td>\n",
       "      <td>income</td>\n",
       "      <td>professionally</td>\n",
       "      <td>matlab</td>\n",
       "      <td>homework</td>\n",
       "      <td>layout</td>\n",
       "      <td>dependent</td>\n",
       "      <td>advisor</td>\n",
       "      <td>quickly</td>\n",
       "      <td>allocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>powers</td>\n",
       "      <td>rule</td>\n",
       "      <td>edu</td>\n",
       "      <td>russia</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>rome</td>\n",
       "      <td>trace</td>\n",
       "      <td>evolved</td>\n",
       "      <td>looking</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Health</th>\n",
       "      <td>concern</td>\n",
       "      <td>survival</td>\n",
       "      <td>fertility</td>\n",
       "      <td>demographic</td>\n",
       "      <td>residents</td>\n",
       "      <td>demography</td>\n",
       "      <td>credential</td>\n",
       "      <td>influenced</td>\n",
       "      <td>multivariate</td>\n",
       "      <td>generalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evening &amp; Weekend MBA</th>\n",
       "      <td>arbitrage</td>\n",
       "      <td>dependent</td>\n",
       "      <td>company</td>\n",
       "      <td>managed</td>\n",
       "      <td>workers</td>\n",
       "      <td>resulting</td>\n",
       "      <td>enabling</td>\n",
       "      <td>drug</td>\n",
       "      <td>practiced</td>\n",
       "      <td>diagnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>did</td>\n",
       "      <td>socialism</td>\n",
       "      <td>powers</td>\n",
       "      <td>cold</td>\n",
       "      <td>rural</td>\n",
       "      <td>trace</td>\n",
       "      <td>formations</td>\n",
       "      <td>modernism</td>\n",
       "      <td>struggles</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      predicted_word_1 predicted_word_10 predicted_word_2  \\\n",
       "course_subject                                                              \n",
       "Engineering                 sponsoring            income   professionally   \n",
       "History                         powers              rule              edu   \n",
       "Public Health                  concern          survival        fertility   \n",
       "Evening & Weekend MBA        arbitrage         dependent          company   \n",
       "History                            did         socialism           powers   \n",
       "\n",
       "                      predicted_word_3 predicted_word_4 predicted_word_5  \\\n",
       "course_subject                                                             \n",
       "Engineering                     matlab         homework           layout   \n",
       "History                         russia     agricultural             rome   \n",
       "Public Health              demographic        residents       demography   \n",
       "Evening & Weekend MBA          managed          workers        resulting   \n",
       "History                           cold            rural            trace   \n",
       "\n",
       "                      predicted_word_6 predicted_word_7 predicted_word_8  \\\n",
       "course_subject                                                             \n",
       "Engineering                  dependent          advisor          quickly   \n",
       "History                          trace          evolved          looking   \n",
       "Public Health               credential       influenced     multivariate   \n",
       "Evening & Weekend MBA         enabling             drug        practiced   \n",
       "History                     formations        modernism        struggles   \n",
       "\n",
       "                      predicted_word_9  \n",
       "course_subject                          \n",
       "Engineering                 allocation  \n",
       "History                           cold  \n",
       "Public Health              generalized  \n",
       "Evening & Weekend MBA       diagnostic  \n",
       "History                          peace  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freq_df_cols = Y_test.columns.difference(['course_title', 'course_description', 'course_name', 'course_alternative_names'])\n",
    "doc_df = Y_test.loc[:,doc_freq_df_cols]\n",
    "test = doc_df.loc[doc_df.course_subject.str.contains('History')]\n",
    "doc_df.set_index('course_subject', inplace=True)\n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_word_1</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course_subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>powers</td>\n",
       "      <td>rule</td>\n",
       "      <td>edu</td>\n",
       "      <td>russia</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>rome</td>\n",
       "      <td>trace</td>\n",
       "      <td>evolved</td>\n",
       "      <td>looking</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>did</td>\n",
       "      <td>socialism</td>\n",
       "      <td>powers</td>\n",
       "      <td>cold</td>\n",
       "      <td>rural</td>\n",
       "      <td>trace</td>\n",
       "      <td>formations</td>\n",
       "      <td>modernism</td>\n",
       "      <td>struggles</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>iran</td>\n",
       "      <td>performing</td>\n",
       "      <td>france</td>\n",
       "      <td>did</td>\n",
       "      <td>crisis</td>\n",
       "      <td>west</td>\n",
       "      <td>evolved</td>\n",
       "      <td>supplies</td>\n",
       "      <td>enlightenment</td>\n",
       "      <td>republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History of Art</th>\n",
       "      <td>economists</td>\n",
       "      <td>allocation</td>\n",
       "      <td>square</td>\n",
       "      <td>investigating</td>\n",
       "      <td>correlation</td>\n",
       "      <td>income</td>\n",
       "      <td>outcome</td>\n",
       "      <td>solubility</td>\n",
       "      <td>fair</td>\n",
       "      <td>optometric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History of Art</th>\n",
       "      <td>doe</td>\n",
       "      <td>france</td>\n",
       "      <td>listings</td>\n",
       "      <td>formations</td>\n",
       "      <td>audience</td>\n",
       "      <td>spain</td>\n",
       "      <td>desirable</td>\n",
       "      <td>ca</td>\n",
       "      <td>ideological</td>\n",
       "      <td>modernism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted_word_1 predicted_word_10 predicted_word_2  \\\n",
       "course_subject                                                       \n",
       "History                  powers              rule              edu   \n",
       "History                     did         socialism           powers   \n",
       "History                    iran        performing           france   \n",
       "History of Art       economists        allocation           square   \n",
       "History of Art              doe            france         listings   \n",
       "\n",
       "               predicted_word_3 predicted_word_4 predicted_word_5  \\\n",
       "course_subject                                                      \n",
       "History                  russia     agricultural             rome   \n",
       "History                    cold            rural            trace   \n",
       "History                     did           crisis             west   \n",
       "History of Art    investigating      correlation           income   \n",
       "History of Art       formations         audience            spain   \n",
       "\n",
       "               predicted_word_6 predicted_word_7 predicted_word_8  \\\n",
       "course_subject                                                      \n",
       "History                   trace          evolved          looking   \n",
       "History              formations        modernism        struggles   \n",
       "History                 evolved         supplies    enlightenment   \n",
       "History of Art          outcome       solubility             fair   \n",
       "History of Art        desirable               ca      ideological   \n",
       "\n",
       "               predicted_word_9  \n",
       "course_subject                   \n",
       "History                    cold  \n",
       "History                   peace  \n",
       "History                republic  \n",
       "History of Art       optometric  \n",
       "History of Art        modernism  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.set_index('course_subject', inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict = defaultdict(list)\n",
    "terms = set()\n",
    "for index,row in doc_df.iterrows():\n",
    "    document_dict[index].extend(row.values)\n",
    "    terms.update(row.values)\n",
    "# doc_freq_dict\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs = len(document_dict.keys())\n",
    "doc_freq_i = 0\n",
    "for key in document_dict.keys():\n",
    "    if 'performing' in document_dict.get(key):\n",
    "        doc_freq_i += 1\n",
    "#         print(doc_freq_i)\n",
    "doc_freq_i / num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_freq_dict = defaultdict()\n",
    "num_docs = len(document_dict.keys())\n",
    "for term in terms:\n",
    "    doc_freq_i = 0\n",
    "#     print(term)\n",
    "    for key in document_dict.keys():\n",
    "        if term in document_dict.get(key):\n",
    "            doc_freq_i += 1\n",
    "    doc_freq_dict[term] = doc_freq_i / (num_docs + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('powers', 0.2822085889570552),\n",
       " ('formations', 0.27607361963190186),\n",
       " ('performing', 0.25766871165644173),\n",
       " ('agricultural', 0.24539877300613497),\n",
       " ('trace', 0.22085889570552147),\n",
       " ('informed', 0.2147239263803681),\n",
       " ('crisis', 0.20245398773006135),\n",
       " ('did', 0.17791411042944785),\n",
       " ('modernism', 0.17791411042944785),\n",
       " ('recording', 0.17791411042944785)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(doc_freq_dict).most_common(10)\n",
    "# sorted(doc_freq_dict, key=doc_freq_dict.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trace', 150),\n",
       " ('powers', 148),\n",
       " ('formations', 133),\n",
       " ('agricultural', 111),\n",
       " ('performing', 108),\n",
       " ('struggles', 92),\n",
       " ('looking', 91),\n",
       " ('income', 90),\n",
       " ('authority', 86),\n",
       " ('crisis', 84),\n",
       " ('did', 80),\n",
       " ('chronological', 72),\n",
       " ('evolved', 71),\n",
       " ('modernism', 69),\n",
       " ('republic', 68)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keyword_counter = Counter(predicted_keyword_list)\n",
    "\n",
    "keyword_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACADEMIC_DEPARTMENT_NAME</th>\n",
       "      <th>ACADEMIC_DIVISION_NAME</th>\n",
       "      <th>MAJOR_NAME</th>\n",
       "      <th>COLLEGE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>African American Studies</td>\n",
       "      <td>L&amp;S Social Sciences Division</td>\n",
       "      <td>Afr Amer Stds-Humanities</td>\n",
       "      <td>Clg of Letters &amp; Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>African American Studies</td>\n",
       "      <td>L&amp;S Social Sciences Division</td>\n",
       "      <td>Afr Amer Stds-Social Sci</td>\n",
       "      <td>Clg of Letters &amp; Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>African American Studies</td>\n",
       "      <td>L&amp;S Social Sciences Division</td>\n",
       "      <td>African American Studies</td>\n",
       "      <td>Clg of Letters &amp; Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ag &amp; Env Chem Grad Grp</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "      <td>Ag &amp; Environmental Chem</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ag &amp; Resource Econ &amp; Pol</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "      <td>Ag &amp; Resource Economics</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACADEMIC_DEPARTMENT_NAME        ACADEMIC_DIVISION_NAME  \\\n",
       "0  African American Studies  L&S Social Sciences Division   \n",
       "1  African American Studies  L&S Social Sciences Division   \n",
       "2  African American Studies  L&S Social Sciences Division   \n",
       "3    Ag & Env Chem Grad Grp      Clg of Natural Resources   \n",
       "4  Ag & Resource Econ & Pol      Clg of Natural Resources   \n",
       "\n",
       "                 MAJOR_NAME              COLLEGE_NAME  \n",
       "0  Afr Amer Stds-Humanities  Clg of Letters & Science  \n",
       "1  Afr Amer Stds-Social Sci  Clg of Letters & Science  \n",
       "2  African American Studies  Clg of Letters & Science  \n",
       "3   Ag & Environmental Chem  Clg of Natural Resources  \n",
       "4   Ag & Resource Economics  Clg of Natural Resources  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_file = infofile = os.path.join(TRAINING_DIR, 'academic_departments.tsv')\n",
    "dpt_df = pd.read_csv(dpt_file, sep='\\t')\n",
    "dpt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACADEMIC_DEPARTMENT_NAME</th>\n",
       "      <th>ACADEMIC_DIVISION_NAME</th>\n",
       "      <th>MAJOR_NAME</th>\n",
       "      <th>COLLEGE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bioengineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Bioengineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bioengineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Translational Medicine</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioengineering-UCSF Grad Grp</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Bioengineering (UCSF)</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bioengineering-UCSF Grad Grp</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Translational Medicine (UCSF)</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>BioE/MSE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>EECS/MSE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>EECS/NE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>ME/NE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>MSE/ME Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>MSE/NE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Computational Eng Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Energy Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Engineering Physics</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Engineering Undeclared</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Eng Math &amp; Statistics</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Environmental Eng Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Manufacturing Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Mechanical Engineering PT MEng</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Nuclear Engineering PT MEng</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ACADEMIC_DEPARTMENT_NAME ACADEMIC_DIVISION_NAME  \\\n",
       "15                 Bioengineering     Clg of Engineering   \n",
       "16                 Bioengineering     Clg of Engineering   \n",
       "17   Bioengineering-UCSF Grad Grp     Clg of Engineering   \n",
       "18   Bioengineering-UCSF Grad Grp     Clg of Engineering   \n",
       "70     Engineering Joint Programs     Clg of Engineering   \n",
       "71     Engineering Joint Programs     Clg of Engineering   \n",
       "72     Engineering Joint Programs     Clg of Engineering   \n",
       "73     Engineering Joint Programs     Clg of Engineering   \n",
       "74     Engineering Joint Programs     Clg of Engineering   \n",
       "75     Engineering Joint Programs     Clg of Engineering   \n",
       "76            Engineering Science     Clg of Engineering   \n",
       "77            Engineering Science     Clg of Engineering   \n",
       "78            Engineering Science     Clg of Engineering   \n",
       "79            Engineering Science     Clg of Engineering   \n",
       "80            Engineering Science     Clg of Engineering   \n",
       "81            Engineering Science     Clg of Engineering   \n",
       "82            Engineering Science     Clg of Engineering   \n",
       "167        Mechanical Engineering     Clg of Engineering   \n",
       "168        Mechanical Engineering     Clg of Engineering   \n",
       "190           Nuclear Engineering     Clg of Engineering   \n",
       "191           Nuclear Engineering     Clg of Engineering   \n",
       "\n",
       "                         MAJOR_NAME        COLLEGE_NAME  \n",
       "15                   Bioengineering  Clg of Engineering  \n",
       "16           Translational Medicine  Clg of Engineering  \n",
       "17            Bioengineering (UCSF)  Clg of Engineering  \n",
       "18    Translational Medicine (UCSF)  Clg of Engineering  \n",
       "70             BioE/MSE Joint Major  Clg of Engineering  \n",
       "71             EECS/MSE Joint Major  Clg of Engineering  \n",
       "72              EECS/NE Joint Major  Clg of Engineering  \n",
       "73                ME/NE Joint Major  Clg of Engineering  \n",
       "74               MSE/ME Joint Major  Clg of Engineering  \n",
       "75               MSE/NE Joint Major  Clg of Engineering  \n",
       "76        Computational Eng Science  Clg of Engineering  \n",
       "77               Energy Engineering  Clg of Engineering  \n",
       "78              Engineering Physics  Clg of Engineering  \n",
       "79           Engineering Undeclared  Clg of Engineering  \n",
       "80            Eng Math & Statistics  Clg of Engineering  \n",
       "81        Environmental Eng Science  Clg of Engineering  \n",
       "82        Manufacturing Engineering  Clg of Engineering  \n",
       "167          Mechanical Engineering  Clg of Engineering  \n",
       "168  Mechanical Engineering PT MEng  Clg of Engineering  \n",
       "190             Nuclear Engineering  Clg of Engineering  \n",
       "191     Nuclear Engineering PT MEng  Clg of Engineering  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_df.loc[dpt_df.ACADEMIC_DEPARTMENT_NAME.str.contains('engineering', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_possible_keywords = Y_test.shape[0] * num_top_words\n",
    "num_predicted_keywords = len(keyword_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(keyword_counter.values()) == Y_test.shape[0] * num_top_words,\\\n",
    "'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.97012302, 12.97012302, 12.97012302, ..., 12.97012302,\n",
       "       12.97012302, 12.97012302])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "unif_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41,  1,  1, ...,  2,  5, 48])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "predicted_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unif_keyword_vector.shape == predicted_keyword_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3620480643963293"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n",
    "\n",
    "cosine_similarity(predicted_keyword_vector, unif_keyword_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([1,-1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "TRAINING_DIR = os.getcwd()\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(TRAINING_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(course_vecs, course_descipts, trained_weights, trained_biases, num_words_per_course):\n",
    "    \"\"\"\n",
    "    lalalal\n",
    "    \n",
    "    \"\"\"\n",
    "    df_with_keywords = course_descipts.copy()\n",
    "    softmax_frame = course_vecs.iloc[:,1:].dot(trained_weights.values) + trained_biases # make predictions\n",
    "\n",
    "    # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "    print('[INFO] Sorting classification results...')\n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_words_per_course:]\n",
    "\n",
    "    print('[INFO] Predicting top k inferred keywords for each course...')\n",
    "    for i in range(num_words_per_course):\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        df_with_keywords['predicted_word_' + str(num_words_per_course-i)] = new_col.values\n",
    "        \n",
    "    return df_with_keywords\n",
    "\n",
    "def calculate_metric(df_with_keywords, metric):\n",
    "    \"\"\"\n",
    "    metrics: {r: recall, p: precision}\n",
    "    \"\"\"\n",
    "    def clean_descrip_title(row):\n",
    "        punc_remover = str.maketrans('', '', string.punctuation)\n",
    "        lowered = row['descrip_title'].lower()\n",
    "        lowered_removed_punc = lowered.translate(punc_remover)\n",
    "        cleaned_set = set(lowered_removed_punc.split())\n",
    "        return cleaned_set\n",
    "\n",
    "    def recall_keywords(row):\n",
    "        return row['description_title_set'].intersection(row['course_keywords_set'])\n",
    "    \n",
    "    prediction_df = df_with_keywords.copy()\n",
    "    only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "    num_keywords_predicted = only_predicted_keywords_df.shape[1]\n",
    "    prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "    prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "    prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "    \n",
    "    if metric == 'r':\n",
    "        print('[INFO] Calculating Recall...')\n",
    "        assert num_keywords_predicted == max_descript_len, 'Number of keywords predicted should equal longest description length'\n",
    "        prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "        average_recall = np.mean(prediction_df['recall'])\n",
    "        return average_recall\n",
    "    if metric == 'p':\n",
    "        print('[INFO] Calculating Precision...')\n",
    "        assert num_keywords_predicted == num_top_words, 'Number of keywords predicted should equal number of predicted words per course'\n",
    "        prediction_df['precision'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / num_top_words)\n",
    "        average_precision = np.mean(prediction_df['precision'])\n",
    "        return average_precision\n",
    "    if metric == 'c':\n",
    "        print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "        predicted_keyword_list = only_predicted_keywords_df.values.tolist()\n",
    "        predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "        keyword_counter = Counter(predicted_keyword_list)\n",
    "        print('[DEBUG] most common keywords: ', keyword_counter.most_common(10))\n",
    "        \n",
    "        num_possible_keywords = df_with_keywords.shape[0] * num_top_words\n",
    "        num_predicted_keywords = len(keyword_counter.keys())\n",
    "        assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "        'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "        unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "        predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "        assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "        'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "    \n",
    "        cos_sim = cosine_similarity(predicted_keyword_vector, unif_keyword_vector)\n",
    "        return cos_sim\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column, max_df=0.0028, use_idf=True):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(bigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab, tf_bias=.5, use_idf=True):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, num_epochs=1):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "info_frame = pd.read_csv(infofile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(info_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = info_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())\n",
    "num_top_words = 10\n",
    "\n",
    "hyperparams_cols = ['use_idf', 'max_df','tf-bias', 'num_epochs', 'recall', 'precision', 'distribution_diff']\n",
    "grid_search_df = pd.DataFrame(columns=hyperparams_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 1.000000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 1.000000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.004000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.004000, tf_bias: 1.000000, num_epochs: 5\n"
     ]
    }
   ],
   "source": [
    "np.arange(0.002, .005, .001)\n",
    "\n",
    "param_grid = {'use_idf': [True],\n",
    "              'max_df': np.arange(0.002, .005, .001), # np.arange(0, .0055, .0005)\n",
    "              'tf_bias': np.arange(.5, 1.5, .5), \n",
    "              'num_epochs': [5]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in grid:\n",
    "    print(\"[HYPERPARAMS] use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.002000, tf_bias: 0.500000, num_epochs: 5***\n",
      "\t [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11379\n",
      "[INFO] Number of bigrams: 1137\n",
      "[INFO] Number of trigrams: 113\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 2s 395us/step - loss: 14005.7337 - acc: 0.0034\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 274us/step - loss: 13148.1320 - acc: 0.0312\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 270us/step - loss: 12619.1936 - acc: 0.0671\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 269us/step - loss: 12117.7610 - acc: 0.0954\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 270us/step - loss: 11659.2431 - acc: 0.1185\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.004567.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.020054.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('optometric', 222), ('investor', 207), ('masters', 206), ('designated', 199), ('arbitrage', 197), ('board', 197), ('pharmacology', 196), ('activism', 193), ('returns', 179), ('phases', 176)]\n",
      "[INFO] Fold 1 cosine similarity: 0.444282.\n",
      "\t [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11214\n",
      "[INFO] Number of bigrams: 1121\n",
      "[INFO] Number of trigrams: 112\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 387us/step - loss: 13424.8618 - acc: 0.0019\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 263us/step - loss: 12615.0271 - acc: 0.0320\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 269us/step - loss: 12102.2007 - acc: 0.0651\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 264us/step - loss: 11631.7935 - acc: 0.0973\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 265us/step - loss: 11182.9652 - acc: 0.1198\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.004746.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.022644.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('designated', 258), ('rna', 257), ('derivative', 257), ('tandem', 257), ('optometric', 257), ('chapter', 255), ('mergers', 216), ('binocular', 207), ('physician', 202), ('consulting', 182)]\n",
      "[INFO] Fold 2 cosine similarity: 0.452868.\n",
      "\t [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11104\n",
      "[INFO] Number of bigrams: 1110\n",
      "[INFO] Number of trigrams: 111\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 371us/step - loss: 13123.4817 - acc: 0.0015\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 261us/step - loss: 12337.6791 - acc: 0.0334\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 261us/step - loss: 11854.0711 - acc: 0.0659\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 263us/step - loss: 11380.6983 - acc: 0.0925\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 298us/step - loss: 10963.9024 - acc: 0.1198\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.004323.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.020678.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('customer', 313), ('chapter', 312), ('binocular', 312), ('refractive', 312), ('injury', 312), ('returns', 312), ('pharmacology', 312), ('derivative', 312), ('arbitrage', 311), ('investments', 225)]\n",
      "[INFO] Fold 3 cosine similarity: 0.370080.\n",
      "\t [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 10993\n",
      "[INFO] Number of bigrams: 1099\n",
      "[INFO] Number of trigrams: 109\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 344us/step - loss: 13081.4006 - acc: 0.0022\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 258us/step - loss: 12294.4997 - acc: 0.0330\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 265us/step - loss: 11822.7923 - acc: 0.0664\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 267us/step - loss: 11357.9855 - acc: 0.0932\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 258us/step - loss: 10956.0117 - acc: 0.1144\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.003899.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.016271.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('binocular', 351), ('masters', 351), ('managed', 350), ('refractive', 350), ('securities', 339), ('revise', 330), ('returns', 314), ('optometry', 301), ('funds', 252), ('prognosis', 219)]\n",
      "[INFO] Fold 4 cosine similarity: 0.358814.\n",
      "\t [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11094\n",
      "[INFO] Number of bigrams: 1109\n",
      "[INFO] Number of trigrams: 110\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 355us/step - loss: 13299.9447 - acc: 0.0031\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 1s 250us/step - loss: 12455.7105 - acc: 0.0330\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 1s 251us/step - loss: 11961.0365 - acc: 0.0746\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 255us/step - loss: 11472.3502 - acc: 0.0957\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 255us/step - loss: 11015.9260 - acc: 0.1227\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.002963.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.011593.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('prognosis', 539), ('optometry', 537), ('arbitrage', 537), ('designated', 537), ('securities', 537), ('binocular', 537), ('masters', 537), ('investments', 529), ('diagnostic', 493), ('revise', 309)]\n",
      "[INFO] Fold 5 cosine similarity: 0.241448.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004099   0.018248           0.373498\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.002000, tf_bias: 1.000000, num_epochs: 5***\n",
      "\t [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11379\n",
      "[INFO] Number of bigrams: 1137\n",
      "[INFO] Number of trigrams: 113\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 2s 372us/step - loss: 4407005.0203 - acc: 0.0031\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 270us/step - loss: 4126807.0047 - acc: 0.0259\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 267us/step - loss: 3961727.5364 - acc: 0.0610\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 266us/step - loss: 3799175.4159 - acc: 0.0842\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 270us/step - loss: 3662107.2341 - acc: 0.1037\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.004237.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.018293.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('struggle', 273), ('actuators', 242), ('optometric', 216), ('investor', 208), ('masters', 208), ('entrepreneur', 205), ('designated', 201), ('expressive', 199), ('arbitrage', 198), ('phases', 197)]\n",
      "[INFO] Fold 1 cosine similarity: 0.438512.\n",
      "\t [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11214\n",
      "[INFO] Number of bigrams: 1121\n",
      "[INFO] Number of trigrams: 112\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 412us/step - loss: 4147804.5202 - acc: 0.0042\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 258us/step - loss: 3888799.2250 - acc: 0.0314\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 255us/step - loss: 3733902.3012 - acc: 0.0515\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 263us/step - loss: 3581789.5815 - acc: 0.0812\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 266us/step - loss: 3442219.2864 - acc: 0.1105\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.004622.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.020678.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('derivative', 258), ('rna', 257), ('designated', 257), ('securities', 257), ('neurons', 257), ('tandem', 257), ('optometric', 257), ('arbitrage', 257), ('refractive', 254), ('extreme', 220)]\n",
      "[INFO] Fold 2 cosine similarity: 0.419808.\n",
      "\t [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11104\n",
      "[INFO] Number of bigrams: 1110\n",
      "[INFO] Number of trigrams: 111\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 410us/step - loss: 4024134.9552 - acc: 0.0027\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 257us/step - loss: 3774417.9978 - acc: 0.0247\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 257us/step - loss: 3618326.9563 - acc: 0.0561\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 266us/step - loss: 3477347.2291 - acc: 0.0849\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 263us/step - loss: 3342933.3190 - acc: 0.1047\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.003985.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.017898.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('chapter', 312), ('prospectus', 312), ('binocular', 312), ('refractive', 312), ('injury', 312), ('pharmacology', 312), ('derivative', 312), ('returns', 311), ('investments', 311), ('stratification', 186)]\n",
      "[INFO] Fold 3 cosine similarity: 0.369343.\n",
      "\t [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 10993\n",
      "[INFO] Number of bigrams: 1099\n",
      "[INFO] Number of trigrams: 109\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 384us/step - loss: 4021017.9328 - acc: 0.0024\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 259us/step - loss: 3770313.8318 - acc: 0.0276\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 1s 254us/step - loss: 3614915.7559 - acc: 0.0510\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 257us/step - loss: 3479119.9052 - acc: 0.0852\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 1s 253us/step - loss: 3349955.6771 - acc: 0.1085\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.003918.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.015729.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('masters', 351), ('managed', 350), ('refractive', 350), ('emerged', 350), ('neurons', 350), ('binocular', 350), ('funds', 349), ('investing', 349), ('tandem', 348), ('additionally', 206)]\n",
      "[INFO] Fold 4 cosine similarity: 0.343872.\n",
      "\t [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11094\n",
      "[INFO] Number of bigrams: 1109\n",
      "[INFO] Number of trigrams: 110\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 393us/step - loss: 4066714.9498 - acc: 0.0024\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 1s 242us/step - loss: 3809152.7361 - acc: 0.0305\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 1s 242us/step - loss: 3651093.0791 - acc: 0.0635\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 1s 242us/step - loss: 3504184.1529 - acc: 0.0900\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 1s 241us/step - loss: 3361166.4329 - acc: 0.1108\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.002839.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.010712.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('arbitrage', 538), ('masters', 538), ('physician', 537), ('refractive', 537), ('binocular', 537), ('revise', 537), ('anomalies', 536), ('investments', 520), ('designated', 509), ('complicated', 419)]\n",
      "[INFO] Fold 5 cosine similarity: 0.238475.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004099   0.018248           0.373498\n",
      "0    True   0.002      1.0          5  0.004010   0.017455           0.367750\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.003000, tf_bias: 0.500000, num_epochs: 5***\n",
      "\t [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11932\n",
      "[INFO] Number of bigrams: 1193\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 435us/step - loss: 20862.7739 - acc: 0.0044\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 280us/step - loss: 19665.7487 - acc: 0.0419\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 280us/step - loss: 18935.3186 - acc: 0.0763\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 289us/step - loss: 18248.2094 - acc: 0.0993\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 298us/step - loss: 17597.5716 - acc: 0.1217\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.006300.\n",
      "[INFO] Predicting on validation set for precision...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.031030.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('violence', 291), ('professionally', 214), ('negotiation', 198), ('managed', 197), ('candidates', 197), ('debt', 197), ('nonprofit', 197), ('managerial', 197), ('leaders', 174), ('worlds', 158)]\n",
      "[INFO] Fold 1 cosine similarity: 0.450000.\n",
      "\t [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11781\n",
      "[INFO] Number of bigrams: 1178\n",
      "[INFO] Number of trigrams: 117\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 448us/step - loss: 20223.1728 - acc: 0.0047\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 299us/step - loss: 19060.0357 - acc: 0.0378\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 280us/step - loss: 18356.8854 - acc: 0.0698\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 280us/step - loss: 17678.5970 - acc: 0.0941\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 276us/step - loss: 17085.7691 - acc: 0.1081\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.006574.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.029763.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('company', 263), ('ocular', 258), ('professionally', 258), ('managed', 257), ('patient', 257), ('negotiation', 257), ('derivative', 257), ('optometric', 250), ('designated', 231), ('debt', 209)]\n",
      "[INFO] Fold 2 cosine similarity: 0.429041.\n",
      "\t [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11666\n",
      "[INFO] Number of bigrams: 1166\n",
      "[INFO] Number of trigrams: 116\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 387us/step - loss: 19911.6160 - acc: 0.0054\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 266us/step - loss: 18770.3211 - acc: 0.0369\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 276us/step - loss: 18084.2730 - acc: 0.0691\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 264us/step - loss: 17421.1307 - acc: 0.0893\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 286us/step - loss: 16845.8590 - acc: 0.1071\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.006034.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.029356.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('nonprofit', 314), ('optometric', 314), ('pedagogical', 312), ('mba', 312), ('derivative', 312), ('ocular', 312), ('company', 302), ('patient', 296), ('returns', 279), ('ventures', 274)]\n",
      "[INFO] Fold 3 cosine similarity: 0.373275.\n",
      "\t [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11564\n",
      "[INFO] Number of bigrams: 1156\n",
      "[INFO] Number of trigrams: 115\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 416us/step - loss: 19822.1847 - acc: 0.0058\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 278us/step - loss: 18707.2381 - acc: 0.0430\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 277us/step - loss: 18040.0002 - acc: 0.0780\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 275us/step - loss: 17394.3296 - acc: 0.0954\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 278us/step - loss: 16803.9184 - acc: 0.1110\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.005716.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.025695.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('optometric', 351), ('patient', 350), ('derivative', 350), ('ocular', 350), ('professionally', 350), ('managerial', 350), ('managed', 332), ('company', 331), ('mba', 290), ('debt', 245)]\n",
      "[INFO] Fold 4 cosine similarity: 0.354565.\n",
      "\t [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11666\n",
      "[INFO] Number of bigrams: 1166\n",
      "[INFO] Number of trigrams: 116\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 436us/step - loss: 20127.9667 - acc: 0.0069\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 262us/step - loss: 18943.5256 - acc: 0.0385\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 267us/step - loss: 18221.2992 - acc: 0.0702\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 270us/step - loss: 17553.2241 - acc: 0.1025\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 265us/step - loss: 16926.3464 - acc: 0.1122\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.004210.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.015661.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('ventures', 538), ('debt', 538), ('managed', 537), ('formulating', 537), ('optometric', 537), ('negotiation', 529), ('nonprofit', 471), ('company', 468), ('binocular', 402), ('budgeting', 386)]\n",
      "[INFO] Fold 5 cosine similarity: 0.251368.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004099   0.018248           0.373498\n",
      "0    True   0.002      1.0          5  0.004010   0.017455           0.367750\n",
      "0    True   0.003      0.5          5  0.004596   0.020404           0.369050\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.003000, tf_bias: 1.000000, num_epochs: 5***\n",
      "\t [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11932\n",
      "[INFO] Number of bigrams: 1193\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 430us/step - loss: 7747254.2969 - acc: 0.0059\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 275us/step - loss: 7291179.4612 - acc: 0.0342\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 278us/step - loss: 7010544.1258 - acc: 0.0669\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 278us/step - loss: 6762297.6807 - acc: 0.0910\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 278us/step - loss: 6514977.2410 - acc: 0.1066\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.006161.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.028455.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('conquest', 292), ('affected', 278), ('conventional', 263), ('optometric', 216), ('professionally', 210), ('trace', 202), ('negotiation', 198), ('managed', 197), ('debt', 197), ('designated', 191)]\n",
      "[INFO] Fold 1 cosine similarity: 0.431485.\n",
      "\t [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of unigrams: 11781\n",
      "[INFO] Number of bigrams: 1178\n",
      "[INFO] Number of trigrams: 117\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 467us/step - loss: 7405813.5384 - acc: 0.0044\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 274us/step - loss: 6964836.9110 - acc: 0.0291\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 278us/step - loss: 6706192.8170 - acc: 0.0620\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 277us/step - loss: 6467866.4045 - acc: 0.0825\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 277us/step - loss: 6232037.7994 - acc: 0.1066\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.006416.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.028136.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('debt', 258), ('company', 258), ('professionally', 258), ('fixed', 257), ('patient', 257), ('ventures', 257), ('derivative', 257), ('ocular', 257), ('craft', 223), ('islam', 207)]\n",
      "[INFO] Fold 2 cosine similarity: 0.437606.\n",
      "\t [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11666\n",
      "[INFO] Number of bigrams: 1166\n",
      "[INFO] Number of trigrams: 116\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 406us/step - loss: 7262761.5220 - acc: 0.0046\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 271us/step - loss: 6834846.3677 - acc: 0.0303\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 270us/step - loss: 6581886.3774 - acc: 0.0624\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 271us/step - loss: 6342444.0290 - acc: 0.0827\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 267us/step - loss: 6118706.5436 - acc: 0.1008\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.005847.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.026847.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('company', 315), ('nonprofit', 313), ('optometric', 313), ('pedagogical', 312), ('mba', 312), ('ventures', 312), ('derivative', 312), ('ocular', 312), ('returns', 308), ('assessing', 300)]\n",
      "[INFO] Fold 3 cosine similarity: 0.368264.\n",
      "\t [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11564\n",
      "[INFO] Number of bigrams: 1156\n",
      "[INFO] Number of trigrams: 115\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 438us/step - loss: 7233952.2877 - acc: 0.0051\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 278us/step - loss: 6801315.5056 - acc: 0.0341\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 280us/step - loss: 6559051.1102 - acc: 0.0673\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 274us/step - loss: 6315517.2054 - acc: 0.0886\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 276us/step - loss: 6092743.6308 - acc: 0.1025\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.005547.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.021966.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('mba', 351), ('patient', 351), ('optometric', 351), ('derivative', 350), ('ocular', 350), ('professionally', 350), ('managed', 349), ('masters', 338), ('company', 319), ('debt', 255)]\n",
      "[INFO] Fold 4 cosine similarity: 0.353119.\n",
      "\t [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11666\n",
      "[INFO] Number of bigrams: 1166\n",
      "[INFO] Number of trigrams: 116\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 435us/step - loss: 7334643.2511 - acc: 0.0034\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 272us/step - loss: 6886197.9027 - acc: 0.0356\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 275us/step - loss: 6624204.5453 - acc: 0.0610\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 267us/step - loss: 6378553.2243 - acc: 0.0851\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 275us/step - loss: 6141668.8349 - acc: 0.1042\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.004057.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.014712.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('managed', 538), ('ventures', 538), ('optometric', 538), ('arbitrage', 537), ('pharmacology', 537), ('debt', 537), ('binocular', 529), ('mergers', 523), ('mba', 505), ('budgeting', 313)]\n",
      "[INFO] Fold 5 cosine similarity: 0.245457.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004099   0.018248           0.373498\n",
      "0    True   0.002      1.0          5  0.004010   0.017455           0.367750\n",
      "0    True   0.003      0.5          5  0.004596   0.020404           0.369050\n",
      "0    True   0.003      1.0          5  0.004848   0.021309           0.368584\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.004000, tf_bias: 0.500000, num_epochs: 5***\n",
      "\t [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12251\n",
      "[INFO] Number of bigrams: 1225\n",
      "[INFO] Number of trigrams: 122\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 425us/step - loss: 27290.0772 - acc: 0.0088\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 274us/step - loss: 25777.1934 - acc: 0.0453\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 274us/step - loss: 24845.6981 - acc: 0.0736\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 278us/step - loss: 24006.7291 - acc: 0.0924\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 273us/step - loss: 23202.4740 - acc: 0.1129\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.007947.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.038821.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('professionally', 211), ('entrepreneurial', 209), ('candidates', 198), ('reproduction', 198), ('assignment', 197), ('managers', 197), ('valuation', 197), ('managed', 196), ('nonprofit', 193), ('colonialism', 185)]\n",
      "[INFO] Fold 1 cosine similarity: 0.456285.\n",
      "\t [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12136\n",
      "[INFO] Number of bigrams: 1213\n",
      "[INFO] Number of trigrams: 121\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 440us/step - loss: 27317.1196 - acc: 0.0080\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 287us/step - loss: 25835.7035 - acc: 0.0439\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5901/5901 [==============================] - 2s 285us/step - loss: 24915.4426 - acc: 0.0717\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 287us/step - loss: 24096.2629 - acc: 0.0851\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 290us/step - loss: 23282.4307 - acc: 0.1112\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.008746.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.038847.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('debt', 258), ('professionally', 258), ('gsis', 257), ('asset', 257), ('managers', 257), ('ocular', 257), ('patients', 257), ('patient', 257), ('entrepreneurial', 257), ('company', 245)]\n",
      "[INFO] Fold 2 cosine similarity: 0.425166.\n",
      "\t [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12015\n",
      "[INFO] Number of bigrams: 1201\n",
      "[INFO] Number of trigrams: 120\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 432us/step - loss: 26879.2158 - acc: 0.0076\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 273us/step - loss: 25388.8999 - acc: 0.0478\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 272us/step - loss: 24548.1255 - acc: 0.0737\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 285us/step - loss: 23707.5559 - acc: 0.0952\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 276us/step - loss: 22952.2670 - acc: 0.1115\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.008345.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.041288.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('grading', 313), ('asset', 313), ('pedagogical', 312), ('valuation', 312), ('ocular', 312), ('nonprofit', 307), ('gsis', 257), ('optometric', 256), ('causal', 234), ('managerial', 201)]\n",
      "[INFO] Fold 3 cosine similarity: 0.387318.\n",
      "\t [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11901\n",
      "[INFO] Number of bigrams: 1190\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 449us/step - loss: 26527.6044 - acc: 0.0088\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 267us/step - loss: 25059.5824 - acc: 0.0481\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 268us/step - loss: 24193.7969 - acc: 0.0763\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 272us/step - loss: 23408.7154 - acc: 0.0973\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 265us/step - loss: 22653.1812 - acc: 0.1110\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.007694.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.032610.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('instructional', 351), ('gsis', 350), ('patient', 350), ('ventures', 350), ('ocular', 350), ('professionally', 350), ('asset', 326), ('managers', 305), ('patients', 262), ('mba', 229)]\n",
      "[INFO] Fold 4 cosine similarity: 0.359417.\n",
      "\t [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11989\n",
      "[INFO] Number of bigrams: 1198\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 465us/step - loss: 26495.2672 - acc: 0.0058\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 275us/step - loss: 24993.4368 - acc: 0.0400\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 276us/step - loss: 24120.2933 - acc: 0.0637\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 279us/step - loss: 23283.6731 - acc: 0.0885\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 280us/step - loss: 22483.2669 - acc: 0.0986\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.005210.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.018847.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('asset', 538), ('ocular', 538), ('professionally', 538), ('patients', 538), ('patient', 537), ('optometric', 537), ('ventures', 534), ('debt', 527), ('gsis', 498), ('managed', 308)]\n",
      "[INFO] Fold 5 cosine similarity: 0.245440.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004099   0.018248           0.373498\n",
      "0    True   0.002      1.0          5  0.004010   0.017455           0.367750\n",
      "0    True   0.003      0.5          5  0.004596   0.020404           0.369050\n",
      "0    True   0.003      1.0          5  0.004848   0.021309           0.368584\n",
      "0    True   0.004      0.5          5  0.005396   0.023863           0.369812\n",
      "***[INFO] Evaluating cross-validated model with hyperparams use_idf: True, max_df: 0.004000, tf_bias: 1.000000, num_epochs: 5***\n",
      "\t [INFO] Fold 1\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12251\n",
      "[INFO] Number of bigrams: 1225\n",
      "[INFO] Number of trigrams: 122\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 516us/step - loss: 11340928.5966 - acc: 0.0046\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 295us/step - loss: 10684060.7539 - acc: 0.0381\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 297us/step - loss: 10309343.9146 - acc: 0.0624\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 289us/step - loss: 9942528.7600 - acc: 0.0868\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 297us/step - loss: 9612957.1844 - acc: 0.0997\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.007730.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.036585.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('represented', 227), ('paid', 221), ('sensors', 215), ('professionally', 210), ('entrepreneurial', 207), ('assignment', 197), ('managed', 197), ('managers', 197), ('valuation', 197), ('venture', 197)]\n",
      "[INFO] Fold 1 cosine similarity: 0.433895.\n",
      "\t [INFO] Fold 2\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12136\n",
      "[INFO] Number of bigrams: 1213\n",
      "[INFO] Number of trigrams: 121\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 504us/step - loss: 11350925.4955 - acc: 0.0061\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 316us/step - loss: 10701809.1030 - acc: 0.0369\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 330us/step - loss: 10335960.2918 - acc: 0.0624\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 320us/step - loss: 9988595.4316 - acc: 0.0810\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 334us/step - loss: 9655173.4157 - acc: 0.0964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.008394.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.035322.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('gsis', 257), ('debt', 257), ('professionally', 257), ('entrepreneurial', 257), ('ocular', 256), ('managed', 238), ('company', 232), ('patients', 201), ('identities', 181), ('seen', 180)]\n",
      "[INFO] Fold 2 cosine similarity: 0.434680.\n",
      "\t [INFO] Fold 3\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 12015\n",
      "[INFO] Number of bigrams: 1201\n",
      "[INFO] Number of trigrams: 120\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 510us/step - loss: 11138555.0310 - acc: 0.0061\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 308us/step - loss: 10492760.0046 - acc: 0.0381\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 331us/step - loss: 10135499.9117 - acc: 0.0671\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 333us/step - loss: 9783097.5948 - acc: 0.0856\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 324us/step - loss: 9461979.4070 - acc: 0.1003\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.007997.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.036746.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('pedagogical', 313), ('gsis', 312), ('causal', 312), ('asset', 312), ('company', 312), ('mba', 312), ('valuation', 312), ('ocular', 312), ('thematic', 223), ('patient', 221)]\n",
      "[INFO] Fold 3 cosine similarity: 0.378132.\n",
      "\t [INFO] Fold 4\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11901\n",
      "[INFO] Number of bigrams: 1190\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 487us/step - loss: 10927865.1491 - acc: 0.0083\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 312us/step - loss: 10298096.7123 - acc: 0.0366\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 306us/step - loss: 9949595.0514 - acc: 0.0668\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 307us/step - loss: 9614299.6962 - acc: 0.0880\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 290us/step - loss: 9303792.2825 - acc: 0.1024\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.007390.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.031254.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('optometric', 351), ('instructional', 350), ('patient', 350), ('ocular', 350), ('professionally', 350), ('gsis', 350), ('derivative', 344), ('debt', 282), ('ventures', 262), ('paid', 250)]\n",
      "[INFO] Fold 4 cosine similarity: 0.355405.\n",
      "\t [INFO] Fold 5\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11989\n",
      "[INFO] Number of bigrams: 1198\n",
      "[INFO] Number of trigrams: 119\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 539us/step - loss: 10833266.5907 - acc: 0.0069\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 314us/step - loss: 10205612.6649 - acc: 0.0352\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 313us/step - loss: 9851036.7677 - acc: 0.0552\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 312us/step - loss: 9501539.8828 - acc: 0.0776\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 330us/step - loss: 9170196.1104 - acc: 0.0925\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.005158.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.017085.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[DEBUG] most common keywords:  [('patients', 539), ('ocular', 539), ('professionally', 538), ('managed', 537), ('debt', 537), ('patient', 537), ('optometric', 537), ('asset', 534), ('mba', 534), ('ventures', 534)]\n",
      "[INFO] Fold 5 cosine similarity: 0.240276.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004099   0.018248           0.373498\n",
      "0    True   0.002      1.0          5  0.004010   0.017455           0.367750\n",
      "0    True   0.003      0.5          5  0.004596   0.020404           0.369050\n",
      "0    True   0.003      1.0          5  0.004848   0.021309           0.368584\n",
      "0    True   0.004      0.5          5  0.005396   0.023863           0.369812\n",
      "0    True   0.004      1.0          5  0.005719   0.025119           0.369590\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'use_idf': [True],\n",
    "              'max_df': np.arange(0.002, .005, .001), # np.arange(0, .0055, .0005)\n",
    "              'tf_bias': np.arange(.5, 1.5, .5), \n",
    "              'num_epochs': [5]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "recall_validation_scores = []\n",
    "precision_validation_scores = []\n",
    "distribution_validation_scores = []\n",
    "\n",
    "for params in grid:\n",
    "    print(\"***[INFO] Evaluating cross-validated model with hyperparams use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d***\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))\n",
    "\n",
    "    fold_num = 1\n",
    "    kf = KFold(n_splits=5, random_state=42) # DO NOT FIX RANDOM STATE WHEN RUNNING THE ACTUAL EXPERIMENT - NVM, should be fixed for reproducibility\n",
    "    for train_idx, valid_idx in kf.split(filtered_vec_df):\n",
    "        print('======== [INFO] Fold %d' % (fold_num))\n",
    "        # X = vectors, Y = descriptions\n",
    "        split_X_train, split_X_valid = filtered_vec_df.iloc[train_idx], filtered_vec_df.iloc[valid_idx]\n",
    "        split_Y_train, split_Y_valid = filtered_descript_df.iloc[train_idx], filtered_descript_df.iloc[valid_idx]\n",
    "\n",
    "        vocab = get_vocab(split_Y_train, textcolumn, max_df=params['max_df'], use_idf=params['use_idf']) \n",
    "        vocab_frame = pd.DataFrame(vocab)\n",
    "        vocabsize = len(vocab)\n",
    "\n",
    "        # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "        split_Y_train_BOW = to_bag_of_words(split_Y_train, textcolumn, vocab, tf_bias=params['tf_bias'], use_idf=params['use_idf'])\n",
    "        split_Y_train_BOW = split_Y_train_BOW.toarray()\n",
    "\n",
    "        (weights_frame, biases) = logistic_regression(split_X_train.iloc[:,1:], split_Y_train_BOW, num_epochs=params['num_epochs'])\n",
    "\n",
    "        print('[INFO] Predicting on validation set for recall...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, max_descript_len)\n",
    "        fold_i_average_recall = calculate_metric(df_with_keywords, 'r')\n",
    "        recall_validation_scores.append(fold_i_average_recall)\n",
    "        print('[INFO] Fold %d recall: %f.' % (fold_num, fold_i_average_recall))\n",
    "\n",
    "        print('[INFO] Predicting on validation set for precision...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, num_top_words)\n",
    "        fold_i_average_precision = calculate_metric(df_with_keywords, 'p')\n",
    "        precision_validation_scores.append(fold_i_average_precision)\n",
    "        print('[INFO] Fold %d precision: %f.' % (fold_num, fold_i_average_precision))\n",
    "\n",
    "        fold_i_distribution_diff = calculate_metric(df_with_keywords, 'c')\n",
    "        distribution_validation_scores.append(fold_i_distribution_diff)\n",
    "        print('[INFO] Fold %d cosine similarity: %f.' % (fold_num, fold_i_distribution_diff))\n",
    "\n",
    "        fold_num += 1\n",
    "\n",
    "    recall_i = np.mean(recall_validation_scores)\n",
    "    precision_i = np.mean(precision_validation_scores)\n",
    "    distribution_diff_i = np.mean(distribution_validation_scores)\n",
    "\n",
    "    model_i_params = [params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs'], \n",
    "                      recall_i, precision_i, distribution_diff_i]\n",
    "    model_i_params = pd.DataFrame([model_i_params], columns=hyperparams_cols)\n",
    "    grid_search_df = grid_search_df.append(model_i_params, sort = False)\n",
    "    print(grid_search_df)\n",
    "    # print('recall scores:', recall_validation_scores)\n",
    "    # print('precision scores:', precision_validation_scores)\n",
    "    # print('distribution scores:', distribution_validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks:\n",
    "\n",
    "1. Rename columns to recall@max_len, precision@10 \n",
    "1. avoid human inspection as metric for now\n",
    "1. See what aspects of unsupervised methods notebook you can incorporate\n",
    "\n",
    "### To implement:\n",
    "\n",
    "1. Distribution difference\n",
    "    - Calculate cosine distance instead of cosine similarity\n",
    "    - Calculate the distance from the vector of original frequencies instead of the uniform distribution\n",
    "    - Calculate the distance from the all 1s vector\n",
    "\n",
    "1. Calculate document frequency and redundancy metric\n",
    "\n",
    "1. Master metric = some combination of metrics + regularization (esp for max_df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use_idf</th>\n",
       "      <th>max_df</th>\n",
       "      <th>tf-bias</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>distribution_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.373661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.366876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>0.368689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>0.368771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.023874</td>\n",
       "      <td>0.368446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>0.025180</td>\n",
       "      <td>0.368866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
       "0    True   0.002      0.5          5  0.004051   0.018519           0.373661\n",
       "0    True   0.002      1.0          5  0.004004   0.017651           0.366876\n",
       "0    True   0.003      0.5          5  0.004589   0.020309           0.368689\n",
       "0    True   0.003      1.0          5  0.004849   0.021268           0.368771\n",
       "0    True   0.004      0.5          5  0.005379   0.023874           0.368446\n",
       "0    True   0.004      1.0          5  0.005716   0.025180           0.368866"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df.to_csv('grid_search_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimization\n",
    "\n",
    "- do not append to dataframes, start w/ lists and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
