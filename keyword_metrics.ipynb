{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keywords_research_helper import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = '/home/matthew/Models-AskOski/ICS/'\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs_temp.tsv')\n",
    "rawfile = os.path.join(TRAINING_DIR, 'course_info_temp.tsv')\n",
    "textcolumn = 'course_description'\n",
    "use_idf = True\n",
    "tf_bias = .5\n",
    "num_epochs = 5\n",
    "num_top_words = 10\n",
    "max_df = 0.0028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[UNIGRAMS] number unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[BIGRAMS] number bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[TRIGRAMS] number trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descrip_title(row):\n",
    "    punc_remover = str.maketrans('', '', string.punctuation)\n",
    "    lowered = row['descrip_title'].lower()\n",
    "    lowered_removed_punc = lowered.translate(punc_remover)\n",
    "    cleaned_set = set(lowered_removed_punc.split())\n",
    "    return cleaned_set\n",
    "\n",
    "def recall_keywords(row):\n",
    "    return row['description_title_set'].intersection(row['keywords_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "raw_frame = pd.read_csv(rawfile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(raw_frame[textcolumn].notnull() == True)[0]\n",
    "filtered_vec_frame = vec_frame.iloc[nonempty_indices,:]\n",
    "filtered_raw_frame = raw_frame.iloc[nonempty_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(filtered_vec_frame, filtered_raw_frame, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(Y_train, textcolumn) # get_vocab(raw_frame, textcolumn) \n",
    "vocab_frame = pd.DataFrame(vocab)\n",
    "    \n",
    "vocabsize = len(vocab)\n",
    "\n",
    "# Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "Y_train_BOW = to_bag_of_words(Y_train, textcolumn, vocab)\n",
    "Y_train_BOW = Y_train_BOW.toarray()\n",
    "Y_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights_frame, biases) = logistic_regression(X_train.iloc[:,1:], Y_train_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_frame = X_test.iloc[:,1:].dot(weights_frame.values) + biases\n",
    "print('[INFO] Sorting classification results...')\n",
    "sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "predicted_keyword_list = []\n",
    "for i in range(num_top_words):\n",
    "    new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "    predicted_keyword_list.extend(new_col.values)\n",
    "    Y_test['predicted_word_' + str(num_top_words-i)] = new_col.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keyword_counter = Counter(predicted_keyword_list)\n",
    "\n",
    "keyword_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_possible_keywords = Y_test.shape[0] * num_top_words\n",
    "num_predicted_keywords = len(keyword_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(keyword_counter.values()) == Y_test.shape[0] * num_top_words,\\\n",
    "'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "unif_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "predicted_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unif_keyword_vector.shape == predicted_keyword_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n",
    "\n",
    "cosine_similarity(predicted_keyword_vector, unif_keyword_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([1,-1], [1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
