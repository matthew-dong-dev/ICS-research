{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = os.getcwd()\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(TRAINING_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'\n",
    "# num_top_words = 10\n",
    "# use_idf = True\n",
    "# tf_bias = .5\n",
    "# num_epochs = 5\n",
    "# max_df = 0.0028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[UNIGRAMS] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[BIGRAMS] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(bigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[TRIGRAMS] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descrip_title(row):\n",
    "    punc_remover = str.maketrans('', '', string.punctuation)\n",
    "    lowered = row['descrip_title'].lower()\n",
    "    lowered_removed_punc = lowered.translate(punc_remover)\n",
    "    cleaned_set = set(lowered_removed_punc.split())\n",
    "    return cleaned_set\n",
    "\n",
    "def recall_keywords(row):\n",
    "    return row['description_title_set'].intersection(row['keywords_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def test(x=1):\n",
    "    print(x)\n",
    "test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "raw_frame = pd.read_csv(rawfile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(raw_frame[textcolumn].notnull() == True)[0]\n",
    "filtered_vec_frame = vec_frame.iloc[nonempty_indices,:]\n",
    "filtered_raw_frame = raw_frame.iloc[nonempty_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(filtered_vec_frame, filtered_raw_frame, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(Y_train, textcolumn) # get_vocab(raw_frame, textcolumn) \n",
    "vocab_frame = pd.DataFrame(vocab)\n",
    "    \n",
    "vocabsize = len(vocab)\n",
    "\n",
    "# Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "Y_train_BOW = to_bag_of_words(Y_train, textcolumn, vocab)\n",
    "Y_train_BOW = Y_train_BOW.toarray()\n",
    "Y_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights_frame, biases) = logistic_regression(X_train.iloc[:,1:], Y_train_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_frame = X_test.iloc[:,1:].dot(weights_frame.values) + biases\n",
    "print('[INFO] Sorting classification results...')\n",
    "sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "predicted_keyword_list = []\n",
    "for i in range(num_top_words):\n",
    "    new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "    predicted_keyword_list.extend(new_col.values)\n",
    "    Y_test['predicted_word_' + str(num_top_words-i)] = new_col.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keyword_counter = Counter(predicted_keyword_list)\n",
    "\n",
    "keyword_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_possible_keywords = Y_test.shape[0] * num_top_words\n",
    "num_predicted_keywords = len(keyword_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(keyword_counter.values()) == Y_test.shape[0] * num_top_words,\\\n",
    "'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "unif_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "predicted_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unif_keyword_vector.shape == predicted_keyword_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n",
    "\n",
    "cosine_similarity(predicted_keyword_vector, unif_keyword_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([1,-1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(course_vecs, course_descipts, trained_weights, trained_biases, num_words_per_course):\n",
    "    \"\"\"\n",
    "    lalalal\n",
    "    \n",
    "    \"\"\"\n",
    "    df_with_keywords = course_descipts.copy()\n",
    "    softmax_frame = course_vecs.iloc[:,1:].dot(trained_weights.values) + trained_biases # make predictions\n",
    "\n",
    "    # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "    print('[INFO] Sorting classification results...')\n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_words_per_course:]\n",
    "\n",
    "    print('[INFO] Predicting top k inferred keywords for each course...')\n",
    "    for i in range(num_words_per_course):\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        df_with_keywords['predicted_word_' + str(num_words_per_course-i)] = new_col.values\n",
    "        \n",
    "    return df_with_keywords\n",
    "\n",
    "def calculate_metric(df_with_keywords, metric):\n",
    "    \"\"\"\n",
    "    metrics: {r: recall, p: precision}\n",
    "    \"\"\"\n",
    "    def clean_descrip_title(row):\n",
    "        punc_remover = str.maketrans('', '', string.punctuation)\n",
    "        lowered = row['descrip_title'].lower()\n",
    "        lowered_removed_punc = lowered.translate(punc_remover)\n",
    "        cleaned_set = set(lowered_removed_punc.split())\n",
    "        return cleaned_set\n",
    "\n",
    "    def recall_keywords(row):\n",
    "        return row['description_title_set'].intersection(row['course_keywords_set'])\n",
    "    \n",
    "    prediction_df = df_with_keywords.copy()\n",
    "    only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "    num_keywords_predicted = only_predicted_keywords_df.shape[1]\n",
    "    prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "    prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "    prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "    \n",
    "    if metric == 'r':\n",
    "        print('[INFO] Calculating Recall...')\n",
    "        assert num_keywords_predicted == max_descript_len, 'Number of keywords predicted should equal longest description length'\n",
    "        prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "        average_recall = np.mean(prediction_df['recall'])\n",
    "        return average_recall\n",
    "    if metric == 'p':\n",
    "        print('[INFO] Calculating Precision...')\n",
    "        assert num_keywords_predicted == num_top_words, 'Number of keywords predicted should equal number of predicted words per course'\n",
    "        prediction_df['precision'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / num_top_words)\n",
    "        average_precision = np.mean(prediction_df['precision'])\n",
    "        return average_precision\n",
    "    if metric == 'c':\n",
    "        print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "        predicted_keyword_list = only_predicted_keywords_df.values.tolist()\n",
    "        predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "        keyword_counter = Counter(predicted_keyword_list)\n",
    "        \n",
    "        num_possible_keywords = df_with_keywords.shape[0] * num_top_words\n",
    "        num_predicted_keywords = len(keyword_counter.keys())\n",
    "        assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "        'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "        unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "        predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "        assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "        'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "    \n",
    "        cos_sim = cosine_similarity(predicted_keyword_vector, unif_keyword_vector)\n",
    "        return cos_sim\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column, max_df=0.0028, use_idf=True):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(bigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab, tf_bias=.5, use_idf=True):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, num_epochs=1):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "info_frame = pd.read_csv(infofile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(info_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = info_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())\n",
    "num_top_words = 10\n",
    "\n",
    "hyperparams_cols = ['use_idf', 'max_df','tf-bias', 'num_epochs', 'recall', 'precision', 'distribution_diff']\n",
    "grid_search_df = pd.DataFrame(columns=hyperparams_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 1.000000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 1.000000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.004000, tf_bias: 0.500000, num_epochs: 5\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.004000, tf_bias: 1.000000, num_epochs: 5\n"
     ]
    }
   ],
   "source": [
    "np.arange(0.002, .005, .001)\n",
    "\n",
    "param_grid = {'use_idf': [True],\n",
    "              'max_df': np.arange(0.002, .005, .001), # np.arange(0, .0055, .0005)\n",
    "              'tf_bias': np.arange(.5, 1.5, .5), \n",
    "              'num_epochs': [5]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in grid:\n",
    "    print(\"[HYPERPARAMS] use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 0.500000, num_epochs: 5\n",
      "[CV] Running cross validation...\n",
      "[INFO] ****** Fold 1 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11379\n",
      "[INFO] number bigrams: 1137\n",
      "[INFO] number trigrams: 1137\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 447us/step - loss: 14078.5549 - acc: 0.0034\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 307us/step - loss: 13184.5129 - acc: 0.0353\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 311us/step - loss: 12652.7602 - acc: 0.0685\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 325us/step - loss: 12140.7327 - acc: 0.0966\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 342us/step - loss: 11689.9028 - acc: 0.1198\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.004462.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.020528.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 1 cosine similarity: 0.464311.\n",
      "[INFO] ****** Fold 2 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11214\n",
      "[INFO] number bigrams: 1121\n",
      "[INFO] number trigrams: 1121\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 420us/step - loss: 13492.8834 - acc: 0.0029\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 289us/step - loss: 12653.0456 - acc: 0.0298\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 268us/step - loss: 12131.8322 - acc: 0.0668\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 285us/step - loss: 11660.1583 - acc: 0.0983\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 289us/step - loss: 11224.5278 - acc: 0.1193\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.004731.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.020678.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 2 cosine similarity: 0.431987.\n",
      "[INFO] ****** Fold 3 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11104\n",
      "[INFO] number bigrams: 1110\n",
      "[INFO] number trigrams: 1110\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 390us/step - loss: 13198.1280 - acc: 0.0022\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 302us/step - loss: 12378.4950 - acc: 0.0308\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 302us/step - loss: 11855.1693 - acc: 0.0702\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 300us/step - loss: 11448.7664 - acc: 0.0952\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 307us/step - loss: 10986.1505 - acc: 0.1200\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.004266.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.020000.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 3 cosine similarity: 0.367211.\n",
      "[INFO] ****** Fold 4 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 10993\n",
      "[INFO] number bigrams: 1099\n",
      "[INFO] number trigrams: 1099\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 393us/step - loss: 13155.3291 - acc: 0.0034\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 306us/step - loss: 12331.3864 - acc: 0.0314\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 285us/step - loss: 11839.2788 - acc: 0.0683\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 317us/step - loss: 11386.1175 - acc: 0.0922\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 312us/step - loss: 10959.5156 - acc: 0.1090\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.003858.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.016746.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 4 cosine similarity: 0.338077.\n",
      "[INFO] ****** Fold 5 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11094\n",
      "[INFO] number bigrams: 1109\n",
      "[INFO] number trigrams: 1109\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 407us/step - loss: 13347.9438 - acc: 0.0025\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 304us/step - loss: 12500.9385 - acc: 0.0320\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 302us/step - loss: 11988.3528 - acc: 0.0737\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 301us/step - loss: 11502.3013 - acc: 0.0978\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 313us/step - loss: 11033.1240 - acc: 0.1234\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.003124.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.012068.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 5 cosine similarity: 0.243163.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004088   0.018004            0.36895\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.002000, tf_bias: 1.000000, num_epochs: 5\n",
      "[CV] Running cross validation...\n",
      "[INFO] ****** Fold 1 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11379\n",
      "[INFO] number bigrams: 1137\n",
      "[INFO] number trigrams: 1137\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 428us/step - loss: 4430541.1036 - acc: 0.0017\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 320us/step - loss: 4137116.9371 - acc: 0.0241\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 312us/step - loss: 3967912.3308 - acc: 0.0544\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 305us/step - loss: 3812368.6634 - acc: 0.0859\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 306us/step - loss: 3660373.5561 - acc: 0.1020\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.004267.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.019783.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 1 cosine similarity: 0.441379.\n",
      "[INFO] ****** Fold 2 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11214\n",
      "[INFO] number bigrams: 1121\n",
      "[INFO] number trigrams: 1121\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 420us/step - loss: 4168938.6704 - acc: 0.0017\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5901/5901 [==============================] - 2s 306us/step - loss: 3894821.5580 - acc: 0.0242\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 308us/step - loss: 3736675.1443 - acc: 0.0588\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 310us/step - loss: 3586308.1576 - acc: 0.0841\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 314us/step - loss: 3454830.7580 - acc: 0.1057\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.004611.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.019932.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 2 cosine similarity: 0.432072.\n",
      "[INFO] ****** Fold 3 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11104\n",
      "[INFO] number bigrams: 1110\n",
      "[INFO] number trigrams: 1110\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 400us/step - loss: 4046710.2098 - acc: 0.0031\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 297us/step - loss: 3783043.3237 - acc: 0.0261\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 297us/step - loss: 3632150.8739 - acc: 0.0598\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 296us/step - loss: 3479988.8830 - acc: 0.0803\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 295us/step - loss: 3350186.3448 - acc: 0.1010\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.004154.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.019390.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 3 cosine similarity: 0.387405.\n",
      "[INFO] ****** Fold 4 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 10993\n",
      "[INFO] number bigrams: 1099\n",
      "[INFO] number trigrams: 1099\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 424us/step - loss: 4046488.0632 - acc: 0.0019\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 323us/step - loss: 3773083.5502 - acc: 0.0314\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 301us/step - loss: 3629045.5991 - acc: 0.0581\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 300us/step - loss: 3488533.1479 - acc: 0.0817\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 292us/step - loss: 3353741.1767 - acc: 0.1029\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.003798.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.015797.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 4 cosine similarity: 0.343495.\n",
      "[INFO] ****** Fold 5 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11094\n",
      "[INFO] number bigrams: 1109\n",
      "[INFO] number trigrams: 1109\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 411us/step - loss: 4089650.4970 - acc: 0.0037\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 311us/step - loss: 3816045.3199 - acc: 0.0305\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 303us/step - loss: 3655575.3336 - acc: 0.0593\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 304us/step - loss: 3506990.4145 - acc: 0.0888\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 303us/step - loss: 3372627.9534 - acc: 0.1110\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.002832.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.011525.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 5 cosine similarity: 0.243924.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004088   0.018004           0.368950\n",
      "0    True   0.002      1.0          5  0.004010   0.017645           0.369303\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 0.500000, num_epochs: 5\n",
      "[CV] Running cross validation...\n",
      "[INFO] ****** Fold 1 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11932\n",
      "[INFO] number bigrams: 1193\n",
      "[INFO] number trigrams: 1193\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 2s 368us/step - loss: 20965.6469 - acc: 0.0071\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 279us/step - loss: 19703.1237 - acc: 0.0417\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 293us/step - loss: 18985.5917 - acc: 0.0776\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 318us/step - loss: 18271.6009 - acc: 0.0985\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 289us/step - loss: 17601.3420 - acc: 0.1159\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.006517.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.031030.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 1 cosine similarity: 0.443762.\n",
      "[INFO] ****** Fold 2 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11781\n",
      "[INFO] number bigrams: 1178\n",
      "[INFO] number trigrams: 1178\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 472us/step - loss: 20311.5159 - acc: 0.0058\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 317us/step - loss: 19094.0974 - acc: 0.0361\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 322us/step - loss: 18406.4242 - acc: 0.0688\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 339us/step - loss: 17719.5702 - acc: 0.0922\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 329us/step - loss: 17086.3193 - acc: 0.1129\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.006409.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.030712.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 2 cosine similarity: 0.422059.\n",
      "[INFO] ****** Fold 3 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11666\n",
      "[INFO] number bigrams: 1166\n",
      "[INFO] number trigrams: 1166\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 444us/step - loss: 20015.2519 - acc: 0.0058\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 303us/step - loss: 18828.1310 - acc: 0.0395\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 312us/step - loss: 18144.0844 - acc: 0.0666\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 303us/step - loss: 17470.3252 - acc: 0.0929\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5901/5901 [==============================] - 2s 318us/step - loss: 16862.4770 - acc: 0.1054\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.006023.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.029153.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 3 cosine similarity: 0.385184.\n",
      "[INFO] ****** Fold 4 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11564\n",
      "[INFO] number bigrams: 1156\n",
      "[INFO] number trigrams: 1156\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 465us/step - loss: 19940.6548 - acc: 0.0051\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 326us/step - loss: 18738.5465 - acc: 0.0398\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 339us/step - loss: 18065.8841 - acc: 0.0754\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 323us/step - loss: 17422.9595 - acc: 0.0957\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 333us/step - loss: 16827.7026 - acc: 0.1120\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.005630.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.025559.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 4 cosine similarity: 0.348016.\n",
      "[INFO] ****** Fold 5 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11666\n",
      "[INFO] number bigrams: 1166\n",
      "[INFO] number trigrams: 1166\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 472us/step - loss: 20221.9172 - acc: 0.0056\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 317us/step - loss: 18982.6616 - acc: 0.0424\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 314us/step - loss: 18272.2859 - acc: 0.0702\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 307us/step - loss: 17600.3567 - acc: 0.0952\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 337us/step - loss: 16962.2587 - acc: 0.1113\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.004210.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.016136.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 5 cosine similarity: 0.250801.\n",
      "  use_idf  max_df  tf-bias num_epochs    recall  precision  distribution_diff\n",
      "0    True   0.002      0.5          5  0.004088   0.018004           0.368950\n",
      "0    True   0.002      1.0          5  0.004010   0.017645           0.369303\n",
      "0    True   0.003      0.5          5  0.004593   0.020602           0.369523\n",
      "[HYPERPARAMS] use_idf: True, max_df: 0.003000, tf_bias: 1.000000, num_epochs: 5\n",
      "[CV] Running cross validation...\n",
      "[INFO] ****** Fold 1 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11932\n",
      "[INFO] number bigrams: 1193\n",
      "[INFO] number trigrams: 1193\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 470us/step - loss: 7790996.0034 - acc: 0.0044\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 304us/step - loss: 7305923.8105 - acc: 0.0310\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 290us/step - loss: 7040494.1886 - acc: 0.0659\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 300us/step - loss: 6759727.3237 - acc: 0.0914\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 325us/step - loss: 6526584.3878 - acc: 0.1127\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.006195.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.029268.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 1 cosine similarity: 0.429063.\n",
      "[INFO] ****** Fold 2 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11781\n",
      "[INFO] number bigrams: 1178\n",
      "[INFO] number trigrams: 1178\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 508us/step - loss: 7447033.4045 - acc: 0.0039\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 341us/step - loss: 6979452.8995 - acc: 0.0314\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 336us/step - loss: 6725306.7476 - acc: 0.0563\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 328us/step - loss: 6478345.0713 - acc: 0.0824\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 324us/step - loss: 6242872.0590 - acc: 0.1041\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.006525.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.027661.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 2 cosine similarity: 0.418964.\n",
      "[INFO] ****** Fold 3 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11666\n",
      "[INFO] number bigrams: 1166\n",
      "[INFO] number trigrams: 1166\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 479us/step - loss: 7302978.3418 - acc: 0.0036\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 319us/step - loss: 6849761.1593 - acc: 0.0346\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 322us/step - loss: 6596073.4470 - acc: 0.0617\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 324us/step - loss: 6348815.1906 - acc: 0.0766\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 314us/step - loss: 6131603.9031 - acc: 0.0978\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.005881.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.027458.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 3 cosine similarity: 0.379329.\n",
      "[INFO] ****** Fold 4 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11564\n",
      "[INFO] number bigrams: 1156\n",
      "[INFO] number trigrams: 1156\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 540us/step - loss: 7265117.7167 - acc: 0.0034\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 311us/step - loss: 6818251.1571 - acc: 0.0347\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 310us/step - loss: 6569790.7034 - acc: 0.0686\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 310us/step - loss: 6329703.6858 - acc: 0.0910\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 313us/step - loss: 6114131.2483 - acc: 0.0993\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.005615.\n",
      "[INFO] Predicting on validation set for precision...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.023729.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 4 cosine similarity: 0.357743.\n",
      "[INFO] ****** Fold 5 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] number unigrams: 11666\n",
      "[INFO] number bigrams: 1166\n",
      "[INFO] number trigrams: 1166\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 485us/step - loss: 7365731.1529 - acc: 0.0049\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 314us/step - loss: 6911814.3023 - acc: 0.0359\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 309us/step - loss: 6644294.4337 - acc: 0.0639\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 308us/step - loss: 6387773.5303 - acc: 0.0846\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 295us/step - loss: 6141699.8878 - acc: 0.0990\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.004075.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'use_idf': [True],\n",
    "              'max_df': np.arange(0.002, .005, .001), # np.arange(0, .0055, .0005)\n",
    "              'tf_bias': np.arange(.5, 1.5, .5), \n",
    "              'num_epochs': [5]} \n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "recall_validation_scores = []\n",
    "precision_validation_scores = []\n",
    "distribution_validation_scores = []\n",
    "\n",
    "for params in grid:\n",
    "    print(\"****** [INFO] Evaluating cross-validated model with use_idf: %r, max_df: %f, tf_bias: %f, num_epochs: %d ******\" % \n",
    "          (params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs']))\n",
    "\n",
    "    fold_num = 1\n",
    "    kf = KFold(n_splits=5, random_state=42) # DO NOT FIX RANDOM STATE WHEN RUNNING THE ACTUAL EXPERIMENT - NVM, should be fixed for reproducibility\n",
    "    for train_idx, valid_idx in kf.split(filtered_vec_df):\n",
    "        print('\\t [INFO] Fold %d' % (fold_num))\n",
    "        # X = vectors, Y = descriptions\n",
    "        split_X_train, split_X_valid = filtered_vec_df.iloc[train_idx], filtered_vec_df.iloc[valid_idx]\n",
    "        split_Y_train, split_Y_valid = filtered_descript_df.iloc[train_idx], filtered_descript_df.iloc[valid_idx]\n",
    "\n",
    "        vocab = get_vocab(split_Y_train, textcolumn, max_df=params['max_df'], use_idf=params['use_idf']) \n",
    "        vocab_frame = pd.DataFrame(vocab)\n",
    "        vocabsize = len(vocab)\n",
    "\n",
    "        # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "        split_Y_train_BOW = to_bag_of_words(split_Y_train, textcolumn, vocab, tf_bias=params['tf_bias'], use_idf=params['use_idf'])\n",
    "        split_Y_train_BOW = split_Y_train_BOW.toarray()\n",
    "\n",
    "        (weights_frame, biases) = logistic_regression(split_X_train.iloc[:,1:], split_Y_train_BOW, num_epochs=params['num_epochs'])\n",
    "\n",
    "        print('[INFO] Predicting on validation set for recall...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, max_descript_len)\n",
    "        fold_i_average_recall = calculate_metric(df_with_keywords, 'r')\n",
    "        recall_validation_scores.append(fold_i_average_recall)\n",
    "        print('[INFO] Fold %d recall: %f.' % (fold_num, fold_i_average_recall))\n",
    "\n",
    "        print('[INFO] Predicting on validation set for precision...')\n",
    "        df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, num_top_words)\n",
    "        fold_i_average_precision = calculate_metric(df_with_keywords, 'p')\n",
    "        precision_validation_scores.append(fold_i_average_precision)\n",
    "        print('[INFO] Fold %d precision: %f.' % (fold_num, fold_i_average_precision))\n",
    "\n",
    "        fold_i_distribution_diff = calculate_metric(df_with_keywords, 'c')\n",
    "        distribution_validation_scores.append(fold_i_distribution_diff)\n",
    "        print('[INFO] Fold %d cosine similarity: %f.' % (fold_num, fold_i_distribution_diff))\n",
    "\n",
    "        fold_num += 1\n",
    "\n",
    "    recall_i = np.mean(recall_validation_scores)\n",
    "    precision_i = np.mean(precision_validation_scores)\n",
    "    distribution_diff_i = np.mean(distribution_validation_scores)\n",
    "\n",
    "    model_i_params = [params['use_idf'], params['max_df'], params['tf_bias'], params['num_epochs'], \n",
    "                      recall_i, precision_i, distribution_diff_i]\n",
    "    model_i_params = pd.DataFrame([model_i_params], columns=hyperparams_cols)\n",
    "    grid_search_df = grid_search_df.append(model_i_params, sort = False)\n",
    "    print(grid_search_df)\n",
    "    # print('recall scores:', recall_validation_scores)\n",
    "    # print('precision scores:', precision_validation_scores)\n",
    "    # print('distribution scores:', distribution_validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
