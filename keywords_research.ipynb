{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## To do \n",
    "\n",
    "1. Implement redundancy metric\n",
    "1. Design a master metric = some combination of metrics + regularization (esp for max_df) \n",
    "1. Distribution difference metric changes:\n",
    "    - Calculate the cosine distance from the vector of original frequencies instead of the uniform distribution\n",
    "    - Calculate the cosine distance from the all 1s vector\n",
    "1. what hyperparameter should create a greater change in in cosine / doc_freq?  or are those not capturing the difference in distribution?\n",
    "\n",
    "\n",
    "Hyperparams\n",
    "\n",
    "1. instead of using regular max_df as cutoff use max_df across subjects as filter? \n",
    "    - calculate df(data) vs df(nonprofit) & count(data) vs count(nonprofit) to use as baseline\n",
    "1. try using hidden layer for the regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **double check changing `data_joining.py` did not mess up any indexing**\n",
    "1. *optimization:* do not append to dataframes, start w/ lists and convert to dataframe OR initialize a numpy matrix for the hyperparameters using np.empty first and then [populate instead of appending](https://stackoverflow.com/questions/13370570/elegant-grid-search-in-python-numpy)\n",
    "1. See what aspects of unsupervised methods notebook you can incorporate\n",
    "\n",
    "--- \n",
    "\n",
    "#### Notes\n",
    "\n",
    "- cosine distance - measure between \"behaviorly informed predicted keywords and true course description\" \n",
    "- avoid human inspection as metric for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed\n",
    "\n",
    "1. [x] Implement document frequency metric\n",
    "1. [x] Change from cosine similarity to cosine distance\n",
    "1. [x] combine keyword research notebooks and have a separate grid_search notebook + clean up training repo\n",
    "1. [x] Rename columns to recall@max_len, precision@10 \n",
    "1. [x] fix [settingWithCopyError](https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_df is a fixed hyperparameter that needs to be part of the grid search, what should the default level be?\n",
    "\n",
    "- max document frequency = removing corpus-specific stop words\n",
    "- what does this error mean `ValueError: max_df corresponds to < documents than min_df` and why does it occur was max_df = 0?\n",
    "\n",
    "### does getting bigrams and trigrams separately make a difference?\n",
    "\n",
    "- try getting the vocab using ngram_range = 1,3 and no limit on max_features\n",
    "\n",
    "\n",
    "## remark: \n",
    "- maybe calculate recall using stemmed words (e.g. economy vs economist?)\n",
    "- in this function remove stop words from the description & title + stem the words\n",
    "\n",
    "## move these functions to a script and then import \n",
    "\n",
    "- doesn't work? \n",
    "\n",
    "max_df = 1.0 (**NEEDS TO BE 1.0 OTHERWISE IT'S NOT TREATED AS A PERCENTAGE and you \"ignore words that only appear in more than 1 document\" so you get all the esoteric words that only appear in one course description**)\n",
    "\n",
    "### double check this is the correct way to make predictions on the test set and make sure you understand what information is contained in `sorted_frame` \n",
    "- does predicted_word_1 correspond to the highest probability?\n",
    "- how does it index into vocab frame when vocab was obtained across all the descriptions?  no, it's only from the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "TRAINING_DIR = os.getcwd()\n",
    "vectorfile = os.path.join(TRAINING_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(TRAINING_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = os.getcwd()\n",
    "DATA_DIR = './data'\n",
    "vectorfile = os.path.join(DATA_DIR, 'course_vecs.tsv')\n",
    "infofile = os.path.join(DATA_DIR, 'course_info.tsv')\n",
    "textcolumn = 'course_description'\n",
    "num_top_words = 10\n",
    "use_idf = True\n",
    "tf_bias = .5\n",
    "num_epochs = 5\n",
    "max_df = 0.0028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(course_vecs, course_descipts, trained_weights, trained_biases, num_words_per_course):\n",
    "    \"\"\"\n",
    "    lalalal\n",
    "    \n",
    "    \"\"\"\n",
    "    df_with_keywords = course_descipts.copy()\n",
    "    softmax_frame = course_vecs.iloc[:,1:].dot(trained_weights.values) + trained_biases # make predictions\n",
    "\n",
    "    # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "    print('[INFO] Sorting classification results...')\n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_words_per_course:]\n",
    "\n",
    "    print('[INFO] Predicting top k inferred keywords for each course...')\n",
    "    for i in range(num_words_per_course):\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        df_with_keywords['predicted_word_' + str(num_words_per_course-i)] = new_col.values\n",
    "        \n",
    "    return df_with_keywords\n",
    "\n",
    "def calculate_metric(df_with_keywords, metric):\n",
    "    \"\"\"\n",
    "    metrics: {r: recall, p: precision}\n",
    "    \"\"\"\n",
    "    def clean_descrip_title(row):\n",
    "        punc_remover = str.maketrans('', '', string.punctuation)\n",
    "        lowered = row['descrip_title'].lower()\n",
    "        lowered_removed_punc = lowered.translate(punc_remover)\n",
    "        cleaned_set = set(lowered_removed_punc.split())\n",
    "        return cleaned_set\n",
    "\n",
    "    def recall_keywords(row):\n",
    "        return row['description_title_set'].intersection(row['course_keywords_set'])\n",
    "    \n",
    "    prediction_df = df_with_keywords.copy()\n",
    "    only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'course_subject', 'course_alternative_names'])]\n",
    "    num_keywords_predicted_per_course = only_predicted_keywords_df.shape[1]\n",
    "    prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "    prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "    prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "    \n",
    "    if metric == 'r':\n",
    "        print('[INFO] Calculating Recall...')\n",
    "        assert num_keywords_predicted_per_course == max_descript_len, 'Number of keywords predicted should equal longest description length'\n",
    "        prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "        average_recall = np.mean(prediction_df['recall'])\n",
    "        return average_recall\n",
    "    if metric == 'p':\n",
    "        print('[INFO] Calculating Precision...')\n",
    "        assert num_keywords_predicted_per_course == num_top_words, 'Number of keywords predicted should equal number of predicted words per course'\n",
    "        prediction_df['precision'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / num_top_words)\n",
    "        average_precision = np.mean(prediction_df['precision'])\n",
    "        return average_precision\n",
    "    if metric == 'c':\n",
    "        print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "        predicted_keyword_list = only_predicted_keywords_df.values.tolist()\n",
    "        predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "        keyword_counter = Counter(predicted_keyword_list)\n",
    "        print('[INFO] Most common keywords by count: ', keyword_counter.most_common(10))\n",
    "        \n",
    "        num_possible_keywords = df_with_keywords.shape[0] * num_top_words\n",
    "        num_predicted_keywords = len(keyword_counter.keys())\n",
    "        assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "        'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "        unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "        predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "        assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "        'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "    \n",
    "        cos_sim = cosine(predicted_keyword_vector, unif_keyword_vector)\n",
    "        return cos_sim\n",
    "    if metric == 'df':\n",
    "        print('[INFO] Calculating Document Frequency of Predicted Keywords across Course Subjects...')\n",
    "        document_df_cols = df_with_keywords.columns.difference(['course_title', 'course_description', 'course_name', 'course_alternative_names'])\n",
    "        document_df = df_with_keywords.loc[:,document_df_cols]\n",
    "        document_df.set_index('course_subject', inplace=True)\n",
    "        \n",
    "        document_dict = defaultdict(list)\n",
    "        terms = set()\n",
    "        for index, row in document_df.iterrows():\n",
    "            document_dict[index].extend(row.values)\n",
    "            terms.update(row.values)\n",
    "\n",
    "        doc_freq_dict = defaultdict()\n",
    "        num_docs = len(document_dict.keys())\n",
    "        for term in terms:\n",
    "            doc_freq_i = 0\n",
    "            for key in document_dict.keys():\n",
    "                if term in document_dict.get(key):\n",
    "                    doc_freq_i += 1\n",
    "            doc_freq_dict[term] = doc_freq_i / (num_docs)\n",
    "            \n",
    "        print('[INFO] Most common keywords by document frequencies: ', Counter(doc_freq_dict).most_common(10)) \n",
    "        average_document_frequency_score = np.mean(list(doc_freq_dict.values()))\n",
    "        return average_document_frequency_score\n",
    "        \n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(dataframe, column):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[UNIGRAMS] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[BIGRAMS] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(bigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[TRIGRAMS] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bag_of_words(dataframe, column, vocab):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descrip_title(row):\n",
    "    punc_remover = str.maketrans('', '', string.punctuation)\n",
    "    lowered = row['descrip_title'].lower()\n",
    "    lowered_removed_punc = lowered.translate(punc_remover)\n",
    "    cleaned_set = set(lowered_removed_punc.split())\n",
    "    return cleaned_set\n",
    "\n",
    "def recall_keywords(row):\n",
    "    return row['description_title_set'].intersection(row['keywords_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def test(x=1):\n",
    "    print(x)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "f = lambda x=1: print(x)\n",
    "f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-fold validation / only Train - test split for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_subject</th>\n",
       "      <th>course_alternative_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xart 98</td>\n",
       "      <td>Directed Group Study</td>\n",
       "      <td>This is a student-initiated course to be offer...</td>\n",
       "      <td>FPF-Art Practice</td>\n",
       "      <td>FPF-Art Practice 98 XART98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xanthro 2ac</td>\n",
       "      <td>Introduction to Archaeology</td>\n",
       "      <td>Prehistory and cultural growth. Introduction t...</td>\n",
       "      <td>FPF-Anthropology</td>\n",
       "      <td>FPF-Anthropology 2AC XANTHRO2AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xstat 2</td>\n",
       "      <td>Introduction to Statistics</td>\n",
       "      <td>Population and variables. Standard measures of...</td>\n",
       "      <td>FPF-Statistics</td>\n",
       "      <td>FPF-Statistics 2 XSTAT2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xmath 1b</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>Continuation of 1A. Techniques of integration;...</td>\n",
       "      <td>FPF-Mathematics</td>\n",
       "      <td>FPF-Mathematics 1B XMATH1B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xphilos 3</td>\n",
       "      <td>The Nature of Mind</td>\n",
       "      <td>Introduction to the philosophy of mind. Topics...</td>\n",
       "      <td>FPF-Philosophy</td>\n",
       "      <td>FPF-Philosophy 3 XPHILOS3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   course_name                 course_title  \\\n",
       "0      Xart 98         Directed Group Study   \n",
       "1  Xanthro 2ac  Introduction to Archaeology   \n",
       "2      Xstat 2   Introduction to Statistics   \n",
       "3     Xmath 1b                     Calculus   \n",
       "4    Xphilos 3           The Nature of Mind   \n",
       "\n",
       "                                  course_description    course_subject  \\\n",
       "0  This is a student-initiated course to be offer...  FPF-Art Practice   \n",
       "1  Prehistory and cultural growth. Introduction t...  FPF-Anthropology   \n",
       "2  Population and variables. Standard measures of...    FPF-Statistics   \n",
       "3  Continuation of 1A. Techniques of integration;...   FPF-Mathematics   \n",
       "4  Introduction to the philosophy of mind. Topics...    FPF-Philosophy   \n",
       "\n",
       "          course_alternative_names  \n",
       "0       FPF-Art Practice 98 XART98  \n",
       "1  FPF-Anthropology 2AC XANTHRO2AC  \n",
       "2          FPF-Statistics 2 XSTAT2  \n",
       "3       FPF-Mathematics 1B XMATH1B  \n",
       "4        FPF-Philosophy 3 XPHILOS3  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "info_frame = pd.read_csv(infofile, sep = '\\t') # Course information\n",
    "info_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info_frame.course_subject.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_frame['abbr_cid'] = info_frame.course_name.str.replace(' ', '_').str.upper()\n",
    "# api_df = pd.read_csv('/home/matthew/Models-AskOski/shared/course_info.tsv', sep = '\\t', ).drop(['Unnamed: 0', 'idx', 'updated_date'],axis=1)\n",
    "# api_df.head(5) \n",
    "# any(api_df.course_subject.isnull())\n",
    "# temp = pd.merge(info_frame, api_df, on='abbr_cid')[['course_name', 'course_title', 'course_description', 'course_alternative_names', 'course_subject']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonempty_indices = np.where(info_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = info_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5901 1476\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(filtered_vec_df, filtered_descript_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Getting vocab...\n",
      "[UNIGRAMS] Number of unigrams: 11580\n",
      "[BIGRAMS] Number of bigrams: 1158\n",
      "[TRIGRAMS] Number of trigrams: 115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(Y_train, textcolumn) # get_vocab(raw_frame, textcolumn) \n",
    "vocab_frame = pd.DataFrame(vocab)\n",
    "    \n",
    "vocabsize = len(vocab)\n",
    "\n",
    "# Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "Y_train_BOW = to_bag_of_words(Y_train, textcolumn, vocab)\n",
    "Y_train_BOW = Y_train_BOW.toarray()\n",
    "Y_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 474us/step - loss: 18861.4395 - acc: 0.0134\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 286us/step - loss: 17395.1526 - acc: 0.0515\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 286us/step - loss: 16529.8059 - acc: 0.0791\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 289us/step - loss: 15801.0023 - acc: 0.0929\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 291us/step - loss: 15172.5785 - acc: 0.1120\n"
     ]
    }
   ],
   "source": [
    "(weights_frame, biases) = logistic_regression(X_train.iloc[:,1:], Y_train_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n"
     ]
    }
   ],
   "source": [
    "softmax_frame = X_test.iloc[:,1:].dot(weights_frame.values) + biases\n",
    "print('[INFO] Sorting classification results...')\n",
    "sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "predicted_keyword_list = []\n",
    "for i in range(num_top_words):\n",
    "    new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "    predicted_keyword_list.extend(new_col.values)\n",
    "    Y_test['predicted_word_' + str(num_top_words-i)] = new_col.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_subject</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_9</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>Theater 52ac</td>\n",
       "      <td>Dance in American Cultures</td>\n",
       "      <td>Dance as a meaning-making expressive form.  De...</td>\n",
       "      <td>Theater Dance &amp; Perf Stds</td>\n",
       "      <td>Theater Dance &amp; Perf Stds 52AC THEATER52AC</td>\n",
       "      <td>explorations</td>\n",
       "      <td>theatre</td>\n",
       "      <td>recording</td>\n",
       "      <td>formations</td>\n",
       "      <td>allowing</td>\n",
       "      <td>performing</td>\n",
       "      <td>performances</td>\n",
       "      <td>supplies</td>\n",
       "      <td>vocal</td>\n",
       "      <td>productions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>Slavic 138</td>\n",
       "      <td>Topics in Russian and Soviet Film</td>\n",
       "      <td>This course will examine the Russian contribut...</td>\n",
       "      <td>Slavic Languages &amp; Lit</td>\n",
       "      <td>Slavic Languages &amp; Lit 138 SLAVIC138</td>\n",
       "      <td>folklore</td>\n",
       "      <td>looking</td>\n",
       "      <td>aspect</td>\n",
       "      <td>soviet</td>\n",
       "      <td>beginner</td>\n",
       "      <td>avant</td>\n",
       "      <td>viewing</td>\n",
       "      <td>garde</td>\n",
       "      <td>russia</td>\n",
       "      <td>slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>Art 301</td>\n",
       "      <td>The Teaching of Art: Practice</td>\n",
       "      <td>Utilizing aspects of pedagogical and andragogi...</td>\n",
       "      <td>Art Practice</td>\n",
       "      <td>Art Practice 301 ART301</td>\n",
       "      <td>notion</td>\n",
       "      <td>communicating</td>\n",
       "      <td>centered</td>\n",
       "      <td>print</td>\n",
       "      <td>audience</td>\n",
       "      <td>avant</td>\n",
       "      <td>garde</td>\n",
       "      <td>performances</td>\n",
       "      <td>recording</td>\n",
       "      <td>talks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>Env sci 100</td>\n",
       "      <td>Introduction to the Methods of Environmental S...</td>\n",
       "      <td>Introduction to basic methods used in environm...</td>\n",
       "      <td>Environmental Sciences</td>\n",
       "      <td>Environmental Sciences 100 ENV SCI100</td>\n",
       "      <td>remote</td>\n",
       "      <td>fate</td>\n",
       "      <td>greatest</td>\n",
       "      <td>habitat</td>\n",
       "      <td>trip</td>\n",
       "      <td>insects</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>wildlife</td>\n",
       "      <td>geologic</td>\n",
       "      <td>espm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>Nuc eng 221</td>\n",
       "      <td>Corrosion in Nuclear Power Systems</td>\n",
       "      <td>Structural metals in nuclear power plants; pro...</td>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Nuclear Engineering 221 NUC ENG221</td>\n",
       "      <td>calculations</td>\n",
       "      <td>radioactive</td>\n",
       "      <td>fast</td>\n",
       "      <td>neutron</td>\n",
       "      <td>beam</td>\n",
       "      <td>fission</td>\n",
       "      <td>reactors</td>\n",
       "      <td>fuel</td>\n",
       "      <td>fusion</td>\n",
       "      <td>reactor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       course_name                                       course_title  \\\n",
       "2288  Theater 52ac                         Dance in American Cultures   \n",
       "6602    Slavic 138                  Topics in Russian and Soviet Film   \n",
       "4483       Art 301                      The Teaching of Art: Practice   \n",
       "5321   Env sci 100  Introduction to the Methods of Environmental S...   \n",
       "5447   Nuc eng 221                 Corrosion in Nuclear Power Systems   \n",
       "\n",
       "                                     course_description  \\\n",
       "2288  Dance as a meaning-making expressive form.  De...   \n",
       "6602  This course will examine the Russian contribut...   \n",
       "4483  Utilizing aspects of pedagogical and andragogi...   \n",
       "5321  Introduction to basic methods used in environm...   \n",
       "5447  Structural metals in nuclear power plants; pro...   \n",
       "\n",
       "                 course_subject                    course_alternative_names  \\\n",
       "2288  Theater Dance & Perf Stds  Theater Dance & Perf Stds 52AC THEATER52AC   \n",
       "6602     Slavic Languages & Lit        Slavic Languages & Lit 138 SLAVIC138   \n",
       "4483               Art Practice                     Art Practice 301 ART301   \n",
       "5321     Environmental Sciences       Environmental Sciences 100 ENV SCI100   \n",
       "5447        Nuclear Engineering          Nuclear Engineering 221 NUC ENG221   \n",
       "\n",
       "     predicted_word_10 predicted_word_9 predicted_word_8 predicted_word_7  \\\n",
       "2288      explorations          theatre        recording       formations   \n",
       "6602          folklore          looking           aspect           soviet   \n",
       "4483            notion    communicating         centered            print   \n",
       "5321            remote             fate         greatest          habitat   \n",
       "5447      calculations      radioactive             fast          neutron   \n",
       "\n",
       "     predicted_word_6 predicted_word_5 predicted_word_4 predicted_word_3  \\\n",
       "2288         allowing       performing     performances         supplies   \n",
       "6602         beginner            avant          viewing            garde   \n",
       "4483         audience            avant            garde     performances   \n",
       "5321             trip          insects     agricultural         wildlife   \n",
       "5447             beam          fission         reactors             fuel   \n",
       "\n",
       "     predicted_word_2 predicted_word_1  \n",
       "2288            vocal      productions  \n",
       "6602           russia           slavic  \n",
       "4483        recording            talks  \n",
       "5321         geologic             espm  \n",
       "5447           fusion          reactor  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1476, 11)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Y_test[Y_test.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_word_1</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course_subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Engineering</th>\n",
       "      <td>sponsoring</td>\n",
       "      <td>income</td>\n",
       "      <td>professionally</td>\n",
       "      <td>matlab</td>\n",
       "      <td>homework</td>\n",
       "      <td>layout</td>\n",
       "      <td>dependent</td>\n",
       "      <td>advisor</td>\n",
       "      <td>quickly</td>\n",
       "      <td>allocation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>powers</td>\n",
       "      <td>rule</td>\n",
       "      <td>edu</td>\n",
       "      <td>russia</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>rome</td>\n",
       "      <td>trace</td>\n",
       "      <td>evolved</td>\n",
       "      <td>looking</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Health</th>\n",
       "      <td>concern</td>\n",
       "      <td>survival</td>\n",
       "      <td>fertility</td>\n",
       "      <td>demographic</td>\n",
       "      <td>residents</td>\n",
       "      <td>demography</td>\n",
       "      <td>credential</td>\n",
       "      <td>influenced</td>\n",
       "      <td>multivariate</td>\n",
       "      <td>generalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evening &amp; Weekend MBA</th>\n",
       "      <td>arbitrage</td>\n",
       "      <td>dependent</td>\n",
       "      <td>company</td>\n",
       "      <td>managed</td>\n",
       "      <td>workers</td>\n",
       "      <td>resulting</td>\n",
       "      <td>enabling</td>\n",
       "      <td>drug</td>\n",
       "      <td>practiced</td>\n",
       "      <td>diagnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>did</td>\n",
       "      <td>socialism</td>\n",
       "      <td>powers</td>\n",
       "      <td>cold</td>\n",
       "      <td>rural</td>\n",
       "      <td>trace</td>\n",
       "      <td>formations</td>\n",
       "      <td>modernism</td>\n",
       "      <td>struggles</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      predicted_word_1 predicted_word_10 predicted_word_2  \\\n",
       "course_subject                                                              \n",
       "Engineering                 sponsoring            income   professionally   \n",
       "History                         powers              rule              edu   \n",
       "Public Health                  concern          survival        fertility   \n",
       "Evening & Weekend MBA        arbitrage         dependent          company   \n",
       "History                            did         socialism           powers   \n",
       "\n",
       "                      predicted_word_3 predicted_word_4 predicted_word_5  \\\n",
       "course_subject                                                             \n",
       "Engineering                     matlab         homework           layout   \n",
       "History                         russia     agricultural             rome   \n",
       "Public Health              demographic        residents       demography   \n",
       "Evening & Weekend MBA          managed          workers        resulting   \n",
       "History                           cold            rural            trace   \n",
       "\n",
       "                      predicted_word_6 predicted_word_7 predicted_word_8  \\\n",
       "course_subject                                                             \n",
       "Engineering                  dependent          advisor          quickly   \n",
       "History                          trace          evolved          looking   \n",
       "Public Health               credential       influenced     multivariate   \n",
       "Evening & Weekend MBA         enabling             drug        practiced   \n",
       "History                     formations        modernism        struggles   \n",
       "\n",
       "                      predicted_word_9  \n",
       "course_subject                          \n",
       "Engineering                 allocation  \n",
       "History                           cold  \n",
       "Public Health              generalized  \n",
       "Evening & Weekend MBA       diagnostic  \n",
       "History                          peace  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freq_df_cols = Y_test.columns.difference(['course_title', 'course_description', 'course_name', 'course_alternative_names'])\n",
    "doc_df = Y_test.loc[:,doc_freq_df_cols]\n",
    "test = doc_df.loc[doc_df.course_subject.str.contains('History')]\n",
    "doc_df.set_index('course_subject', inplace=True)\n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_word_1</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>course_subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>powers</td>\n",
       "      <td>rule</td>\n",
       "      <td>edu</td>\n",
       "      <td>russia</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>rome</td>\n",
       "      <td>trace</td>\n",
       "      <td>evolved</td>\n",
       "      <td>looking</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>did</td>\n",
       "      <td>socialism</td>\n",
       "      <td>powers</td>\n",
       "      <td>cold</td>\n",
       "      <td>rural</td>\n",
       "      <td>trace</td>\n",
       "      <td>formations</td>\n",
       "      <td>modernism</td>\n",
       "      <td>struggles</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>iran</td>\n",
       "      <td>performing</td>\n",
       "      <td>france</td>\n",
       "      <td>did</td>\n",
       "      <td>crisis</td>\n",
       "      <td>west</td>\n",
       "      <td>evolved</td>\n",
       "      <td>supplies</td>\n",
       "      <td>enlightenment</td>\n",
       "      <td>republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History of Art</th>\n",
       "      <td>economists</td>\n",
       "      <td>allocation</td>\n",
       "      <td>square</td>\n",
       "      <td>investigating</td>\n",
       "      <td>correlation</td>\n",
       "      <td>income</td>\n",
       "      <td>outcome</td>\n",
       "      <td>solubility</td>\n",
       "      <td>fair</td>\n",
       "      <td>optometric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History of Art</th>\n",
       "      <td>doe</td>\n",
       "      <td>france</td>\n",
       "      <td>listings</td>\n",
       "      <td>formations</td>\n",
       "      <td>audience</td>\n",
       "      <td>spain</td>\n",
       "      <td>desirable</td>\n",
       "      <td>ca</td>\n",
       "      <td>ideological</td>\n",
       "      <td>modernism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted_word_1 predicted_word_10 predicted_word_2  \\\n",
       "course_subject                                                       \n",
       "History                  powers              rule              edu   \n",
       "History                     did         socialism           powers   \n",
       "History                    iran        performing           france   \n",
       "History of Art       economists        allocation           square   \n",
       "History of Art              doe            france         listings   \n",
       "\n",
       "               predicted_word_3 predicted_word_4 predicted_word_5  \\\n",
       "course_subject                                                      \n",
       "History                  russia     agricultural             rome   \n",
       "History                    cold            rural            trace   \n",
       "History                     did           crisis             west   \n",
       "History of Art    investigating      correlation           income   \n",
       "History of Art       formations         audience            spain   \n",
       "\n",
       "               predicted_word_6 predicted_word_7 predicted_word_8  \\\n",
       "course_subject                                                      \n",
       "History                   trace          evolved          looking   \n",
       "History              formations        modernism        struggles   \n",
       "History                 evolved         supplies    enlightenment   \n",
       "History of Art          outcome       solubility             fair   \n",
       "History of Art        desirable               ca      ideological   \n",
       "\n",
       "               predicted_word_9  \n",
       "course_subject                   \n",
       "History                    cold  \n",
       "History                   peace  \n",
       "History                republic  \n",
       "History of Art       optometric  \n",
       "History of Art        modernism  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.set_index('course_subject', inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict = defaultdict(list)\n",
    "terms = set()\n",
    "for index,row in doc_df.iterrows():\n",
    "    document_dict[index].extend(row.values)\n",
    "    terms.update(row.values)\n",
    "# doc_freq_dict\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs = len(document_dict.keys())\n",
    "doc_freq_i = 0\n",
    "for key in document_dict.keys():\n",
    "    if 'performing' in document_dict.get(key):\n",
    "        doc_freq_i += 1\n",
    "#         print(doc_freq_i)\n",
    "doc_freq_i / num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_freq_dict = defaultdict()\n",
    "num_docs = len(document_dict.keys())\n",
    "for term in terms:\n",
    "    doc_freq_i = 0\n",
    "#     print(term)\n",
    "    for key in document_dict.keys():\n",
    "        if term in document_dict.get(key):\n",
    "            doc_freq_i += 1\n",
    "    doc_freq_dict[term] = doc_freq_i / (num_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('powers', 0.2822085889570552),\n",
       " ('formations', 0.27607361963190186),\n",
       " ('performing', 0.25766871165644173),\n",
       " ('agricultural', 0.24539877300613497),\n",
       " ('trace', 0.22085889570552147),\n",
       " ('informed', 0.2147239263803681),\n",
       " ('crisis', 0.20245398773006135),\n",
       " ('did', 0.17791411042944785),\n",
       " ('modernism', 0.17791411042944785),\n",
       " ('recording', 0.17791411042944785)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(doc_freq_dict).most_common(10)\n",
    "# sorted(doc_freq_dict, key=doc_freq_dict.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024354106442170894"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(doc_freq_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trace', 150),\n",
       " ('powers', 148),\n",
       " ('formations', 133),\n",
       " ('agricultural', 111),\n",
       " ('performing', 108),\n",
       " ('struggles', 92),\n",
       " ('looking', 91),\n",
       " ('income', 90),\n",
       " ('authority', 86),\n",
       " ('crisis', 84),\n",
       " ('did', 80),\n",
       " ('chronological', 72),\n",
       " ('evolved', 71),\n",
       " ('modernism', 69),\n",
       " ('republic', 68)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keyword_counter = Counter(predicted_keyword_list)\n",
    "\n",
    "keyword_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACADEMIC_DEPARTMENT_NAME</th>\n",
       "      <th>ACADEMIC_DIVISION_NAME</th>\n",
       "      <th>MAJOR_NAME</th>\n",
       "      <th>COLLEGE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>African American Studies</td>\n",
       "      <td>L&amp;S Social Sciences Division</td>\n",
       "      <td>Afr Amer Stds-Humanities</td>\n",
       "      <td>Clg of Letters &amp; Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>African American Studies</td>\n",
       "      <td>L&amp;S Social Sciences Division</td>\n",
       "      <td>Afr Amer Stds-Social Sci</td>\n",
       "      <td>Clg of Letters &amp; Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>African American Studies</td>\n",
       "      <td>L&amp;S Social Sciences Division</td>\n",
       "      <td>African American Studies</td>\n",
       "      <td>Clg of Letters &amp; Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ag &amp; Env Chem Grad Grp</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "      <td>Ag &amp; Environmental Chem</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ag &amp; Resource Econ &amp; Pol</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "      <td>Ag &amp; Resource Economics</td>\n",
       "      <td>Clg of Natural Resources</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACADEMIC_DEPARTMENT_NAME        ACADEMIC_DIVISION_NAME  \\\n",
       "0  African American Studies  L&S Social Sciences Division   \n",
       "1  African American Studies  L&S Social Sciences Division   \n",
       "2  African American Studies  L&S Social Sciences Division   \n",
       "3    Ag & Env Chem Grad Grp      Clg of Natural Resources   \n",
       "4  Ag & Resource Econ & Pol      Clg of Natural Resources   \n",
       "\n",
       "                 MAJOR_NAME              COLLEGE_NAME  \n",
       "0  Afr Amer Stds-Humanities  Clg of Letters & Science  \n",
       "1  Afr Amer Stds-Social Sci  Clg of Letters & Science  \n",
       "2  African American Studies  Clg of Letters & Science  \n",
       "3   Ag & Environmental Chem  Clg of Natural Resources  \n",
       "4   Ag & Resource Economics  Clg of Natural Resources  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_file = infofile = os.path.join(TRAINING_DIR, 'academic_departments.tsv')\n",
    "dpt_df = pd.read_csv(dpt_file, sep='\\t')\n",
    "dpt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACADEMIC_DEPARTMENT_NAME</th>\n",
       "      <th>ACADEMIC_DIVISION_NAME</th>\n",
       "      <th>MAJOR_NAME</th>\n",
       "      <th>COLLEGE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bioengineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Bioengineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bioengineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Translational Medicine</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bioengineering-UCSF Grad Grp</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Bioengineering (UCSF)</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bioengineering-UCSF Grad Grp</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Translational Medicine (UCSF)</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>BioE/MSE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>EECS/MSE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>EECS/NE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>ME/NE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>MSE/ME Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Engineering Joint Programs</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>MSE/NE Joint Major</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Computational Eng Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Energy Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Engineering Physics</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Engineering Undeclared</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Eng Math &amp; Statistics</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Environmental Eng Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Engineering Science</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Manufacturing Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Mechanical Engineering PT MEng</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Nuclear Engineering</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "      <td>Nuclear Engineering PT MEng</td>\n",
       "      <td>Clg of Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ACADEMIC_DEPARTMENT_NAME ACADEMIC_DIVISION_NAME  \\\n",
       "15                 Bioengineering     Clg of Engineering   \n",
       "16                 Bioengineering     Clg of Engineering   \n",
       "17   Bioengineering-UCSF Grad Grp     Clg of Engineering   \n",
       "18   Bioengineering-UCSF Grad Grp     Clg of Engineering   \n",
       "70     Engineering Joint Programs     Clg of Engineering   \n",
       "71     Engineering Joint Programs     Clg of Engineering   \n",
       "72     Engineering Joint Programs     Clg of Engineering   \n",
       "73     Engineering Joint Programs     Clg of Engineering   \n",
       "74     Engineering Joint Programs     Clg of Engineering   \n",
       "75     Engineering Joint Programs     Clg of Engineering   \n",
       "76            Engineering Science     Clg of Engineering   \n",
       "77            Engineering Science     Clg of Engineering   \n",
       "78            Engineering Science     Clg of Engineering   \n",
       "79            Engineering Science     Clg of Engineering   \n",
       "80            Engineering Science     Clg of Engineering   \n",
       "81            Engineering Science     Clg of Engineering   \n",
       "82            Engineering Science     Clg of Engineering   \n",
       "167        Mechanical Engineering     Clg of Engineering   \n",
       "168        Mechanical Engineering     Clg of Engineering   \n",
       "190           Nuclear Engineering     Clg of Engineering   \n",
       "191           Nuclear Engineering     Clg of Engineering   \n",
       "\n",
       "                         MAJOR_NAME        COLLEGE_NAME  \n",
       "15                   Bioengineering  Clg of Engineering  \n",
       "16           Translational Medicine  Clg of Engineering  \n",
       "17            Bioengineering (UCSF)  Clg of Engineering  \n",
       "18    Translational Medicine (UCSF)  Clg of Engineering  \n",
       "70             BioE/MSE Joint Major  Clg of Engineering  \n",
       "71             EECS/MSE Joint Major  Clg of Engineering  \n",
       "72              EECS/NE Joint Major  Clg of Engineering  \n",
       "73                ME/NE Joint Major  Clg of Engineering  \n",
       "74               MSE/ME Joint Major  Clg of Engineering  \n",
       "75               MSE/NE Joint Major  Clg of Engineering  \n",
       "76        Computational Eng Science  Clg of Engineering  \n",
       "77               Energy Engineering  Clg of Engineering  \n",
       "78              Engineering Physics  Clg of Engineering  \n",
       "79           Engineering Undeclared  Clg of Engineering  \n",
       "80            Eng Math & Statistics  Clg of Engineering  \n",
       "81        Environmental Eng Science  Clg of Engineering  \n",
       "82        Manufacturing Engineering  Clg of Engineering  \n",
       "167          Mechanical Engineering  Clg of Engineering  \n",
       "168  Mechanical Engineering PT MEng  Clg of Engineering  \n",
       "190             Nuclear Engineering  Clg of Engineering  \n",
       "191     Nuclear Engineering PT MEng  Clg of Engineering  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_df.loc[dpt_df.ACADEMIC_DEPARTMENT_NAME.str.contains('engineering', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_possible_keywords = Y_test.shape[0] * num_top_words\n",
    "num_predicted_keywords = len(keyword_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(keyword_counter.values()) == Y_test.shape[0] * num_top_words,\\\n",
    "'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.97012302, 12.97012302, 12.97012302, ..., 12.97012302,\n",
       "       12.97012302, 12.97012302])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "unif_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41,  1,  1, ...,  2,  5, 48])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "predicted_keyword_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unif_keyword_vector.shape == predicted_keyword_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3620480643963293"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)\n",
    "\n",
    "cosine_similarity(predicted_keyword_vector, unif_keyword_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([1,-1], [1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation:\n",
    "\n",
    "1. Recall\n",
    "1. Distribution\n",
    "1. Tf-idf\n",
    "1. Train each model on your courses & self-evaluate efficacy of keywords for categorizing (general) + efficacy for granular details (specific)\n",
    "\n",
    "Possible Parameters for Grid Search:\n",
    "\n",
    "- max_df (limit the commonly occuring words)  \n",
    "- max_features (vocab size) \n",
    "- use_idf to set the weights as tf-idf instead of counts\n",
    "- num_epochs \n",
    "- try training on subset? (no, the model needs to see every possible word it can predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## move these functions to a script and then import \n",
    "\n",
    "- doesn't work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation:\n",
    "\n",
    "1. Recall\n",
    "1. Distribution\n",
    "1. Tf-idf\n",
    "1. Train each model on your courses & self-evaluate efficacy of keywords for categorizing (general) + efficacy for granular details (specific)\n",
    "\n",
    "Possible Parameters for Grid Search:\n",
    "\n",
    "- max_df (limit the commonly occuring words)  \n",
    "- max_features (vocab size) \n",
    "- use_idf to set the weights as tf-idf instead of counts\n",
    "- num_epochs \n",
    "- try training on subset? (no, the model needs to see every possible word it can predict)\n",
    "\n",
    "### max_df is a fixed hyperparameter that needs to be part of the grid search, what should the default level be?\n",
    "\n",
    "- max document frequency = removing corpus-specific stop words\n",
    "- what does this error mean `ValueError: max_df corresponds to < documents than min_df` and why does it occur was max_df = 0?\n",
    "\n",
    "### does getting bigrams and trigrams separately make a difference?\n",
    "\n",
    "- try getting the vocab using ngram_range = 1,3 and no limit on max_features\n",
    "\n",
    "### tf_bias is another fixed hyperparam that should be part of the grid search, default level is currently .5\n",
    "- higher level = more specific\n",
    "\n",
    "## remark: \n",
    "- maybe calculate recall using stemmed words (e.g. economy vs economist?)\n",
    "- in this function remove stop words from the description & title + stem the words\n",
    "\n",
    "## move these functions to a script and then import \n",
    "\n",
    "- doesn't work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeout_decorator.timeout(600, exception_message='timeout occured at get_vocab')\n",
    "def get_vocab(dataframe, column):\n",
    "    print(\"[INFO] Getting vocab...\")\n",
    "\n",
    "    dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    # max_df_param = 0.0028  # 1.0 # 0.0036544883\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(1,1), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    unigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of unigrams: %d' % (len(unigrams)))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(2,2), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    bigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of bigrams: %d' % (len(bigrams)))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = max_df, stop_words='english', ngram_range=(3,3), max_features=max(1, int(len(unigrams)/10)), use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    trigrams = vectorizer.get_feature_names()\n",
    "    print('[INFO] Number of trigrams: %d' % (len(trigrams)))\n",
    "\n",
    "    vocab = np.concatenate((unigrams, bigrams, trigrams))\n",
    "    vocab_list = list(vocab)\n",
    "    removed_numbers_list = [word for word in vocab_list if not any(char.isdigit() for char in word)]\n",
    "    vocab = np.array(removed_numbers_list)\n",
    "#     pd.DataFrame(vocab).to_csv(outputfile+'_vocab.tsv', sep = '\\t', encoding='utf-8', index = False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf_bias is another fixed hyperparam that should be part of the grid search, default level is currently .5\n",
    "- higher level = more specific\n",
    "\n",
    "## remark: \n",
    "- maybe calculate recall using stemmed words (e.g. economy vs economist?)\n",
    "- in this function remove stop words from the description & title + stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeout_decorator.timeout(600, exception_message='timeout occured at to_bag_of_words')\n",
    "def to_bag_of_words(dataframe, column, vocab):\n",
    "    \"\"\"Input: raw dataframe, text column, and vocabulary.\n",
    "    Returns a sparse matrix of the bag of words representation of the column.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', vocabulary=vocab, use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(dataframe[column].values.astype('U'))\n",
    "    if tf_bias == -999:\n",
    "        return X\n",
    "    return (X.multiply(1/X.count_nonzero())).power(-tf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeout_decorator.timeout(3600, exception_message='timeout occured at logistic_regression')\n",
    "def logistic_regression(X, Y):\n",
    "    print('[INFO] Performing logistic regression...')\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "#     print('input shape: ', X.shape[1])  # 300 = number of cols in the feature matrix?\n",
    "#     print('vocab size: ', vocabsize) # 2400 = len(get_vocab(raw_frame, textcolumn)) = num words parsed from description corpus\n",
    "#     x = Dense(30, activation='sigmoid')(inputs)\n",
    "#     predictions = Dense(vocabsize, activation='softmax')(x)\n",
    "    predictions = Dense(vocabsize, activation='softmax')(inputs)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=num_epochs)\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    biases = model.layers[1].get_weights()[1]\n",
    "    weights_frame = pd.DataFrame(weights)\n",
    "    biases_frame = pd.DataFrame(biases)\n",
    "#     weights_frame.to_csv(outputfile+'_weights.tsv', sep = '\\t', index = False)\n",
    "#     biases_frame.to_csv(outputfile+'_biases.tsv', sep = '\\t', index = False)\n",
    "    return(weights_frame, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(course_vecs, course_descipts, trained_weights, trained_biases, num_words_per_course):\n",
    "    \"\"\"\n",
    "    lalalal\n",
    "    \n",
    "    \"\"\"\n",
    "    df_with_keywords = course_descipts.copy()\n",
    "    softmax_frame = course_vecs.iloc[:,1:].dot(trained_weights.values) + trained_biases # make predictions\n",
    "\n",
    "    # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "    print('[INFO] Sorting classification results...')\n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_words_per_course:]\n",
    "\n",
    "    print('[INFO] Predicting top k inferred keywords for each course...')\n",
    "    for i in range(num_words_per_course):\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        df_with_keywords['predicted_word_' + str(num_words_per_course-i)] = new_col.values\n",
    "        \n",
    "    return df_with_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(df_with_keywords, metric):\n",
    "    \"\"\"\n",
    "    metrics: {r: recall, p: precision}\n",
    "    \"\"\"\n",
    "#     try: \n",
    "    prediction_df = df_with_keywords.copy()\n",
    "    only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "    num_keywords_predicted = only_predicted_keywords_df.shape[1]\n",
    "    prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "    prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "    prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "\n",
    "    if metric == 'r':\n",
    "        print('[INFO] Calculating Recall...')\n",
    "        assert num_keywords_predicted == max_descript_len, 'Number of keywords predicted should equal longest description length'\n",
    "        prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "        average_recall = np.mean(prediction_df['recall'])\n",
    "        return average_recall\n",
    "    if metric == 'p':\n",
    "        print('[INFO] Calculating Precision...')\n",
    "        assert num_keywords_predicted == num_top_words, 'Number of keywords predicted should equal number of predicted words per course'\n",
    "        prediction_df['precision'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / num_top_words)\n",
    "        average_precision = np.mean(prediction_df['precision'])\n",
    "        return average_precision\n",
    "    if metric == 'c':\n",
    "        print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "        predicted_keyword_list = only_predicted_keywords_df.values.tolist()\n",
    "        predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "        keyword_counter = Counter(predicted_keyword_list)\n",
    "\n",
    "        num_possible_keywords = df_with_keywords.shape[0] * num_top_words\n",
    "        num_predicted_keywords = len(keyword_counter.keys())\n",
    "        assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "        'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "        unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "        predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "        assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "        'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "\n",
    "        cos_sim = cosine_similarity(predicted_keyword_vector, unif_keyword_vector)\n",
    "        return cos_sim\n",
    "#         raise ValueError(\"Specify which metric to calculate.\")\n",
    "#     except TypeError as err: \n",
    "#         print(err.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remark: \n",
    "- maybe calculate recall using stemmed words (e.g. economy vs economist?)\n",
    "- in this function remove stop words from the description & title + stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descrip_title(row):\n",
    "    punc_remover = str.maketrans('', '', string.punctuation)\n",
    "    lowered = row['descrip_title'].lower()\n",
    "    lowered_removed_punc = lowered.translate(punc_remover)\n",
    "    cleaned_set = set(lowered_removed_punc.split())\n",
    "    return cleaned_set\n",
    "\n",
    "def recall_keywords(row):\n",
    "    return row['description_title_set'].intersection(row['course_keywords_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment Attempt with 1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_frame = pd.read_csv(vectorfile, sep = '\\t') # Vector space representation of each user, all numeric\n",
    "raw_frame = pd.read_csv(rawfile, sep = '\\t') # Course information\n",
    "\n",
    "nonempty_indices = np.where(raw_frame[textcolumn].notnull())[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:].reset_index(drop = True)\n",
    "filtered_descript_df = raw_frame.iloc[nonempty_indices,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(raw_frame[textcolumn].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can get the indices of the CV split and predict only on each subset\n",
    "\n",
    "- No it's more complicated than that you have to fit the model only on the subsets first, but then not all the words will be in the prediction vocabulary\n",
    "- Not necessarily a big deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_descript_index = filtered_descript_df.course_description.str.split().str.len().idxmax()\n",
    "max_descript_len = max(filtered_descript_df.course_description.str.split().str.len())\n",
    "max_descript_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "description_lengths = filtered_descript_df.course_description.str.split().str.len()\n",
    "\n",
    "plt.hist(description_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After World War II, we witnessed a \"revolution\" in human rights theory, practice, and institution building. The implications of viewing individuals as equal and endowed with certain rights is potentially far reaching as in the declaration that individuals hold many of those rights irrespective of the views of their government. Yet, we also live in a world of sovereign states with sovereign state\\'s rights. We see everyday a clash between the rights of the individual and lack of duty to fulfill those rights when an individual\\'s home state is unwilling or unable to do so. After introducing the idea of human rights, its historic development and various international human rights mechanisms, this course will ask what post-World War II conceptions of human rights mean for a number of specific issues including humanitarian intervention, international criminal justice, U.S. foreign policy, immigration, and economic rights. Looking in-depth at these five areas, we will ask how ideas about human rights, laws about human rights, and institutions to protect human rights have on how states and other global actors act, and how individuals have faired. '"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_descript_df.loc[max_descript_index, 'course_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### should get_vocab be from entire dataset or just the train split?\n",
    "- e.g. use all 13000 words or just 11000?\n",
    "- makes more sense to just use the train split because that preserves the integrity of the split, you shouldn't use data from your test set to fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df = .002\n",
    "\n",
    "- [UNIGRAMS] number unigrams: 11608\n",
    "- [BIGRAMS] number unigrams: 1160\n",
    "- [TRIGRAMS] number unigrams: 1160\n",
    "\n",
    "max_df = 1.0 (**NEEDS TO BE 1.0 OTHERWISE IT'S NOT TREATED AS A PERCENTAGE and you \"ignore words that only appear in more than 1 document\" so you get all the esoteric words that only appear in one course description**)\n",
    "- [UNIGRAMS] number unigrams: 13270\n",
    "- [BIGRAMS] number bigrams: 1327\n",
    "- [TRIGRAMS] number trigrams: 1327\n",
    "- (300, 15435)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5900 1476\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(filtered_vec_df, filtered_descript_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11608\n",
      "[INFO] Number of bigrams: 1160\n",
      "[INFO] Number of trigrams: 1160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(Y_train, textcolumn) # get_vocab(raw_frame, textcolumn) \n",
    "vocab_frame = pd.DataFrame(vocab)\n",
    "    \n",
    "vocabsize = len(vocab)\n",
    "\n",
    "# Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "Y_train_BOW = to_bag_of_words(Y_train, textcolumn, vocab)\n",
    "Y_train_BOW = Y_train_BOW.toarray()\n",
    "Y_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 568us/step - loss: 19122.8845 - acc: 0.0039\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 326us/step - loss: 17952.5103 - acc: 0.0347\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 334us/step - loss: 17293.3725 - acc: 0.0759\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 334us/step - loss: 16661.2146 - acc: 0.0936\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 330us/step - loss: 16069.7676 - acc: 0.1139\n"
     ]
    }
   ],
   "source": [
    "# Train the coefficients for the vectorspace factors to predict the bag of words\n",
    "# time_train_model_bf = time.time()\n",
    "\n",
    "# Only train on instances with non-empty texts\n",
    "\n",
    "(weights_frame, biases) = logistic_regression(X_train.iloc[:,1:], Y_train_BOW)\n",
    "# Obtain the softmax predictions for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 13451)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### double check this is the correct way to make predictions on the test set and make sure you understand what information is contained in `sorted_frame` \n",
    "- does predicted_word_1 correspond to the highest probability?\n",
    "- how does it index into vocab frame when vocab was obtained across all the descriptions?  no, it's only from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_9</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Engin 45</td>\n",
       "      <td>Properties of Materials</td>\n",
       "      <td>Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering</td>\n",
       "      <td>Engineering 45 ENGIN45</td>\n",
       "      <td>drug</td>\n",
       "      <td>metals</td>\n",
       "      <td>atomic</td>\n",
       "      <td>particles</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>bulk</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>kinetic</td>\n",
       "      <td>beam</td>\n",
       "      <td>polymers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>History 198bc</td>\n",
       "      <td>Berkeley Connect for Upper Division Students</td>\n",
       "      <td>Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.</td>\n",
       "      <td>History 198BC HISTORY198BC</td>\n",
       "      <td>http</td>\n",
       "      <td>violence</td>\n",
       "      <td>words</td>\n",
       "      <td>conquest</td>\n",
       "      <td>twentieth</td>\n",
       "      <td>called</td>\n",
       "      <td>powers</td>\n",
       "      <td>edu</td>\n",
       "      <td>peace</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_name                                  course_title  \\\n",
       "1103  Engin 45       Properties of Materials                        \n",
       "2338  History 198bc  Berkeley Connect for Upper Division Students   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             course_description  \\\n",
       "1103  Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering                                                                                                                                                                                                                                             \n",
       "2338  Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.   \n",
       "\n",
       "        course_alternative_names predicted_word_10 predicted_word_9  \\\n",
       "1103  Engineering 45 ENGIN45      drug              metals            \n",
       "2338  History 198BC HISTORY198BC  http              violence          \n",
       "\n",
       "     predicted_word_8 predicted_word_7 predicted_word_6 predicted_word_5  \\\n",
       "1103  atomic           particles        astronomy        bulk              \n",
       "2338  words            conquest         twentieth        called            \n",
       "\n",
       "     predicted_word_4 predicted_word_3 predicted_word_2 predicted_word_1  \n",
       "1103  sufficient       kinetic          beam             polymers         \n",
       "2338  powers           edu              peace            russia           "
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_keywords = predict(X_test, Y_test, weights_frame, biases, num_top_words)\n",
    "df_with_keywords.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Calculating Recall...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of keywords predicted should equal longest description length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-575-5e168050fb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_keywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-570-b4b7c8dd3c5d>\u001b[0m in \u001b[0;36mcalculate_metric\u001b[0;34m(df_with_keywords, metric)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Calculating Recall...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mnum_keywords_predicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_descript_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Number of keywords predicted should equal longest description length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprediction_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shared_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_descript_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0maverage_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of keywords predicted should equal longest description length"
     ]
    }
   ],
   "source": [
    "calculate_metric(df_with_keywords, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Calculating Precision...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.015322033898305085"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metric(df_with_keywords, 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top inferred keywords for each course...\n"
     ]
    }
   ],
   "source": [
    "# softmax_frame = X_test.iloc[:,1:].dot(weights_frame.values) + biases # make predictions\n",
    "\n",
    "# From the softmax predictions, save the top 10 predicted words for each data point\n",
    "print('[INFO] Sorting classification results...')\n",
    "sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "print('[INFO] Predicting top inferred keywords for each course...')\n",
    "for i in range(num_top_words):\n",
    "    new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "    Y_test['predicted_word_' + str(num_top_words-i)] = new_col.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sorting classification results...\n"
     ]
    }
   ],
   "source": [
    "# predict max_len number of words\n",
    "\n",
    "# softmax_frame = X_test.iloc[:,1:].dot(weights_frame.values) + biases # make predictions\n",
    "\n",
    "# # From the softmax predictions, save the top 10 predicted words for each data point\n",
    "# print('[INFO] Sorting classification results...')\n",
    "# sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-max_descript_len:]\n",
    "\n",
    "# for i in range(max_descript_len):\n",
    "#     new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "#     Y_test['predicted_word_' + str(max_descript_len-i)] = new_col.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_9</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Engin 45</td>\n",
       "      <td>Properties of Materials</td>\n",
       "      <td>Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering</td>\n",
       "      <td>Engineering 45 ENGIN45</td>\n",
       "      <td>drug</td>\n",
       "      <td>metals</td>\n",
       "      <td>atomic</td>\n",
       "      <td>particles</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>bulk</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>kinetic</td>\n",
       "      <td>beam</td>\n",
       "      <td>polymers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>History 198bc</td>\n",
       "      <td>Berkeley Connect for Upper Division Students</td>\n",
       "      <td>Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.</td>\n",
       "      <td>History 198BC HISTORY198BC</td>\n",
       "      <td>http</td>\n",
       "      <td>violence</td>\n",
       "      <td>words</td>\n",
       "      <td>conquest</td>\n",
       "      <td>twentieth</td>\n",
       "      <td>called</td>\n",
       "      <td>powers</td>\n",
       "      <td>edu</td>\n",
       "      <td>peace</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_name                                  course_title  \\\n",
       "1103  Engin 45       Properties of Materials                        \n",
       "2338  History 198bc  Berkeley Connect for Upper Division Students   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             course_description  \\\n",
       "1103  Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering                                                                                                                                                                                                                                             \n",
       "2338  Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.   \n",
       "\n",
       "        course_alternative_names predicted_word_10 predicted_word_9  \\\n",
       "1103  Engineering 45 ENGIN45      drug              metals            \n",
       "2338  History 198BC HISTORY198BC  http              violence          \n",
       "\n",
       "     predicted_word_8 predicted_word_7 predicted_word_6 predicted_word_5  \\\n",
       "1103  atomic           particles        astronomy        bulk              \n",
       "2338  words            conquest         twentieth        called            \n",
       "\n",
       "     predicted_word_4 predicted_word_3 predicted_word_2 predicted_word_1  \n",
       "1103  sufficient       kinetic          beam             polymers         \n",
       "2338  powers           edu              peace            russia           "
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_recall(df):\n",
    "#     prediction_df = df.copy()\n",
    "#     only_predicted_keywords_df = prediction_df[prediction_df.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "#     num_keywords_predicted = only_predicted_keywords_df.shape[1]\n",
    "#     prediction_df['course_keywords'] = only_predicted_keywords_df.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "#     prediction_df = prediction_df[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "#     prediction_df['course_keywords'] = prediction_df['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "#     prediction_df['course_keywords_set'] = prediction_df['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "#     prediction_df['descrip_title'] = prediction_df['course_title'] + ' ' + prediction_df['course_description']\n",
    "#     prediction_df['description_title_set'] = prediction_df.apply(clean_descrip_title, axis = 1)\n",
    "#     prediction_df['shared_words'] = prediction_df.apply(recall_keywords, axis = 1)\n",
    "#     prediction_df['recall'] = prediction_df['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "#     fold_i_average_recall = np.mean(prediction_df['recall'])\n",
    "#     return fold_i_average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13441</th>\n",
       "      <th>13442</th>\n",
       "      <th>13443</th>\n",
       "      <th>13444</th>\n",
       "      <th>13445</th>\n",
       "      <th>13446</th>\n",
       "      <th>13447</th>\n",
       "      <th>13448</th>\n",
       "      <th>13449</th>\n",
       "      <th>13450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>1256</td>\n",
       "      <td>7367</td>\n",
       "      <td>4211</td>\n",
       "      <td>920</td>\n",
       "      <td>6169</td>\n",
       "      <td>9469</td>\n",
       "      <td>5411</td>\n",
       "      <td>692</td>\n",
       "      <td>9597</td>\n",
       "      <td>7794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>1306</td>\n",
       "      <td>1168</td>\n",
       "      <td>4773</td>\n",
       "      <td>5950</td>\n",
       "      <td>11027</td>\n",
       "      <td>2019</td>\n",
       "      <td>3142</td>\n",
       "      <td>7439</td>\n",
       "      <td>7884</td>\n",
       "      <td>8988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      13441  13442  13443  13444  13445  13446  13447  13448  13449  13450\n",
       "1103  1256   7367   4211   920    6169   9469   5411   692    9597   7794 \n",
       "2338  1306   1168   4773   5950   11027  2019   3142   7439   7884   8988 "
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_frame.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13441</th>\n",
       "      <th>13442</th>\n",
       "      <th>13443</th>\n",
       "      <th>13444</th>\n",
       "      <th>13445</th>\n",
       "      <th>13446</th>\n",
       "      <th>13447</th>\n",
       "      <th>13448</th>\n",
       "      <th>13449</th>\n",
       "      <th>13450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>-13.619102</td>\n",
       "      <td>-12.834158</td>\n",
       "      <td>-13.334953</td>\n",
       "      <td>-12.993541</td>\n",
       "      <td>-12.190391</td>\n",
       "      <td>-12.566942</td>\n",
       "      <td>-13.327277</td>\n",
       "      <td>-12.202205</td>\n",
       "      <td>-13.228280</td>\n",
       "      <td>-11.900171</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.803132</td>\n",
       "      <td>-14.733360</td>\n",
       "      <td>-14.764064</td>\n",
       "      <td>-14.752999</td>\n",
       "      <td>-14.749906</td>\n",
       "      <td>-14.779751</td>\n",
       "      <td>-14.758421</td>\n",
       "      <td>-14.746018</td>\n",
       "      <td>-14.811518</td>\n",
       "      <td>-14.795294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>-22.217343</td>\n",
       "      <td>-20.583056</td>\n",
       "      <td>-21.253432</td>\n",
       "      <td>-21.003830</td>\n",
       "      <td>-19.247303</td>\n",
       "      <td>-20.695284</td>\n",
       "      <td>-21.758510</td>\n",
       "      <td>-20.265141</td>\n",
       "      <td>-21.447015</td>\n",
       "      <td>-18.852893</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.730102</td>\n",
       "      <td>-24.618149</td>\n",
       "      <td>-24.645153</td>\n",
       "      <td>-24.560171</td>\n",
       "      <td>-24.575732</td>\n",
       "      <td>-24.695749</td>\n",
       "      <td>-24.655557</td>\n",
       "      <td>-24.585224</td>\n",
       "      <td>-24.689903</td>\n",
       "      <td>-24.599509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  13451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5  \\\n",
       "1103 -13.619102 -12.834158 -13.334953 -12.993541 -12.190391 -12.566942   \n",
       "2338 -22.217343 -20.583056 -21.253432 -21.003830 -19.247303 -20.695284   \n",
       "\n",
       "              6          7          8          9    ...          13441  \\\n",
       "1103 -13.327277 -12.202205 -13.228280 -11.900171    ...     -14.803132   \n",
       "2338 -21.758510 -20.265141 -21.447015 -18.852893    ...     -24.730102   \n",
       "\n",
       "          13442      13443      13444      13445      13446      13447  \\\n",
       "1103 -14.733360 -14.764064 -14.752999 -14.749906 -14.779751 -14.758421   \n",
       "2338 -24.618149 -24.645153 -24.560171 -24.575732 -24.695749 -24.655557   \n",
       "\n",
       "          13448      13449      13450  \n",
       "1103 -14.746018 -14.811518 -14.795294  \n",
       "2338 -24.585224 -24.689903 -24.599509  \n",
       "\n",
       "[2 rows x 13451 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_frame.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024999999999999998"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_recall(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_9</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Engin 45</td>\n",
       "      <td>Properties of Materials</td>\n",
       "      <td>Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering</td>\n",
       "      <td>Engineering 45 ENGIN45</td>\n",
       "      <td>drug</td>\n",
       "      <td>metals</td>\n",
       "      <td>atomic</td>\n",
       "      <td>particles</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>bulk</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>kinetic</td>\n",
       "      <td>beam</td>\n",
       "      <td>polymers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>History 198bc</td>\n",
       "      <td>Berkeley Connect for Upper Division Students</td>\n",
       "      <td>Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.</td>\n",
       "      <td>History 198BC HISTORY198BC</td>\n",
       "      <td>http</td>\n",
       "      <td>violence</td>\n",
       "      <td>words</td>\n",
       "      <td>conquest</td>\n",
       "      <td>twentieth</td>\n",
       "      <td>called</td>\n",
       "      <td>powers</td>\n",
       "      <td>edu</td>\n",
       "      <td>peace</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_name                                  course_title  \\\n",
       "1103  Engin 45       Properties of Materials                        \n",
       "2338  History 198bc  Berkeley Connect for Upper Division Students   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             course_description  \\\n",
       "1103  Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering                                                                                                                                                                                                                                             \n",
       "2338  Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.   \n",
       "\n",
       "        course_alternative_names predicted_word_10 predicted_word_9  \\\n",
       "1103  Engineering 45 ENGIN45      drug              metals            \n",
       "2338  History 198BC HISTORY198BC  http              violence          \n",
       "\n",
       "     predicted_word_8 predicted_word_7 predicted_word_6 predicted_word_5  \\\n",
       "1103  atomic           particles        astronomy        bulk              \n",
       "2338  words            conquest         twentieth        called            \n",
       "\n",
       "     predicted_word_4 predicted_word_3 predicted_word_2 predicted_word_1  \n",
       "1103  sufficient       kinetic          beam             polymers         \n",
       "2338  powers           edu              peace            russia           "
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_keywords</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>course_keywords_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Engin 45</td>\n",
       "      <td>Properties of Materials</td>\n",
       "      <td>Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering</td>\n",
       "      <td>astronomy, beam, bulk, gases, ion, matlab, particles, polymers, solar, sponsoring</td>\n",
       "      <td>Engineering 45 ENGIN45</td>\n",
       "      <td>{beam, gases, polymers, matlab, particles, bulk, astronomy, ion, solar, sponsoring}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>History 198bc</td>\n",
       "      <td>Berkeley Connect for Upper Division Students</td>\n",
       "      <td>Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.</td>\n",
       "      <td>boundaries, called, conquest, edu, http, looking, peace, powers, russia, west</td>\n",
       "      <td>History 198BC HISTORY198BC</td>\n",
       "      <td>{looking, conquest, powers, peace, edu, russia, west, boundaries, called, http}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_name                                  course_title  \\\n",
       "1103  Engin 45       Properties of Materials                        \n",
       "2338  History 198bc  Berkeley Connect for Upper Division Students   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             course_description  \\\n",
       "1103  Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering                                                                                                                                                                                                                                             \n",
       "2338  Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.   \n",
       "\n",
       "                                                                        course_keywords  \\\n",
       "1103  astronomy, beam, bulk, gases, ion, matlab, particles, polymers, solar, sponsoring   \n",
       "2338  boundaries, called, conquest, edu, http, looking, peace, powers, russia, west       \n",
       "\n",
       "        course_alternative_names  \\\n",
       "1103  Engineering 45 ENGIN45       \n",
       "2338  History 198BC HISTORY198BC   \n",
       "\n",
       "                                                                      course_keywords_set  \n",
       "1103  {beam, gases, polymers, matlab, particles, bulk, astronomy, ion, solar, sponsoring}  \n",
       "2338  {looking, conquest, powers, peace, edu, russia, west, boundaries, called, http}      "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keywords = Y_test[Y_test.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "Y_test['course_keywords'] = predicted_keywords.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "Y_test = Y_test[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "Y_test['course_keywords'] = Y_test['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "Y_test['course_keywords_set'] = Y_test['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "Y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ocular', 322),\n",
       " ('professionally', 321),\n",
       " ('debt', 319),\n",
       " ('securities', 319),\n",
       " ('managerial', 319),\n",
       " ('formulating', 313),\n",
       " ('alliances', 308),\n",
       " ('optometric', 269),\n",
       " ('neurons', 258),\n",
       " ('managed', 212)]"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keyword_list = predicted_keywords.values.tolist()\n",
    "predicted_keyword_list = list(chain.from_iterable(predicted_keyword_list))\n",
    "\n",
    "keyword_counter = Counter(predicted_keyword_list)\n",
    "\n",
    "keyword_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_keywords</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>course_keywords_set</th>\n",
       "      <th>descrip_title</th>\n",
       "      <th>description_title_set</th>\n",
       "      <th>shared_words</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Engin 45</td>\n",
       "      <td>Properties of Materials</td>\n",
       "      <td>Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering</td>\n",
       "      <td>astronomy, beam, bulk, gases, ion, matlab, particles, polymers, solar, sponsoring</td>\n",
       "      <td>Engineering 45 ENGIN45</td>\n",
       "      <td>{beam, gases, polymers, matlab, particles, bulk, astronomy, ion, solar, sponsoring}</td>\n",
       "      <td>Properties of Materials Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering</td>\n",
       "      <td>{and, physics, relation, properties, devoted, concrete, the, semiconducting, emphasis, special, electrical, ceramics, sponsoring, science, department, basic, polymers, microstructure, materials, metals, between, application, chemistry, of, mechanical, principles, engineering, to}</td>\n",
       "      <td>{polymers, sponsoring}</td>\n",
       "      <td>0.01105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>History 198bc</td>\n",
       "      <td>Berkeley Connect for Upper Division Students</td>\n",
       "      <td>Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.</td>\n",
       "      <td>boundaries, called, conquest, edu, http, looking, peace, powers, russia, west</td>\n",
       "      <td>History 198BC HISTORY198BC</td>\n",
       "      <td>{looking, conquest, powers, peace, edu, russia, west, boundaries, called, http}</td>\n",
       "      <td>Berkeley Connect for Upper Division Students Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.</td>\n",
       "      <td>{build, and, field, berkeley, for, with, faculty, a, academic, featuring, the, offered, discussions, meet, in, required, order, through, helps, facilitated, resources, panel, graduate, that, curriculum, students, over, various, course, alumni, their, division, upper, department, attend, facultydirected, on, oneonone, lectures, departments, advising, be, mentoring, program, majors, student, are, trips, of, participate, campus, semester, not, is, intellectual, smallgroup, mentor, go, to, community, connect, by, declared, regular, enrolled, following}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>Econ 224</td>\n",
       "      <td>Economics of Institutions</td>\n",
       "      <td>This course develops the proposition that institutions have pervasive ramifications for understanding economic organization. A comparative institutional approach is employed whereby the transaction is made the basic unit of analysis and alternative modes of organization are assessed with respect to their comparative contracting properties.</td>\n",
       "      <td>allocation, capture, critiques, demonstrating, economists, goods, homework, leave, modeled, survival</td>\n",
       "      <td>Economics 224 ECON224</td>\n",
       "      <td>{allocation, critiques, modeled, homework, demonstrating, leave, goods, capture, survival, economists}</td>\n",
       "      <td>Economics of Institutions This course develops the proposition that institutions have pervasive ramifications for understanding economic organization. A comparative institutional approach is employed whereby the transaction is made the basic unit of analysis and alternative modes of organization are assessed with respect to their comparative contracting properties.</td>\n",
       "      <td>{and, respect, for, with, proposition, properties, a, institutional, the, comparative, organization, contracting, modes, that, whereby, alternative, course, their, understanding, economics, basic, ramifications, develops, this, assessed, transaction, are, of, unit, have, analysis, is, approach, pervasive, employed, economic, made, institutions, to}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>Ewmba 236d</td>\n",
       "      <td>Portfolio Management</td>\n",
       "      <td>This course explores the broad range of portfolio management in practice. The class will examine the assets, strategies, characteristics, operations, and concerns unique to each type of portfolio. Practitioners will present descriptions of their businesses as well as methods and strategies that they employ.</td>\n",
       "      <td>alliances, debt, formulating, managerial, neurons, ocular, optometric, professionally, securities, statements</td>\n",
       "      <td>Evening &amp; Weekend MBA 236D EWMBA236D</td>\n",
       "      <td>{formulating, ocular, neurons, optometric, securities, statements, managerial, alliances, debt, professionally}</td>\n",
       "      <td>Portfolio Management This course explores the broad range of portfolio management in practice. The class will examine the assets, strategies, characteristics, operations, and concerns unique to each type of portfolio. Practitioners will present descriptions of their businesses as well as methods and strategies that they employ.</td>\n",
       "      <td>{present, and, each, well, the, in, type, management, concerns, will, practice, that, descriptions, course, businesses, their, employ, broad, explores, portfolio, methods, assets, class, practitioners, this, of, strategies, they, examine, unique, operations, characteristics, range, as, to}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>History n131b</td>\n",
       "      <td>Social History of the United States: 1914-Present</td>\n",
       "      <td>The nature and development of social and economic institutions; class, family, and racial relationships; sex roles; and cultural norms in the United States.</td>\n",
       "      <td>achievement, did, lived, points, powers, recognition, rural, twentieth, violence, west</td>\n",
       "      <td>History N131B HISTORYN131B</td>\n",
       "      <td>{lived, points, powers, achievement, violence, rural, west, recognition, twentieth, did}</td>\n",
       "      <td>Social History of the United States: 1914-Present The nature and development of social and economic institutions; class, family, and racial relationships; sex roles; and cultural norms in the United States.</td>\n",
       "      <td>{and, family, institutions, roles, norms, nature, states, cultural, class, history, the, racial, economic, 1914present, united, sex, development, of, social, relationships, in}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_name                                       course_title  \\\n",
       "1103  Engin 45       Properties of Materials                             \n",
       "2338  History 198bc  Berkeley Connect for Upper Division Students        \n",
       "7196  Econ 224       Economics of Institutions                           \n",
       "4013  Ewmba 236d     Portfolio Management                                \n",
       "2684  History n131b  Social History of the United States: 1914-Present   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             course_description  \\\n",
       "1103  Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering                                                                                                                                                                                                                                             \n",
       "2338  Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.   \n",
       "7196  This course develops the proposition that institutions have pervasive ramifications for understanding economic organization. A comparative institutional approach is employed whereby the transaction is made the basic unit of analysis and alternative modes of organization are assessed with respect to their comparative contracting properties.                                                                                                                                                                                                                                                       \n",
       "4013  This course explores the broad range of portfolio management in practice. The class will examine the assets, strategies, characteristics, operations, and concerns unique to each type of portfolio. Practitioners will present descriptions of their businesses as well as methods and strategies that they employ.                                                                                                                                                                                                                                                                                        \n",
       "2684  The nature and development of social and economic institutions; class, family, and racial relationships; sex roles; and cultural norms in the United States.                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "                                                                                                    course_keywords  \\\n",
       "1103  astronomy, beam, bulk, gases, ion, matlab, particles, polymers, solar, sponsoring                               \n",
       "2338  boundaries, called, conquest, edu, http, looking, peace, powers, russia, west                                   \n",
       "7196  allocation, capture, critiques, demonstrating, economists, goods, homework, leave, modeled, survival            \n",
       "4013  alliances, debt, formulating, managerial, neurons, ocular, optometric, professionally, securities, statements   \n",
       "2684  achievement, did, lived, points, powers, recognition, rural, twentieth, violence, west                          \n",
       "\n",
       "                  course_alternative_names  \\\n",
       "1103  Engineering 45 ENGIN45                 \n",
       "2338  History 198BC HISTORY198BC             \n",
       "7196  Economics 224 ECON224                  \n",
       "4013  Evening & Weekend MBA 236D EWMBA236D   \n",
       "2684  History N131B HISTORYN131B             \n",
       "\n",
       "                                                                                                  course_keywords_set  \\\n",
       "1103  {beam, gases, polymers, matlab, particles, bulk, astronomy, ion, solar, sponsoring}                               \n",
       "2338  {looking, conquest, powers, peace, edu, russia, west, boundaries, called, http}                                   \n",
       "7196  {allocation, critiques, modeled, homework, demonstrating, leave, goods, capture, survival, economists}            \n",
       "4013  {formulating, ocular, neurons, optometric, securities, statements, managerial, alliances, debt, professionally}   \n",
       "2684  {lived, points, powers, achievement, violence, rural, west, recognition, twentieth, did}                          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               descrip_title  \\\n",
       "1103  Properties of Materials Application of basic principles of physics and chemistry to the engineering properties of materials. Special emphasis devoted to relation between microstructure and the mechanical properties of metals, concrete, polymers, and ceramics, and the electrical properties of semiconducting materials. Sponsoring Department: Materials Science and Engineering                                                                                                                                                                                                                                                                  \n",
       "2338  Berkeley Connect for Upper Division Students Berkeley Connect is a mentoring program, offered through various academic departments, that helps students build intellectual community. Over the course of a semester, enrolled students participate in regular small-group discussions facilitated by a graduate student mentor (following a faculty-directed curriculum), meet with their graduate student mentor for one-on-one academic advising, attend lectures and panel discussions featuring department faculty and alumni, and go on field trips to campus resources. Students are not required to be declared majors in order to participate.   \n",
       "7196  Economics of Institutions This course develops the proposition that institutions have pervasive ramifications for understanding economic organization. A comparative institutional approach is employed whereby the transaction is made the basic unit of analysis and alternative modes of organization are assessed with respect to their comparative contracting properties.                                                                                                                                                                                                                                                                          \n",
       "4013  Portfolio Management This course explores the broad range of portfolio management in practice. The class will examine the assets, strategies, characteristics, operations, and concerns unique to each type of portfolio. Practitioners will present descriptions of their businesses as well as methods and strategies that they employ.                                                                                                                                                                                                                                                                                                                \n",
       "2684  Social History of the United States: 1914-Present The nature and development of social and economic institutions; class, family, and racial relationships; sex roles; and cultural norms in the United States.                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           description_title_set  \\\n",
       "1103  {and, physics, relation, properties, devoted, concrete, the, semiconducting, emphasis, special, electrical, ceramics, sponsoring, science, department, basic, polymers, microstructure, materials, metals, between, application, chemistry, of, mechanical, principles, engineering, to}                                                                                                                                                                                                                                                                                     \n",
       "2338  {build, and, field, berkeley, for, with, faculty, a, academic, featuring, the, offered, discussions, meet, in, required, order, through, helps, facilitated, resources, panel, graduate, that, curriculum, students, over, various, course, alumni, their, division, upper, department, attend, facultydirected, on, oneonone, lectures, departments, advising, be, mentoring, program, majors, student, are, trips, of, participate, campus, semester, not, is, intellectual, smallgroup, mentor, go, to, community, connect, by, declared, regular, enrolled, following}   \n",
       "7196  {and, respect, for, with, proposition, properties, a, institutional, the, comparative, organization, contracting, modes, that, whereby, alternative, course, their, understanding, economics, basic, ramifications, develops, this, assessed, transaction, are, of, unit, have, analysis, is, approach, pervasive, employed, economic, made, institutions, to}                                                                                                                                                                                                               \n",
       "4013  {present, and, each, well, the, in, type, management, concerns, will, practice, that, descriptions, course, businesses, their, employ, broad, explores, portfolio, methods, assets, class, practitioners, this, of, strategies, they, examine, unique, operations, characteristics, range, as, to}                                                                                                                                                                                                                                                                           \n",
       "2684  {and, family, institutions, roles, norms, nature, states, cultural, class, history, the, racial, economic, 1914present, united, sex, development, of, social, relationships, in}                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "                shared_words   recall  \n",
       "1103  {polymers, sponsoring}  0.01105  \n",
       "2338  {}                      0.00000  \n",
       "7196  {}                      0.00000  \n",
       "4013  {}                      0.00000  \n",
       "2684  {}                      0.00000  "
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test['descrip_title'] = Y_test['course_title'] + ' ' + Y_test['course_description']\n",
    "Y_test['description_title_set'] = Y_test.apply(clean_descrip_title, axis = 1)\n",
    "Y_test['shared_words'] = Y_test.apply(recall_keywords, axis = 1)\n",
    "Y_test['recall'] = Y_test['shared_words'].apply(lambda words: len(list(words)) / max_descript_len)\n",
    "Y_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what's the threshold for good recall?  \n",
    "- varies wrt the vocabulary, can have higher recall but poor keywords, the evaluation metric should be in tandem with some **\"relevancy\"** metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013662429441973977"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_test.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test[Y_test['recall'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end 1\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## experiment attempt 2: with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running cross validation...\n",
      "[INFO] ****** Fold 1 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11855\n",
      "[INFO] Number of bigrams: 1185\n",
      "[INFO] Number of trigrams: 1185\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 451us/step - loss: 19737.1038 - acc: 0.0042\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 288us/step - loss: 18545.6686 - acc: 0.0395\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 284us/step - loss: 17855.6044 - acc: 0.0753\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 293us/step - loss: 17181.1920 - acc: 0.1032\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 294us/step - loss: 16580.0680 - acc: 0.1147\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.006360.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 1 precision: 0.029472.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 1 cosine similarity: 0.451431.\n",
      "[INFO] ****** Fold 2 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11723\n",
      "[INFO] Number of bigrams: 1172\n",
      "[INFO] Number of trigrams: 1172\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 409us/step - loss: 19393.4586 - acc: 0.0056\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 285us/step - loss: 18219.7107 - acc: 0.0386\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 282us/step - loss: 17569.6273 - acc: 0.0668\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 292us/step - loss: 16929.4509 - acc: 0.0930\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 293us/step - loss: 16326.8215 - acc: 0.1079\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.006439.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 2 precision: 0.028610.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 2 cosine similarity: 0.434308.\n",
      "[INFO] ****** Fold 3 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11593\n",
      "[INFO] Number of bigrams: 1159\n",
      "[INFO] Number of trigrams: 1159\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 419us/step - loss: 18898.3640 - acc: 0.0080\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 294us/step - loss: 17766.4856 - acc: 0.0429\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 295us/step - loss: 17115.9945 - acc: 0.0717\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 288us/step - loss: 16483.0691 - acc: 0.0930\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 288us/step - loss: 15911.3307 - acc: 0.1115\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.005604.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 3 precision: 0.025898.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 3 cosine similarity: 0.362058.\n",
      "[INFO] ****** Fold 4 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11491\n",
      "[INFO] Number of bigrams: 1149\n",
      "[INFO] Number of trigrams: 1149\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 401us/step - loss: 18867.7999 - acc: 0.0054\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 276us/step - loss: 17742.7044 - acc: 0.0417\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 282us/step - loss: 17099.6785 - acc: 0.0761\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 279us/step - loss: 16484.2068 - acc: 0.1034\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 283us/step - loss: 15878.4627 - acc: 0.1154\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.005412.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 4 precision: 0.023051.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 4 cosine similarity: 0.358837.\n",
      "[INFO] ****** Fold 5 ******\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11594\n",
      "[INFO] Number of bigrams: 1159\n",
      "[INFO] Number of trigrams: 1159\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 2s 411us/step - loss: 19115.3253 - acc: 0.0044\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 291us/step - loss: 17939.3938 - acc: 0.0422\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 294us/step - loss: 17239.2755 - acc: 0.0798\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 298us/step - loss: 16626.6431 - acc: 0.0996\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 292us/step - loss: 16001.7478 - acc: 0.1222\n",
      "[INFO] Predicting on validation set for recall...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.004004.\n",
      "[INFO] Predicting on validation set for precision...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Predicting top k inferred keywords for each course...\n",
      "[INFO] Calculating Precision...\n",
      "[INFO] Fold 5 precision: 0.014915.\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 5 cosine similarity: 0.252136.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.029471544715447155,\n",
       " 0.028610169491525426,\n",
       " 0.025898305084745766,\n",
       " 0.02305084745762712,\n",
       " 0.014915254237288136]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, random_state = 42)\n",
    "\n",
    "print('[INFO] Running cross validation...')\n",
    "\n",
    "recall_validation_scores = []\n",
    "precision_validation_scores = []\n",
    "distribution_validation_scores = []\n",
    "fold_num = 1\n",
    "# X = vectors, Y = descriptions\n",
    "for train_idx, valid_idx in kf.split(filtered_vec_df):\n",
    "    print('[INFO] ****** Fold %d ******' % (fold_num))\n",
    "    \n",
    "    split_X_train, split_X_valid = filtered_vec_df.iloc[train_idx], filtered_vec_df.iloc[valid_idx]\n",
    "    split_Y_train, split_Y_valid = filtered_descript_df.iloc[train_idx], filtered_descript_df.iloc[valid_idx]\n",
    "    \n",
    "    vocab = get_vocab(split_Y_train, textcolumn) \n",
    "    vocab_frame = pd.DataFrame(vocab)\n",
    "    vocabsize = len(vocab)\n",
    "\n",
    "    # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "    split_Y_train_BOW = to_bag_of_words(split_Y_train, textcolumn, vocab)\n",
    "    split_Y_train_BOW = split_Y_train_BOW.toarray()\n",
    "    \n",
    "    (weights_frame, biases) = logistic_regression(split_X_train.iloc[:,1:], split_Y_train_BOW)\n",
    "    \n",
    "    print('[INFO] Predicting on validation set for recall...')\n",
    "    df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, max_descript_len)\n",
    "    \n",
    "    fold_i_average_recall = calculate_metric(df_with_keywords, 'r')\n",
    "    recall_validation_scores.append(fold_i_average_recall)\n",
    "    print('[INFO] Fold %d recall: %f.' % (fold_num, fold_i_average_recall))\n",
    "    \n",
    "    print('[INFO] Predicting on validation set for precision...')\n",
    "    df_with_keywords = predict(split_X_valid, split_Y_valid, weights_frame, biases, num_top_words)\n",
    "    fold_i_average_precision = calculate_metric(df_with_keywords, 'p')\n",
    "    precision_validation_scores.append(fold_i_average_precision)\n",
    "    print('[INFO] Fold %d precision: %f.' % (fold_num, fold_i_average_precision))\n",
    "    \n",
    "    fold_i_distribution_diff = calculate_metric(df_with_keywords, 'c')\n",
    "    distribution_validation_scores.append(fold_i_distribution_diff)\n",
    "    print('[INFO] Fold %d cosine similarity: %f.' % (fold_num, fold_i_distribution_diff))\n",
    "    \n",
    "    fold_num += 1\n",
    "       \n",
    "# print(f\"Cosine Similarity: {distribution_cosine_sim_scores} \\nRecall: {recall_validation_scores}\")\n",
    "precision_validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4598174502142759,\n",
       " 0.4373838326840337,\n",
       " 0.37181566231750807,\n",
       " 0.348645018646041,\n",
       " 0.2401963885567444]"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0062023686535207885,\n",
       " 0.006277741361550707,\n",
       " 0.0056784343103286825,\n",
       " 0.005341324094016293,\n",
       " 0.003951680868995224]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.0028,\n",
       " 5,\n",
       " 10,\n",
       " 0.005563703654641191,\n",
       " 0.02438922419732672,\n",
       " 0.37175375639262154]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_i = np.mean(recall_validation_scores)\n",
    "precision_i = np.mean(precision_validation_scores)\n",
    "distribution_diff_i = np.mean(distribution_validation_scores)\n",
    "\n",
    "hyperparams_cols = ['tf-bias', 'max_df', 'num_epochs', 'num_top_words', 'recall', 'precision', 'distribution_diff']\n",
    "hyperparams_df = pd.DataFrame(columns=hyperparams_cols)\n",
    "model_i_info_list = [tf_bias, max_df, num_epochs, num_top_words, recall_i, precision_i, distribution_diff_i]\n",
    "\n",
    "model_i_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf-bias</th>\n",
       "      <th>max_df</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>num_top_words</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>distribution_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.024389</td>\n",
       "      <td>0.371754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tf-bias  max_df num_epochs num_top_words    recall  precision  \\\n",
       "0      0.5  0.0028          5            10  0.005564   0.024389   \n",
       "\n",
       "   distribution_diff  \n",
       "0           0.371754  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_i_info = pd.DataFrame([model_i_info_list], columns=hyperparams_cols)\n",
    "hyperparams_df = hyperparams_df.append(model_i_info, sort= False)\n",
    "\n",
    "hyperparams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running cross validation...\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11855\n",
      "[INFO] Number of bigrams: 1185\n",
      "[INFO] Number of trigrams: 1185\n",
      "[INFO] Fold: 1.\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5900/5900 [==============================] - 3s 581us/step - loss: 19747.8247 - acc: 0.0037\n",
      "Epoch 2/5\n",
      "5900/5900 [==============================] - 2s 329us/step - loss: 18561.7911 - acc: 0.0356\n",
      "Epoch 3/5\n",
      "5900/5900 [==============================] - 2s 358us/step - loss: 17840.9189 - acc: 0.0712\n",
      "Epoch 4/5\n",
      "5900/5900 [==============================] - 2s 329us/step - loss: 17170.6873 - acc: 0.1014\n",
      "Epoch 5/5\n",
      "5900/5900 [==============================] - 2s 330us/step - loss: 16555.9515 - acc: 0.1225\n",
      "[INFO] Predicting on validation set...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 1 cosine similarity: 0.450594.\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 1 recall: 0.296070.\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11723\n",
      "[INFO] Number of bigrams: 1172\n",
      "[INFO] Number of trigrams: 1172\n",
      "[INFO] Fold: 2.\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 581us/step - loss: 19395.3846 - acc: 0.0066\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 332us/step - loss: 18237.2616 - acc: 0.0390\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 358us/step - loss: 17556.6668 - acc: 0.0658\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 343us/step - loss: 16903.2593 - acc: 0.0941\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 339us/step - loss: 16317.8449 - acc: 0.1102\n",
      "[INFO] Predicting on validation set...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 2 cosine similarity: 0.433048.\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 2 recall: 0.275932.\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11593\n",
      "[INFO] Number of bigrams: 1159\n",
      "[INFO] Number of trigrams: 1159\n",
      "[INFO] Fold: 3.\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 589us/step - loss: 18902.6379 - acc: 0.0046\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 346us/step - loss: 17769.5376 - acc: 0.0359\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 331us/step - loss: 17106.8412 - acc: 0.0754\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 350us/step - loss: 16480.6876 - acc: 0.0903\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 348us/step - loss: 15916.9190 - acc: 0.1124\n",
      "[INFO] Predicting on validation set...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 3 cosine similarity: 0.370711.\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 3 recall: 0.271186.\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11491\n",
      "[INFO] Number of bigrams: 1149\n",
      "[INFO] Number of trigrams: 1149\n",
      "[INFO] Fold: 4.\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 568us/step - loss: 18881.9300 - acc: 0.0068\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 333us/step - loss: 17732.6913 - acc: 0.0419\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 347us/step - loss: 17086.7533 - acc: 0.0757\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 324us/step - loss: 16473.3715 - acc: 0.0976\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 324us/step - loss: 15909.3776 - acc: 0.1135\n",
      "[INFO] Predicting on validation set...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 4 cosine similarity: 0.350433.\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 4 recall: 0.238644.\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Number of unigrams: 11594\n",
      "[INFO] Number of bigrams: 1159\n",
      "[INFO] Number of trigrams: 1159\n",
      "[INFO] Fold: 5.\n",
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "5901/5901 [==============================] - 3s 580us/step - loss: 19110.7088 - acc: 0.0056\n",
      "Epoch 2/5\n",
      "5901/5901 [==============================] - 2s 333us/step - loss: 17920.5592 - acc: 0.0397\n",
      "Epoch 3/5\n",
      "5901/5901 [==============================] - 2s 337us/step - loss: 17271.4257 - acc: 0.0735\n",
      "Epoch 4/5\n",
      "5901/5901 [==============================] - 2s 338us/step - loss: 16612.2403 - acc: 0.1035\n",
      "Epoch 5/5\n",
      "5901/5901 [==============================] - 2s 345us/step - loss: 15986.9455 - acc: 0.1147\n",
      "[INFO] Predicting on validation set...\n",
      "[INFO] Sorting classification results...\n",
      "[INFO] Calculating Cosine Similarity Between Keyword Distributions...\n",
      "[INFO] Fold 5 cosine similarity: 0.251227.\n",
      "[INFO] Calculating Recall...\n",
      "[INFO] Fold 5 recall: 0.149831.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45059365190454925,\n",
       " 0.43304753146092856,\n",
       " 0.37071133532055667,\n",
       " 0.3504333662832029,\n",
       " 0.25122744033738553]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[INFO] Running cross validation...')\n",
    "\n",
    "recall_validation_scores = []\n",
    "distribution_cosine_sim_scores = []\n",
    "fold_num = 1\n",
    "# X = vectors, Y = descriptions\n",
    "for train_idx, valid_idx in kf.split(filtered_vec_df):\n",
    "    print('[INFO] Fold %d' % (fold_num))\n",
    "    \n",
    "    split_X_train, split_X_valid = filtered_vec_df.iloc[train_idx], filtered_vec_df.iloc[valid_idx]\n",
    "    split_Y_train, split_Y_valid = filtered_descript_df.iloc[train_idx], filtered_descript_df.iloc[valid_idx]\n",
    "    \n",
    "    vocab = get_vocab(split_Y_train, textcolumn) \n",
    "    vocab_frame = pd.DataFrame(vocab)\n",
    "    vocabsize = len(vocab)\n",
    "\n",
    "    # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "    Y_train_BOW = to_bag_of_words(split_Y_train, textcolumn, vocab)\n",
    "    Y_train_BOW = Y_train_BOW.toarray()\n",
    "    \n",
    "    (weights_frame, biases) = logistic_regression(split_X_train.iloc[:,1:], Y_train_BOW)\n",
    "    \n",
    "    print('[INFO] Predicting on validation set...')\n",
    "    softmax_frame = split_X_valid.iloc[:,1:].dot(weights_frame.values) + biases\n",
    "    \n",
    "    print('[INFO] Sorting classification results...')\n",
    "    \n",
    "    sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "    predicted_keyword_list = []\n",
    "    for i in range(num_top_words):\n",
    "        new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "        predicted_keyword_list.extend(new_col.values)\n",
    "        split_Y_valid['predicted_word_' + str(num_top_words-i)] = new_col.values\n",
    "        \n",
    "        \n",
    "    print('[INFO] Calculating Cosine Similarity Between Keyword Distributions...')\n",
    "    keyword_counter = Counter(predicted_keyword_list)\n",
    "    num_possible_keywords = split_Y_valid.shape[0] * num_top_words\n",
    "    num_predicted_keywords = len(keyword_counter.keys())\n",
    "    \n",
    "    assert sum(keyword_counter.values()) == split_Y_valid.shape[0] * num_top_words,\\\n",
    "    'Total number of predicted keywords should equal number of courses * number of predicted keywords per course.'\n",
    "    \n",
    "    unif_keyword_vector = np.repeat(num_possible_keywords / num_predicted_keywords, num_predicted_keywords)\n",
    "    predicted_keyword_vector = np.array(list(keyword_counter.values()))\n",
    "\n",
    "    assert unif_keyword_vector.shape == predicted_keyword_vector.shape,\\\n",
    "    'Uniform keyword frequency vector should have same dimension as predicted keywords frequency vector.'\n",
    "    \n",
    "    fold_i_cos_sim = cosine_similarity(predicted_keyword_vector, unif_keyword_vector)\n",
    "    \n",
    "    distribution_cosine_sim_scores.append(fold_i_cos_sim)\n",
    "    \n",
    "    print('[INFO] Fold %d cosine similarity: %f.' % (fold_num, fold_i_cos_sim))\n",
    "    \n",
    "    print('[INFO] Calculating Recall...')\n",
    "    predicted_keywords = split_Y_valid[split_Y_valid.columns.difference(['course_name', 'course_title', 'course_description', 'tf_bias', 'course_alternative_names'])]\n",
    "    split_Y_valid['course_keywords'] = predicted_keywords.iloc[:,:].apply(lambda x: ', '.join(x), axis=1)\n",
    "    split_Y_valid = split_Y_valid[['course_name', 'course_title', 'course_description', 'course_keywords', 'course_alternative_names']]\n",
    "    split_Y_valid['course_keywords'] = split_Y_valid['course_keywords'].apply(lambda keywords: ', '.join(sorted(set([word.strip() for word in keywords.split(',')]))))\n",
    "    split_Y_valid['keywords_set'] = split_Y_valid['course_keywords'].apply(lambda keywords: (set([word.strip() for word in keywords.split(',')])))\n",
    "    split_Y_valid['descrip_title'] = split_Y_valid['course_title'] + ' ' + split_Y_valid['course_description']\n",
    "    split_Y_valid['description_title_set'] = split_Y_valid.apply(clean_descrip_title, axis = 1)\n",
    "    split_Y_valid['recall_keywords'] = split_Y_valid.apply(recall_keywords, axis = 1)\n",
    "    split_Y_valid['recall'] = split_Y_valid['recall_keywords'].apply(lambda keyword_set: len(list(keyword_set)))\n",
    "    fold_i_average_recall = np.mean(split_Y_valid.recall)\n",
    "    \n",
    "    print('[INFO] Fold %d recall: %f.' % (fold_num, fold_i_average_recall))\n",
    "    fold_num += 1\n",
    "    \n",
    "    recall_validation_scores.append(fold_i_average_recall)\n",
    "       \n",
    "# print(f\"Cosine Similarity: {distribution_cosine_sim_scores} \\nRecall: {recall_validation_scores}\")\n",
    "distribution_cosine_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3712026650613246 0.24633273620871804\n"
     ]
    }
   ],
   "source": [
    "recall_i = np.mean(recall_validation_scores)\n",
    "cos_sim_i = np.mean(distribution_cosine_sim_scores)\n",
    "print(cos_sim_i, recall_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf-bias</th>\n",
       "      <th>max_df</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>num_top_words</th>\n",
       "      <th>recall</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.246333</td>\n",
       "      <td>0.371203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tf-bias  max_df num_epochs num_top_words    recall  cosine_similarity\n",
       "0  0.5      0.0028  5          10            0.246333  0.371203         "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams_cols = ['tf-bias', 'max_df', 'num_epochs', 'num_top_words', 'recall', 'cosine_similarity']\n",
    "hyperparams_df = pd.DataFrame(columns=hyperparams_cols)\n",
    "model_i_info_list = [tf_bias, max_df, num_epochs, num_top_words, recall_i, cos_sim_i]\n",
    "model_i_info = pd.DataFrame([model_i_info_list], columns=hyperparams_cols)\n",
    "hyperparams_df.append(model_i_info, sort= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end 2\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start time: 1542055699.329339\n",
      "[INFO] Reading /home/matthew/Models-AskOski/ICS/course_vecs_temp.tsv...\n",
      "[INFO] sep: \t\n",
      "[INFO] Reading /home/matthew/Models-AskOski/ICS/course_info_temp.tsv...\n",
      "[INFO] sep: \t\n",
      "[INFO] Getting data took 0.7570998668670654\n",
      "[INFO] Getting vocab...\n",
      "[INFO] Taking at most 2000 (most frequent) unigrams\n",
      "[UNIGRAMS] number unigrams: 12971\n",
      "[INFO] Getting vocab and bow took 2.9103753566741943\n"
     ]
    }
   ],
   "source": [
    "timebf = time.time()\n",
    "print('[INFO] Start time: ' + str(timebf))\n",
    "# get data\n",
    "time_get_data_bf = time.time()\n",
    "vec_frame = read_big_csv(vectorfile) # Vector space representation of each user, all numeric\n",
    "raw_frame = read_big_csv(rawfile) # Course information\n",
    "\n",
    "len_vec_frame = len(vec_frame.index)\n",
    "len_raw_frame = len(raw_frame.index)\n",
    "if (len_vec_frame != len_raw_frame):\n",
    "    print('[DEBUG] vector file and raw file entries do not line up: ' + str(len_vec_frame) + ' ' + str(len_raw_frame))\n",
    "    sys.exit()\n",
    "\n",
    "nonempty_indices = np.where(raw_frame[textcolumn].notnull() == True)[0]\n",
    "filtered_vec_df = vec_frame.iloc[nonempty_indices,:]\n",
    "filtered_descript_df = raw_frame.iloc[nonempty_indices,:]\n",
    "\n",
    "time_get_data_af = time.time()\n",
    "print('[INFO] Getting data took ' + str(time_get_data_af - time_get_data_bf))\n",
    "\n",
    "if (textcolumn != ''):\n",
    "    ### Using the textcolumn, obtain a bow encoding and train the vectorspace coeffs to predict the bow of a point. ###\n",
    "    # Get the vocab\n",
    "    time_get_vocab_and_bow_bf = time.time()\n",
    "    vocab = get_vocab(raw_frame, textcolumn)\n",
    "    vocab_frame = pd.DataFrame(vocab)\n",
    "    \n",
    "#     vocab_frame.to_csv(vocab_dir + '/complete-vocab.tsv', sep = '\\t', index = False)\n",
    "    \n",
    "    vocabsize = len(vocab)\n",
    "    \n",
    "#     bow_spmatrix = to_bag_of_words(raw_frame, textcolumn, vocab)\n",
    "#     bow_ndarray = bow_spmatrix.toarray()\n",
    "#     bow_frame = pd.DataFrame(bow_ndarray)\n",
    "\n",
    "    # Convert the textcolumn of the raw dataframe into bag of words representation\n",
    "    filtered_bow_spmatrix = to_bag_of_words(filtered_descript_df, textcolumn, vocab)\n",
    "    filtered_bow_ndarray = filtered_bow_spmatrix.toarray()\n",
    "    \n",
    "    # filtered_bow_frame = pd.DataFrame(filtered_bow_ndarray)\n",
    "    \n",
    "    time_get_vocab_and_bow_af = time.time()\n",
    "    print('[INFO] Getting vocab and bow took ' + str(time_get_vocab_and_bow_af - time_get_vocab_and_bow_bf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aacm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbasid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0        aacm\n",
       "1        aapi\n",
       "2         aas\n",
       "3  abandoning\n",
       "4     abbasid"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performing logistic regression...\n",
      "Epoch 1/5\n",
      "7376/7376 [==============================] - 7s 890us/step - loss: 22575.9649 - acc: 0.0027\n",
      "Epoch 2/5\n",
      "7376/7376 [==============================] - 3s 359us/step - loss: 21370.5847 - acc: 0.0144\n",
      "Epoch 3/5\n",
      "7376/7376 [==============================] - 3s 353us/step - loss: 20612.6393 - acc: 0.0252\n",
      "Epoch 4/5\n",
      "7376/7376 [==============================] - 3s 362us/step - loss: 19917.2440 - acc: 0.0332\n",
      "Epoch 5/5\n",
      "7376/7376 [==============================] - 3s 373us/step - loss: 19249.6370 - acc: 0.0408\n"
     ]
    }
   ],
   "source": [
    "# Train the coefficients for the vectorspace factors to predict the bag of words\n",
    "time_train_model_bf = time.time()\n",
    "\n",
    "# Only train on instances with non-empty texts\n",
    "\n",
    "(weights_frame, biases) = logistic_regression(filtered_vec_df.iloc[:,1:], filtered_bow_ndarray)\n",
    "# Obtain the softmax predictions for all instances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15012"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15012,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 15012)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7530, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_frame.iloc[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model took 0.6972157955169678\n",
      "[INFO] Sorting classification results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15002</th>\n",
       "      <th>15003</th>\n",
       "      <th>15004</th>\n",
       "      <th>15005</th>\n",
       "      <th>15006</th>\n",
       "      <th>15007</th>\n",
       "      <th>15008</th>\n",
       "      <th>15009</th>\n",
       "      <th>15010</th>\n",
       "      <th>15011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11394</td>\n",
       "      <td>2850</td>\n",
       "      <td>4200</td>\n",
       "      <td>237</td>\n",
       "      <td>3783</td>\n",
       "      <td>3344</td>\n",
       "      <td>9735</td>\n",
       "      <td>4287</td>\n",
       "      <td>8948</td>\n",
       "      <td>4397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4947</td>\n",
       "      <td>4287</td>\n",
       "      <td>3957</td>\n",
       "      <td>3398</td>\n",
       "      <td>806</td>\n",
       "      <td>782</td>\n",
       "      <td>4397</td>\n",
       "      <td>3783</td>\n",
       "      <td>4626</td>\n",
       "      <td>8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11125</td>\n",
       "      <td>11150</td>\n",
       "      <td>8741</td>\n",
       "      <td>9009</td>\n",
       "      <td>7969</td>\n",
       "      <td>1742</td>\n",
       "      <td>3398</td>\n",
       "      <td>1663</td>\n",
       "      <td>4397</td>\n",
       "      <td>7884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3415</td>\n",
       "      <td>780</td>\n",
       "      <td>806</td>\n",
       "      <td>782</td>\n",
       "      <td>10710</td>\n",
       "      <td>805</td>\n",
       "      <td>11002</td>\n",
       "      <td>10174</td>\n",
       "      <td>5649</td>\n",
       "      <td>10289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3783</td>\n",
       "      <td>2558</td>\n",
       "      <td>10289</td>\n",
       "      <td>782</td>\n",
       "      <td>10174</td>\n",
       "      <td>1663</td>\n",
       "      <td>3398</td>\n",
       "      <td>4397</td>\n",
       "      <td>5649</td>\n",
       "      <td>9735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   15002  15003  15004  15005  15006  15007  15008  15009  15010  15011\n",
       "0  11394   2850   4200    237   3783   3344   9735   4287   8948   4397\n",
       "1   4947   4287   3957   3398    806    782   4397   3783   4626   8864\n",
       "2  11125  11150   8741   9009   7969   1742   3398   1663   4397   7884\n",
       "3   3415    780    806    782  10710    805  11002  10174   5649  10289\n",
       "4   3783   2558  10289    782  10174   1663   3398   4397   5649   9735"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_frame = vec_frame.iloc[:,1:].dot(weights_frame.values) + biases\n",
    "\n",
    "\n",
    "time_train_model_af = time.time()\n",
    "print('[INFO] Training model took ' + str(time_get_data_af - time_get_data_bf))\n",
    "\n",
    "# From the softmax predictions, save the top 10 predicted words for each data point\n",
    "time_get_top_predictions_bf = time.time()\n",
    "print('[INFO] Sorting classification results...')\n",
    "sorted_frame = np.argsort(softmax_frame,axis=1).iloc[:,-num_top_words:]\n",
    "\n",
    "sorted_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Getting top predictions for each point took 448.31853723526\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>course_title</th>\n",
       "      <th>course_description</th>\n",
       "      <th>course_alternative_names</th>\n",
       "      <th>predicted_word_10</th>\n",
       "      <th>predicted_word_9</th>\n",
       "      <th>predicted_word_8</th>\n",
       "      <th>predicted_word_7</th>\n",
       "      <th>predicted_word_6</th>\n",
       "      <th>predicted_word_5</th>\n",
       "      <th>predicted_word_4</th>\n",
       "      <th>predicted_word_3</th>\n",
       "      <th>predicted_word_2</th>\n",
       "      <th>predicted_word_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Educ 298c</td>\n",
       "      <td>Group Studies, Seminars, or Group Research--DC...</td>\n",
       "      <td>Advanced group study in education. Topics vary...</td>\n",
       "      <td>Education 298C EDUC298C</td>\n",
       "      <td>securities</td>\n",
       "      <td>arbitrage</td>\n",
       "      <td>optometric</td>\n",
       "      <td>managed</td>\n",
       "      <td>nonprofit</td>\n",
       "      <td>company</td>\n",
       "      <td>ventures</td>\n",
       "      <td>patient</td>\n",
       "      <td>professionally</td>\n",
       "      <td>ocular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5069</th>\n",
       "      <td>Art 218</td>\n",
       "      <td>Seminar: Theory and Criticism</td>\n",
       "      <td>Weekly meetings will provide a forum for the d...</td>\n",
       "      <td>Art Practice 218 ART218</td>\n",
       "      <td>emotional</td>\n",
       "      <td>drawings</td>\n",
       "      <td>print</td>\n",
       "      <td>fine</td>\n",
       "      <td>sequencing</td>\n",
       "      <td>choices</td>\n",
       "      <td>garden</td>\n",
       "      <td>discover</td>\n",
       "      <td>varies</td>\n",
       "      <td>words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>Civ eng 200a</td>\n",
       "      <td>Environmental Fluid Mechanics</td>\n",
       "      <td>Fluid mechanics of the natural water and air e...</td>\n",
       "      <td>Civil &amp; Environmental Eng 200A CIV ENG200A</td>\n",
       "      <td>traffic</td>\n",
       "      <td>walls</td>\n",
       "      <td>turbulence</td>\n",
       "      <td>mixing</td>\n",
       "      <td>stratified</td>\n",
       "      <td>equation</td>\n",
       "      <td>geophysical</td>\n",
       "      <td>frames</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>shear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>Optom 231a</td>\n",
       "      <td>Graduate Specialty Clinics</td>\n",
       "      <td>Clinical examination of patients in designated...</td>\n",
       "      <td>Optometry 231A OPTOM231A</td>\n",
       "      <td>budgeting</td>\n",
       "      <td>securities</td>\n",
       "      <td>optometric</td>\n",
       "      <td>nonprofit</td>\n",
       "      <td>ventures</td>\n",
       "      <td>managed</td>\n",
       "      <td>company</td>\n",
       "      <td>patient</td>\n",
       "      <td>professionally</td>\n",
       "      <td>ocular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>Pb hlth 226a</td>\n",
       "      <td>Health Economics</td>\n",
       "      <td>This course introduces students to the economi...</td>\n",
       "      <td>Public Health 226A PB HLTH226A</td>\n",
       "      <td>hazards</td>\n",
       "      <td>indirect</td>\n",
       "      <td>driven</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>proposals</td>\n",
       "      <td>promote</td>\n",
       "      <td>concern</td>\n",
       "      <td>promotion</td>\n",
       "      <td>agents</td>\n",
       "      <td>infectious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       course_name                                       course_title  \\\n",
       "380      Educ 298c  Group Studies, Seminars, or Group Research--DC...   \n",
       "5069       Art 218                      Seminar: Theory and Criticism   \n",
       "2842  Civ eng 200a                      Environmental Fluid Mechanics   \n",
       "4051    Optom 231a                         Graduate Specialty Clinics   \n",
       "4688  Pb hlth 226a                                   Health Economics   \n",
       "\n",
       "                                     course_description  \\\n",
       "380   Advanced group study in education. Topics vary...   \n",
       "5069  Weekly meetings will provide a forum for the d...   \n",
       "2842  Fluid mechanics of the natural water and air e...   \n",
       "4051  Clinical examination of patients in designated...   \n",
       "4688  This course introduces students to the economi...   \n",
       "\n",
       "                        course_alternative_names predicted_word_10  \\\n",
       "380                      Education 298C EDUC298C        securities   \n",
       "5069                     Art Practice 218 ART218         emotional   \n",
       "2842  Civil & Environmental Eng 200A CIV ENG200A           traffic   \n",
       "4051                    Optometry 231A OPTOM231A         budgeting   \n",
       "4688              Public Health 226A PB HLTH226A           hazards   \n",
       "\n",
       "     predicted_word_9 predicted_word_8 predicted_word_7 predicted_word_6  \\\n",
       "380         arbitrage       optometric          managed        nonprofit   \n",
       "5069         drawings            print             fine       sequencing   \n",
       "2842            walls       turbulence           mixing       stratified   \n",
       "4051       securities       optometric        nonprofit         ventures   \n",
       "4688         indirect           driven       immigrants        proposals   \n",
       "\n",
       "     predicted_word_5 predicted_word_4 predicted_word_3 predicted_word_2  \\\n",
       "380           company         ventures          patient   professionally   \n",
       "5069          choices           garden         discover           varies   \n",
       "2842         equation      geophysical           frames      groundwater   \n",
       "4051          managed          company          patient   professionally   \n",
       "4688          promote          concern        promotion           agents   \n",
       "\n",
       "     predicted_word_1  \n",
       "380            ocular  \n",
       "5069            words  \n",
       "2842            shear  \n",
       "4051           ocular  \n",
       "4688       infectious  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i in range(num_top_words):\n",
    "#     print(i)\n",
    "#     print(sorted_frame.iloc[:,i])\n",
    "    new_col = vocab_frame.iloc[sorted_frame.iloc[:,i],0] # get the ith top vocab word for each entry\n",
    "#     print(new_col)\n",
    "    raw_frame['predicted_word_' + str(num_top_words-i)] = new_col.values\n",
    "    \n",
    "    \n",
    "time_get_top_predictions_af = time.time()\n",
    "print('[INFO] Getting top predictions for each point took ' + str(time_get_top_predictions_af - time_get_top_predictions_bf))\n",
    "\n",
    "raw_frame.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
